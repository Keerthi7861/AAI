{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19dc3d71-b3bc-4add-9400-ce13615a6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age;\"job\";\"marital\";\"education\";\"default\";\"balance\";\"housing\";\"loan\";\"contact\";\"day\";\"month\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"y\"'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv(r\"C:\\Users\\23adsb58\\Documents\\bank\\bank.csv\")\n",
    "print(data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e36611-6cfa-4f3c-92e6-20503ea807b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
      "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
      "       'previous', 'poutcome', 'y'],\n",
      "      dtype='object')\n",
      "\n",
      "Training Batch GD...\n",
      "Epoch 1: train_loss=0.7011, val_loss=0.6929, val_acc=0.5177\n",
      "Epoch 2: train_loss=0.6975, val_loss=0.6896, val_acc=0.5664\n",
      "Epoch 3: train_loss=0.6927, val_loss=0.6863, val_acc=0.6298\n",
      "Epoch 4: train_loss=0.6904, val_loss=0.6831, val_acc=0.6844\n",
      "Epoch 5: train_loss=0.6873, val_loss=0.6799, val_acc=0.7080\n",
      "Epoch 6: train_loss=0.6839, val_loss=0.6767, val_acc=0.7301\n",
      "Epoch 7: train_loss=0.6798, val_loss=0.6736, val_acc=0.7640\n",
      "Epoch 8: train_loss=0.6768, val_loss=0.6706, val_acc=0.7994\n",
      "Epoch 9: train_loss=0.6746, val_loss=0.6675, val_acc=0.8230\n",
      "Epoch 10: train_loss=0.6699, val_loss=0.6645, val_acc=0.8363\n",
      "Epoch 11: train_loss=0.6683, val_loss=0.6615, val_acc=0.8510\n",
      "Epoch 12: train_loss=0.6646, val_loss=0.6586, val_acc=0.8599\n",
      "Epoch 13: train_loss=0.6605, val_loss=0.6557, val_acc=0.8643\n",
      "Epoch 14: train_loss=0.6588, val_loss=0.6529, val_acc=0.8732\n",
      "Epoch 15: train_loss=0.6559, val_loss=0.6500, val_acc=0.8776\n",
      "Epoch 16: train_loss=0.6516, val_loss=0.6472, val_acc=0.8805\n",
      "Epoch 17: train_loss=0.6490, val_loss=0.6445, val_acc=0.8820\n",
      "Epoch 18: train_loss=0.6471, val_loss=0.6417, val_acc=0.8820\n",
      "Epoch 19: train_loss=0.6444, val_loss=0.6390, val_acc=0.8835\n",
      "Epoch 20: train_loss=0.6412, val_loss=0.6364, val_acc=0.8850\n",
      "Epoch 21: train_loss=0.6378, val_loss=0.6337, val_acc=0.8850\n",
      "Epoch 22: train_loss=0.6368, val_loss=0.6311, val_acc=0.8850\n",
      "Epoch 23: train_loss=0.6334, val_loss=0.6285, val_acc=0.8850\n",
      "Epoch 24: train_loss=0.6304, val_loss=0.6259, val_acc=0.8850\n",
      "Epoch 25: train_loss=0.6285, val_loss=0.6234, val_acc=0.8850\n",
      "Epoch 26: train_loss=0.6259, val_loss=0.6209, val_acc=0.8850\n",
      "Epoch 27: train_loss=0.6221, val_loss=0.6184, val_acc=0.8850\n",
      "Epoch 28: train_loss=0.6203, val_loss=0.6159, val_acc=0.8850\n",
      "Epoch 29: train_loss=0.6179, val_loss=0.6135, val_acc=0.8850\n",
      "Epoch 30: train_loss=0.6153, val_loss=0.6111, val_acc=0.8850\n",
      "Epoch 31: train_loss=0.6121, val_loss=0.6087, val_acc=0.8850\n",
      "Epoch 32: train_loss=0.6098, val_loss=0.6063, val_acc=0.8850\n",
      "Epoch 33: train_loss=0.6080, val_loss=0.6040, val_acc=0.8850\n",
      "Epoch 34: train_loss=0.6056, val_loss=0.6016, val_acc=0.8850\n",
      "Epoch 35: train_loss=0.6022, val_loss=0.5993, val_acc=0.8850\n",
      "Epoch 36: train_loss=0.6023, val_loss=0.5970, val_acc=0.8850\n",
      "Epoch 37: train_loss=0.5981, val_loss=0.5948, val_acc=0.8850\n",
      "Epoch 38: train_loss=0.5949, val_loss=0.5925, val_acc=0.8850\n",
      "Epoch 39: train_loss=0.5941, val_loss=0.5903, val_acc=0.8850\n",
      "Epoch 40: train_loss=0.5914, val_loss=0.5881, val_acc=0.8850\n",
      "Epoch 41: train_loss=0.5894, val_loss=0.5859, val_acc=0.8850\n",
      "Epoch 42: train_loss=0.5880, val_loss=0.5837, val_acc=0.8850\n",
      "Epoch 43: train_loss=0.5850, val_loss=0.5816, val_acc=0.8850\n",
      "Epoch 44: train_loss=0.5831, val_loss=0.5795, val_acc=0.8850\n",
      "Epoch 45: train_loss=0.5811, val_loss=0.5773, val_acc=0.8850\n",
      "Epoch 46: train_loss=0.5785, val_loss=0.5752, val_acc=0.8850\n",
      "Epoch 47: train_loss=0.5764, val_loss=0.5732, val_acc=0.8850\n",
      "Epoch 48: train_loss=0.5738, val_loss=0.5711, val_acc=0.8850\n",
      "Epoch 49: train_loss=0.5722, val_loss=0.5691, val_acc=0.8850\n",
      "Epoch 50: train_loss=0.5698, val_loss=0.5670, val_acc=0.8850\n",
      "Epoch 51: train_loss=0.5678, val_loss=0.5650, val_acc=0.8850\n",
      "Epoch 52: train_loss=0.5666, val_loss=0.5630, val_acc=0.8850\n",
      "Epoch 53: train_loss=0.5634, val_loss=0.5610, val_acc=0.8850\n",
      "Epoch 54: train_loss=0.5617, val_loss=0.5591, val_acc=0.8850\n",
      "Epoch 55: train_loss=0.5599, val_loss=0.5571, val_acc=0.8850\n",
      "Epoch 56: train_loss=0.5572, val_loss=0.5552, val_acc=0.8850\n",
      "Epoch 57: train_loss=0.5569, val_loss=0.5533, val_acc=0.8850\n",
      "Epoch 58: train_loss=0.5548, val_loss=0.5514, val_acc=0.8850\n",
      "Epoch 59: train_loss=0.5522, val_loss=0.5495, val_acc=0.8850\n",
      "Epoch 60: train_loss=0.5493, val_loss=0.5476, val_acc=0.8850\n",
      "Epoch 61: train_loss=0.5483, val_loss=0.5457, val_acc=0.8850\n",
      "Epoch 62: train_loss=0.5480, val_loss=0.5439, val_acc=0.8850\n",
      "Epoch 63: train_loss=0.5457, val_loss=0.5421, val_acc=0.8850\n",
      "Epoch 64: train_loss=0.5416, val_loss=0.5403, val_acc=0.8850\n",
      "Epoch 65: train_loss=0.5414, val_loss=0.5385, val_acc=0.8850\n",
      "Epoch 66: train_loss=0.5398, val_loss=0.5367, val_acc=0.8850\n",
      "Epoch 67: train_loss=0.5373, val_loss=0.5349, val_acc=0.8850\n",
      "Epoch 68: train_loss=0.5358, val_loss=0.5331, val_acc=0.8850\n",
      "Epoch 69: train_loss=0.5350, val_loss=0.5314, val_acc=0.8850\n",
      "Epoch 70: train_loss=0.5324, val_loss=0.5297, val_acc=0.8850\n",
      "Epoch 71: train_loss=0.5311, val_loss=0.5280, val_acc=0.8850\n",
      "Epoch 72: train_loss=0.5284, val_loss=0.5263, val_acc=0.8850\n",
      "Epoch 73: train_loss=0.5287, val_loss=0.5246, val_acc=0.8850\n",
      "Epoch 74: train_loss=0.5255, val_loss=0.5229, val_acc=0.8850\n",
      "Epoch 75: train_loss=0.5248, val_loss=0.5213, val_acc=0.8850\n",
      "Epoch 76: train_loss=0.5223, val_loss=0.5196, val_acc=0.8850\n",
      "Epoch 77: train_loss=0.5206, val_loss=0.5180, val_acc=0.8850\n",
      "Epoch 78: train_loss=0.5203, val_loss=0.5164, val_acc=0.8850\n",
      "Epoch 79: train_loss=0.5166, val_loss=0.5148, val_acc=0.8850\n",
      "Epoch 80: train_loss=0.5162, val_loss=0.5132, val_acc=0.8850\n",
      "Epoch 81: train_loss=0.5160, val_loss=0.5116, val_acc=0.8850\n",
      "Epoch 82: train_loss=0.5131, val_loss=0.5100, val_acc=0.8850\n",
      "Epoch 83: train_loss=0.5116, val_loss=0.5085, val_acc=0.8850\n",
      "Epoch 84: train_loss=0.5104, val_loss=0.5069, val_acc=0.8850\n",
      "Epoch 85: train_loss=0.5102, val_loss=0.5054, val_acc=0.8850\n",
      "Epoch 86: train_loss=0.5063, val_loss=0.5039, val_acc=0.8850\n",
      "Epoch 87: train_loss=0.5047, val_loss=0.5024, val_acc=0.8850\n",
      "Epoch 88: train_loss=0.5044, val_loss=0.5009, val_acc=0.8850\n",
      "Epoch 89: train_loss=0.5034, val_loss=0.4994, val_acc=0.8850\n",
      "Epoch 90: train_loss=0.5011, val_loss=0.4979, val_acc=0.8850\n",
      "Epoch 91: train_loss=0.4995, val_loss=0.4965, val_acc=0.8850\n",
      "Epoch 92: train_loss=0.4982, val_loss=0.4951, val_acc=0.8850\n",
      "Epoch 93: train_loss=0.4960, val_loss=0.4936, val_acc=0.8850\n",
      "Epoch 94: train_loss=0.4955, val_loss=0.4922, val_acc=0.8850\n",
      "Epoch 95: train_loss=0.4942, val_loss=0.4908, val_acc=0.8850\n",
      "Epoch 96: train_loss=0.4930, val_loss=0.4894, val_acc=0.8850\n",
      "Epoch 97: train_loss=0.4920, val_loss=0.4880, val_acc=0.8850\n",
      "Epoch 98: train_loss=0.4899, val_loss=0.4866, val_acc=0.8850\n",
      "Epoch 99: train_loss=0.4890, val_loss=0.4853, val_acc=0.8850\n",
      "Epoch 100: train_loss=0.4873, val_loss=0.4839, val_acc=0.8850\n",
      "Epoch 101: train_loss=0.4863, val_loss=0.4826, val_acc=0.8850\n",
      "Epoch 102: train_loss=0.4855, val_loss=0.4813, val_acc=0.8850\n",
      "Epoch 103: train_loss=0.4832, val_loss=0.4799, val_acc=0.8850\n",
      "Epoch 104: train_loss=0.4827, val_loss=0.4786, val_acc=0.8850\n",
      "Epoch 105: train_loss=0.4813, val_loss=0.4774, val_acc=0.8850\n",
      "Epoch 106: train_loss=0.4805, val_loss=0.4761, val_acc=0.8850\n",
      "Epoch 107: train_loss=0.4802, val_loss=0.4748, val_acc=0.8850\n",
      "Epoch 108: train_loss=0.4804, val_loss=0.4736, val_acc=0.8850\n",
      "Epoch 109: train_loss=0.4763, val_loss=0.4723, val_acc=0.8850\n",
      "Epoch 110: train_loss=0.4756, val_loss=0.4711, val_acc=0.8850\n",
      "Epoch 111: train_loss=0.4748, val_loss=0.4699, val_acc=0.8850\n",
      "Epoch 112: train_loss=0.4742, val_loss=0.4687, val_acc=0.8850\n",
      "Epoch 113: train_loss=0.4727, val_loss=0.4675, val_acc=0.8850\n",
      "Epoch 114: train_loss=0.4715, val_loss=0.4663, val_acc=0.8850\n",
      "Epoch 115: train_loss=0.4686, val_loss=0.4651, val_acc=0.8850\n",
      "Epoch 116: train_loss=0.4692, val_loss=0.4640, val_acc=0.8850\n",
      "Epoch 117: train_loss=0.4662, val_loss=0.4628, val_acc=0.8850\n",
      "Epoch 118: train_loss=0.4680, val_loss=0.4617, val_acc=0.8850\n",
      "Epoch 119: train_loss=0.4642, val_loss=0.4605, val_acc=0.8850\n",
      "Epoch 120: train_loss=0.4642, val_loss=0.4594, val_acc=0.8850\n",
      "Epoch 121: train_loss=0.4641, val_loss=0.4583, val_acc=0.8850\n",
      "Epoch 122: train_loss=0.4632, val_loss=0.4572, val_acc=0.8850\n",
      "Epoch 123: train_loss=0.4619, val_loss=0.4561, val_acc=0.8850\n",
      "Epoch 124: train_loss=0.4587, val_loss=0.4550, val_acc=0.8850\n",
      "Epoch 125: train_loss=0.4598, val_loss=0.4540, val_acc=0.8850\n",
      "Epoch 126: train_loss=0.4589, val_loss=0.4529, val_acc=0.8850\n",
      "Epoch 127: train_loss=0.4569, val_loss=0.4519, val_acc=0.8850\n",
      "Epoch 128: train_loss=0.4563, val_loss=0.4508, val_acc=0.8850\n",
      "Epoch 129: train_loss=0.4543, val_loss=0.4498, val_acc=0.8850\n",
      "Epoch 130: train_loss=0.4542, val_loss=0.4488, val_acc=0.8850\n",
      "Epoch 131: train_loss=0.4537, val_loss=0.4478, val_acc=0.8850\n",
      "Epoch 132: train_loss=0.4519, val_loss=0.4468, val_acc=0.8850\n",
      "Epoch 133: train_loss=0.4528, val_loss=0.4458, val_acc=0.8850\n",
      "Epoch 134: train_loss=0.4519, val_loss=0.4448, val_acc=0.8850\n",
      "Epoch 135: train_loss=0.4495, val_loss=0.4438, val_acc=0.8850\n",
      "Epoch 136: train_loss=0.4493, val_loss=0.4429, val_acc=0.8850\n",
      "Epoch 137: train_loss=0.4486, val_loss=0.4419, val_acc=0.8850\n",
      "Epoch 138: train_loss=0.4455, val_loss=0.4410, val_acc=0.8850\n",
      "Epoch 139: train_loss=0.4454, val_loss=0.4400, val_acc=0.8850\n",
      "Epoch 140: train_loss=0.4454, val_loss=0.4391, val_acc=0.8850\n",
      "Epoch 141: train_loss=0.4451, val_loss=0.4382, val_acc=0.8850\n",
      "Epoch 142: train_loss=0.4430, val_loss=0.4373, val_acc=0.8850\n",
      "Epoch 143: train_loss=0.4436, val_loss=0.4364, val_acc=0.8850\n",
      "Epoch 144: train_loss=0.4422, val_loss=0.4355, val_acc=0.8850\n",
      "Epoch 145: train_loss=0.4421, val_loss=0.4347, val_acc=0.8850\n",
      "Epoch 146: train_loss=0.4391, val_loss=0.4338, val_acc=0.8850\n",
      "Epoch 147: train_loss=0.4394, val_loss=0.4329, val_acc=0.8850\n",
      "Epoch 148: train_loss=0.4379, val_loss=0.4321, val_acc=0.8850\n",
      "Epoch 149: train_loss=0.4388, val_loss=0.4312, val_acc=0.8850\n",
      "Epoch 150: train_loss=0.4374, val_loss=0.4304, val_acc=0.8850\n",
      "Epoch 151: train_loss=0.4364, val_loss=0.4296, val_acc=0.8850\n",
      "Epoch 152: train_loss=0.4370, val_loss=0.4288, val_acc=0.8850\n",
      "Epoch 153: train_loss=0.4341, val_loss=0.4280, val_acc=0.8850\n",
      "Epoch 154: train_loss=0.4354, val_loss=0.4272, val_acc=0.8850\n",
      "Epoch 155: train_loss=0.4357, val_loss=0.4264, val_acc=0.8850\n",
      "Epoch 156: train_loss=0.4327, val_loss=0.4256, val_acc=0.8850\n",
      "Epoch 157: train_loss=0.4310, val_loss=0.4248, val_acc=0.8850\n",
      "Epoch 158: train_loss=0.4315, val_loss=0.4241, val_acc=0.8850\n",
      "Epoch 159: train_loss=0.4300, val_loss=0.4233, val_acc=0.8850\n",
      "Epoch 160: train_loss=0.4309, val_loss=0.4226, val_acc=0.8850\n",
      "Epoch 161: train_loss=0.4278, val_loss=0.4218, val_acc=0.8850\n",
      "Epoch 162: train_loss=0.4284, val_loss=0.4211, val_acc=0.8850\n",
      "Epoch 163: train_loss=0.4273, val_loss=0.4204, val_acc=0.8850\n",
      "Epoch 164: train_loss=0.4270, val_loss=0.4196, val_acc=0.8850\n",
      "Epoch 165: train_loss=0.4249, val_loss=0.4189, val_acc=0.8850\n",
      "Epoch 166: train_loss=0.4254, val_loss=0.4182, val_acc=0.8850\n",
      "Epoch 167: train_loss=0.4231, val_loss=0.4175, val_acc=0.8850\n",
      "Epoch 168: train_loss=0.4249, val_loss=0.4168, val_acc=0.8850\n",
      "Epoch 169: train_loss=0.4244, val_loss=0.4162, val_acc=0.8850\n",
      "Epoch 170: train_loss=0.4228, val_loss=0.4155, val_acc=0.8850\n",
      "Epoch 171: train_loss=0.4220, val_loss=0.4148, val_acc=0.8850\n",
      "Epoch 172: train_loss=0.4221, val_loss=0.4142, val_acc=0.8850\n",
      "Epoch 173: train_loss=0.4217, val_loss=0.4135, val_acc=0.8850\n",
      "Epoch 174: train_loss=0.4224, val_loss=0.4129, val_acc=0.8850\n",
      "Epoch 175: train_loss=0.4207, val_loss=0.4123, val_acc=0.8850\n",
      "Epoch 176: train_loss=0.4209, val_loss=0.4117, val_acc=0.8850\n",
      "Epoch 177: train_loss=0.4187, val_loss=0.4110, val_acc=0.8850\n",
      "Epoch 178: train_loss=0.4170, val_loss=0.4104, val_acc=0.8850\n",
      "Epoch 179: train_loss=0.4191, val_loss=0.4098, val_acc=0.8850\n",
      "Epoch 180: train_loss=0.4180, val_loss=0.4092, val_acc=0.8850\n",
      "Epoch 181: train_loss=0.4181, val_loss=0.4086, val_acc=0.8850\n",
      "Epoch 182: train_loss=0.4170, val_loss=0.4080, val_acc=0.8850\n",
      "Epoch 183: train_loss=0.4167, val_loss=0.4075, val_acc=0.8850\n",
      "Epoch 184: train_loss=0.4163, val_loss=0.4069, val_acc=0.8850\n",
      "Epoch 185: train_loss=0.4144, val_loss=0.4063, val_acc=0.8850\n",
      "Epoch 186: train_loss=0.4155, val_loss=0.4057, val_acc=0.8850\n",
      "Epoch 187: train_loss=0.4137, val_loss=0.4052, val_acc=0.8850\n",
      "Epoch 188: train_loss=0.4136, val_loss=0.4046, val_acc=0.8850\n",
      "Epoch 189: train_loss=0.4127, val_loss=0.4041, val_acc=0.8850\n",
      "Epoch 190: train_loss=0.4119, val_loss=0.4036, val_acc=0.8850\n",
      "Epoch 191: train_loss=0.4120, val_loss=0.4030, val_acc=0.8850\n",
      "Epoch 192: train_loss=0.4135, val_loss=0.4025, val_acc=0.8850\n",
      "Epoch 193: train_loss=0.4113, val_loss=0.4020, val_acc=0.8850\n",
      "Epoch 194: train_loss=0.4099, val_loss=0.4015, val_acc=0.8850\n",
      "Epoch 195: train_loss=0.4108, val_loss=0.4010, val_acc=0.8850\n",
      "Epoch 196: train_loss=0.4108, val_loss=0.4005, val_acc=0.8850\n",
      "Epoch 197: train_loss=0.4104, val_loss=0.4000, val_acc=0.8850\n",
      "Epoch 198: train_loss=0.4079, val_loss=0.3995, val_acc=0.8850\n",
      "Epoch 199: train_loss=0.4080, val_loss=0.3990, val_acc=0.8850\n",
      "Epoch 200: train_loss=0.4079, val_loss=0.3985, val_acc=0.8850\n",
      "Epoch 201: train_loss=0.4050, val_loss=0.3980, val_acc=0.8850\n",
      "Epoch 202: train_loss=0.4076, val_loss=0.3975, val_acc=0.8850\n",
      "Epoch 203: train_loss=0.4091, val_loss=0.3971, val_acc=0.8850\n",
      "Epoch 204: train_loss=0.4047, val_loss=0.3966, val_acc=0.8850\n",
      "Epoch 205: train_loss=0.4066, val_loss=0.3962, val_acc=0.8850\n",
      "Epoch 206: train_loss=0.4055, val_loss=0.3957, val_acc=0.8850\n",
      "Epoch 207: train_loss=0.4084, val_loss=0.3953, val_acc=0.8850\n",
      "Epoch 208: train_loss=0.4033, val_loss=0.3948, val_acc=0.8850\n",
      "Epoch 209: train_loss=0.4037, val_loss=0.3944, val_acc=0.8850\n",
      "Epoch 210: train_loss=0.4031, val_loss=0.3939, val_acc=0.8850\n",
      "Epoch 211: train_loss=0.4012, val_loss=0.3935, val_acc=0.8850\n",
      "Epoch 212: train_loss=0.4027, val_loss=0.3931, val_acc=0.8850\n",
      "Epoch 213: train_loss=0.4010, val_loss=0.3927, val_acc=0.8850\n",
      "Epoch 214: train_loss=0.4016, val_loss=0.3922, val_acc=0.8850\n",
      "Epoch 215: train_loss=0.4011, val_loss=0.3918, val_acc=0.8850\n",
      "Epoch 216: train_loss=0.3999, val_loss=0.3914, val_acc=0.8850\n",
      "Epoch 217: train_loss=0.4014, val_loss=0.3910, val_acc=0.8850\n",
      "Epoch 218: train_loss=0.3971, val_loss=0.3906, val_acc=0.8850\n",
      "Epoch 219: train_loss=0.3971, val_loss=0.3902, val_acc=0.8850\n",
      "Epoch 220: train_loss=0.4018, val_loss=0.3898, val_acc=0.8850\n",
      "Epoch 221: train_loss=0.3985, val_loss=0.3894, val_acc=0.8850\n",
      "Epoch 222: train_loss=0.3991, val_loss=0.3890, val_acc=0.8850\n",
      "Epoch 223: train_loss=0.3981, val_loss=0.3886, val_acc=0.8850\n",
      "Epoch 224: train_loss=0.3989, val_loss=0.3883, val_acc=0.8850\n",
      "Epoch 225: train_loss=0.4022, val_loss=0.3879, val_acc=0.8850\n",
      "Epoch 226: train_loss=0.3980, val_loss=0.3875, val_acc=0.8850\n",
      "Epoch 227: train_loss=0.3974, val_loss=0.3872, val_acc=0.8850\n",
      "Epoch 228: train_loss=0.3947, val_loss=0.3868, val_acc=0.8850\n",
      "Epoch 229: train_loss=0.3954, val_loss=0.3865, val_acc=0.8850\n",
      "Epoch 230: train_loss=0.3973, val_loss=0.3861, val_acc=0.8850\n",
      "Epoch 231: train_loss=0.3948, val_loss=0.3858, val_acc=0.8850\n",
      "Epoch 232: train_loss=0.3941, val_loss=0.3854, val_acc=0.8850\n",
      "Epoch 233: train_loss=0.3945, val_loss=0.3851, val_acc=0.8850\n",
      "Epoch 234: train_loss=0.3948, val_loss=0.3847, val_acc=0.8850\n",
      "Epoch 235: train_loss=0.3943, val_loss=0.3844, val_acc=0.8850\n",
      "Epoch 236: train_loss=0.3929, val_loss=0.3840, val_acc=0.8850\n",
      "Epoch 237: train_loss=0.3944, val_loss=0.3837, val_acc=0.8850\n",
      "Epoch 238: train_loss=0.3944, val_loss=0.3834, val_acc=0.8850\n",
      "Epoch 239: train_loss=0.3958, val_loss=0.3831, val_acc=0.8850\n",
      "Epoch 240: train_loss=0.3949, val_loss=0.3827, val_acc=0.8850\n",
      "Epoch 241: train_loss=0.3933, val_loss=0.3824, val_acc=0.8850\n",
      "Epoch 242: train_loss=0.3931, val_loss=0.3821, val_acc=0.8850\n",
      "Epoch 243: train_loss=0.3929, val_loss=0.3818, val_acc=0.8850\n",
      "Epoch 244: train_loss=0.3915, val_loss=0.3815, val_acc=0.8850\n",
      "Epoch 245: train_loss=0.3913, val_loss=0.3812, val_acc=0.8850\n",
      "Epoch 246: train_loss=0.3915, val_loss=0.3809, val_acc=0.8850\n",
      "Epoch 247: train_loss=0.3895, val_loss=0.3806, val_acc=0.8850\n",
      "Epoch 248: train_loss=0.3908, val_loss=0.3803, val_acc=0.8850\n",
      "Epoch 249: train_loss=0.3893, val_loss=0.3800, val_acc=0.8850\n",
      "Epoch 250: train_loss=0.3893, val_loss=0.3797, val_acc=0.8850\n",
      "Epoch 251: train_loss=0.3897, val_loss=0.3794, val_acc=0.8850\n",
      "Epoch 252: train_loss=0.3909, val_loss=0.3791, val_acc=0.8850\n",
      "Epoch 253: train_loss=0.3900, val_loss=0.3788, val_acc=0.8850\n",
      "Epoch 254: train_loss=0.3887, val_loss=0.3786, val_acc=0.8850\n",
      "Epoch 255: train_loss=0.3887, val_loss=0.3783, val_acc=0.8850\n",
      "Epoch 256: train_loss=0.3894, val_loss=0.3780, val_acc=0.8850\n",
      "Epoch 257: train_loss=0.3891, val_loss=0.3777, val_acc=0.8850\n",
      "Epoch 258: train_loss=0.3895, val_loss=0.3775, val_acc=0.8850\n",
      "Epoch 259: train_loss=0.3858, val_loss=0.3772, val_acc=0.8850\n",
      "Epoch 260: train_loss=0.3871, val_loss=0.3769, val_acc=0.8850\n",
      "Epoch 261: train_loss=0.3893, val_loss=0.3767, val_acc=0.8850\n",
      "Epoch 262: train_loss=0.3886, val_loss=0.3764, val_acc=0.8850\n",
      "Epoch 263: train_loss=0.3873, val_loss=0.3762, val_acc=0.8850\n",
      "Epoch 264: train_loss=0.3881, val_loss=0.3759, val_acc=0.8850\n",
      "Epoch 265: train_loss=0.3846, val_loss=0.3756, val_acc=0.8850\n",
      "Epoch 266: train_loss=0.3866, val_loss=0.3754, val_acc=0.8850\n",
      "Epoch 267: train_loss=0.3888, val_loss=0.3752, val_acc=0.8850\n",
      "Epoch 268: train_loss=0.3853, val_loss=0.3749, val_acc=0.8850\n",
      "Epoch 269: train_loss=0.3865, val_loss=0.3747, val_acc=0.8850\n",
      "Epoch 270: train_loss=0.3847, val_loss=0.3744, val_acc=0.8850\n",
      "Epoch 271: train_loss=0.3852, val_loss=0.3742, val_acc=0.8850\n",
      "Epoch 272: train_loss=0.3849, val_loss=0.3739, val_acc=0.8850\n",
      "Epoch 273: train_loss=0.3849, val_loss=0.3737, val_acc=0.8850\n",
      "Epoch 274: train_loss=0.3858, val_loss=0.3735, val_acc=0.8850\n",
      "Epoch 275: train_loss=0.3828, val_loss=0.3732, val_acc=0.8850\n",
      "Epoch 276: train_loss=0.3838, val_loss=0.3730, val_acc=0.8850\n",
      "Epoch 277: train_loss=0.3824, val_loss=0.3728, val_acc=0.8850\n",
      "Epoch 278: train_loss=0.3821, val_loss=0.3725, val_acc=0.8850\n",
      "Epoch 279: train_loss=0.3827, val_loss=0.3723, val_acc=0.8850\n",
      "Epoch 280: train_loss=0.3822, val_loss=0.3721, val_acc=0.8850\n",
      "Epoch 281: train_loss=0.3841, val_loss=0.3719, val_acc=0.8850\n",
      "Epoch 282: train_loss=0.3837, val_loss=0.3716, val_acc=0.8850\n",
      "Epoch 283: train_loss=0.3829, val_loss=0.3714, val_acc=0.8850\n",
      "Epoch 284: train_loss=0.3816, val_loss=0.3712, val_acc=0.8850\n",
      "Epoch 285: train_loss=0.3819, val_loss=0.3710, val_acc=0.8850\n",
      "Epoch 286: train_loss=0.3823, val_loss=0.3708, val_acc=0.8850\n",
      "Epoch 287: train_loss=0.3820, val_loss=0.3706, val_acc=0.8850\n",
      "Epoch 288: train_loss=0.3809, val_loss=0.3703, val_acc=0.8850\n",
      "Epoch 289: train_loss=0.3797, val_loss=0.3701, val_acc=0.8850\n",
      "Epoch 290: train_loss=0.3827, val_loss=0.3699, val_acc=0.8850\n",
      "Epoch 291: train_loss=0.3794, val_loss=0.3697, val_acc=0.8850\n",
      "Epoch 292: train_loss=0.3807, val_loss=0.3695, val_acc=0.8850\n",
      "Epoch 293: train_loss=0.3791, val_loss=0.3693, val_acc=0.8850\n",
      "Epoch 294: train_loss=0.3799, val_loss=0.3691, val_acc=0.8850\n",
      "Epoch 295: train_loss=0.3803, val_loss=0.3689, val_acc=0.8850\n",
      "Epoch 296: train_loss=0.3779, val_loss=0.3687, val_acc=0.8850\n",
      "Epoch 297: train_loss=0.3782, val_loss=0.3685, val_acc=0.8850\n",
      "Epoch 298: train_loss=0.3777, val_loss=0.3683, val_acc=0.8850\n",
      "Epoch 299: train_loss=0.3786, val_loss=0.3681, val_acc=0.8850\n",
      "Epoch 300: train_loss=0.3809, val_loss=0.3679, val_acc=0.8850\n",
      "Epoch 301: train_loss=0.3785, val_loss=0.3677, val_acc=0.8850\n",
      "Epoch 302: train_loss=0.3779, val_loss=0.3675, val_acc=0.8850\n",
      "Epoch 303: train_loss=0.3773, val_loss=0.3673, val_acc=0.8850\n",
      "Epoch 304: train_loss=0.3787, val_loss=0.3671, val_acc=0.8850\n",
      "Epoch 305: train_loss=0.3778, val_loss=0.3670, val_acc=0.8850\n",
      "Epoch 306: train_loss=0.3778, val_loss=0.3668, val_acc=0.8850\n",
      "Epoch 307: train_loss=0.3783, val_loss=0.3666, val_acc=0.8850\n",
      "Epoch 308: train_loss=0.3754, val_loss=0.3664, val_acc=0.8850\n",
      "Epoch 309: train_loss=0.3779, val_loss=0.3662, val_acc=0.8850\n",
      "Epoch 310: train_loss=0.3750, val_loss=0.3660, val_acc=0.8850\n",
      "Epoch 311: train_loss=0.3772, val_loss=0.3658, val_acc=0.8850\n",
      "Epoch 312: train_loss=0.3770, val_loss=0.3657, val_acc=0.8850\n",
      "Epoch 313: train_loss=0.3781, val_loss=0.3655, val_acc=0.8850\n",
      "Epoch 314: train_loss=0.3768, val_loss=0.3653, val_acc=0.8850\n",
      "Epoch 315: train_loss=0.3763, val_loss=0.3651, val_acc=0.8850\n",
      "Epoch 316: train_loss=0.3764, val_loss=0.3650, val_acc=0.8850\n",
      "Epoch 317: train_loss=0.3755, val_loss=0.3648, val_acc=0.8850\n",
      "Epoch 318: train_loss=0.3773, val_loss=0.3646, val_acc=0.8850\n",
      "Epoch 319: train_loss=0.3744, val_loss=0.3644, val_acc=0.8850\n",
      "Epoch 320: train_loss=0.3769, val_loss=0.3643, val_acc=0.8850\n",
      "Epoch 321: train_loss=0.3740, val_loss=0.3641, val_acc=0.8850\n",
      "Epoch 322: train_loss=0.3747, val_loss=0.3639, val_acc=0.8850\n",
      "Epoch 323: train_loss=0.3736, val_loss=0.3638, val_acc=0.8850\n",
      "Epoch 324: train_loss=0.3762, val_loss=0.3636, val_acc=0.8850\n",
      "Epoch 325: train_loss=0.3742, val_loss=0.3634, val_acc=0.8850\n",
      "Epoch 326: train_loss=0.3761, val_loss=0.3633, val_acc=0.8850\n",
      "Epoch 327: train_loss=0.3731, val_loss=0.3631, val_acc=0.8850\n",
      "Epoch 328: train_loss=0.3750, val_loss=0.3629, val_acc=0.8850\n",
      "Epoch 329: train_loss=0.3772, val_loss=0.3628, val_acc=0.8850\n",
      "Epoch 330: train_loss=0.3739, val_loss=0.3626, val_acc=0.8850\n",
      "Epoch 331: train_loss=0.3728, val_loss=0.3625, val_acc=0.8850\n",
      "Epoch 332: train_loss=0.3755, val_loss=0.3623, val_acc=0.8850\n",
      "Epoch 333: train_loss=0.3702, val_loss=0.3621, val_acc=0.8850\n",
      "Epoch 334: train_loss=0.3744, val_loss=0.3620, val_acc=0.8850\n",
      "Epoch 335: train_loss=0.3701, val_loss=0.3618, val_acc=0.8850\n",
      "Epoch 336: train_loss=0.3739, val_loss=0.3617, val_acc=0.8850\n",
      "Epoch 337: train_loss=0.3707, val_loss=0.3615, val_acc=0.8850\n",
      "Epoch 338: train_loss=0.3721, val_loss=0.3614, val_acc=0.8850\n",
      "Epoch 339: train_loss=0.3707, val_loss=0.3612, val_acc=0.8850\n",
      "Epoch 340: train_loss=0.3723, val_loss=0.3611, val_acc=0.8850\n",
      "Epoch 341: train_loss=0.3719, val_loss=0.3609, val_acc=0.8850\n",
      "Epoch 342: train_loss=0.3716, val_loss=0.3607, val_acc=0.8850\n",
      "Epoch 343: train_loss=0.3702, val_loss=0.3606, val_acc=0.8850\n",
      "Epoch 344: train_loss=0.3708, val_loss=0.3604, val_acc=0.8850\n",
      "Epoch 345: train_loss=0.3711, val_loss=0.3603, val_acc=0.8850\n",
      "Epoch 346: train_loss=0.3705, val_loss=0.3601, val_acc=0.8850\n",
      "Epoch 347: train_loss=0.3695, val_loss=0.3600, val_acc=0.8850\n",
      "Epoch 348: train_loss=0.3690, val_loss=0.3598, val_acc=0.8850\n",
      "Epoch 349: train_loss=0.3740, val_loss=0.3597, val_acc=0.8850\n",
      "Epoch 350: train_loss=0.3711, val_loss=0.3596, val_acc=0.8850\n",
      "Epoch 351: train_loss=0.3705, val_loss=0.3594, val_acc=0.8850\n",
      "Epoch 352: train_loss=0.3711, val_loss=0.3593, val_acc=0.8850\n",
      "Epoch 353: train_loss=0.3716, val_loss=0.3591, val_acc=0.8850\n",
      "Epoch 354: train_loss=0.3694, val_loss=0.3590, val_acc=0.8850\n",
      "Epoch 355: train_loss=0.3694, val_loss=0.3588, val_acc=0.8850\n",
      "Epoch 356: train_loss=0.3679, val_loss=0.3587, val_acc=0.8850\n",
      "Epoch 357: train_loss=0.3690, val_loss=0.3586, val_acc=0.8850\n",
      "Epoch 358: train_loss=0.3689, val_loss=0.3584, val_acc=0.8850\n",
      "Epoch 359: train_loss=0.3706, val_loss=0.3583, val_acc=0.8850\n",
      "Epoch 360: train_loss=0.3690, val_loss=0.3581, val_acc=0.8850\n",
      "Epoch 361: train_loss=0.3664, val_loss=0.3580, val_acc=0.8850\n",
      "Epoch 362: train_loss=0.3704, val_loss=0.3579, val_acc=0.8850\n",
      "Epoch 363: train_loss=0.3682, val_loss=0.3577, val_acc=0.8850\n",
      "Epoch 364: train_loss=0.3673, val_loss=0.3576, val_acc=0.8850\n",
      "Epoch 365: train_loss=0.3665, val_loss=0.3574, val_acc=0.8850\n",
      "Epoch 366: train_loss=0.3699, val_loss=0.3573, val_acc=0.8850\n",
      "Epoch 367: train_loss=0.3676, val_loss=0.3572, val_acc=0.8850\n",
      "Epoch 368: train_loss=0.3686, val_loss=0.3570, val_acc=0.8850\n",
      "Epoch 369: train_loss=0.3673, val_loss=0.3569, val_acc=0.8850\n",
      "Epoch 370: train_loss=0.3692, val_loss=0.3568, val_acc=0.8850\n",
      "Epoch 371: train_loss=0.3677, val_loss=0.3566, val_acc=0.8850\n",
      "Epoch 372: train_loss=0.3658, val_loss=0.3565, val_acc=0.8850\n",
      "Epoch 373: train_loss=0.3666, val_loss=0.3564, val_acc=0.8850\n",
      "Epoch 374: train_loss=0.3634, val_loss=0.3562, val_acc=0.8850\n",
      "Epoch 375: train_loss=0.3663, val_loss=0.3561, val_acc=0.8850\n",
      "Epoch 376: train_loss=0.3678, val_loss=0.3560, val_acc=0.8850\n",
      "Epoch 377: train_loss=0.3658, val_loss=0.3558, val_acc=0.8850\n",
      "Epoch 378: train_loss=0.3665, val_loss=0.3557, val_acc=0.8850\n",
      "Epoch 379: train_loss=0.3667, val_loss=0.3556, val_acc=0.8850\n",
      "Epoch 380: train_loss=0.3659, val_loss=0.3555, val_acc=0.8850\n",
      "Epoch 381: train_loss=0.3649, val_loss=0.3553, val_acc=0.8850\n",
      "Epoch 382: train_loss=0.3634, val_loss=0.3552, val_acc=0.8850\n",
      "Epoch 383: train_loss=0.3639, val_loss=0.3551, val_acc=0.8850\n",
      "Epoch 384: train_loss=0.3659, val_loss=0.3549, val_acc=0.8850\n",
      "Epoch 385: train_loss=0.3636, val_loss=0.3548, val_acc=0.8850\n",
      "Epoch 386: train_loss=0.3671, val_loss=0.3547, val_acc=0.8850\n",
      "Epoch 387: train_loss=0.3650, val_loss=0.3546, val_acc=0.8850\n",
      "Epoch 388: train_loss=0.3672, val_loss=0.3544, val_acc=0.8850\n",
      "Epoch 389: train_loss=0.3645, val_loss=0.3543, val_acc=0.8850\n",
      "Epoch 390: train_loss=0.3646, val_loss=0.3542, val_acc=0.8850\n",
      "Epoch 391: train_loss=0.3653, val_loss=0.3541, val_acc=0.8850\n",
      "Epoch 392: train_loss=0.3636, val_loss=0.3539, val_acc=0.8850\n",
      "Epoch 393: train_loss=0.3639, val_loss=0.3538, val_acc=0.8850\n",
      "Epoch 394: train_loss=0.3660, val_loss=0.3537, val_acc=0.8850\n",
      "Epoch 395: train_loss=0.3658, val_loss=0.3536, val_acc=0.8850\n",
      "Epoch 396: train_loss=0.3621, val_loss=0.3535, val_acc=0.8850\n",
      "Epoch 397: train_loss=0.3638, val_loss=0.3533, val_acc=0.8850\n",
      "Epoch 398: train_loss=0.3614, val_loss=0.3532, val_acc=0.8850\n",
      "Epoch 399: train_loss=0.3650, val_loss=0.3531, val_acc=0.8850\n",
      "Epoch 400: train_loss=0.3624, val_loss=0.3530, val_acc=0.8850\n",
      "Epoch 401: train_loss=0.3638, val_loss=0.3529, val_acc=0.8850\n",
      "Epoch 402: train_loss=0.3656, val_loss=0.3527, val_acc=0.8850\n",
      "Epoch 403: train_loss=0.3618, val_loss=0.3526, val_acc=0.8850\n",
      "Epoch 404: train_loss=0.3633, val_loss=0.3525, val_acc=0.8850\n",
      "Epoch 405: train_loss=0.3617, val_loss=0.3524, val_acc=0.8850\n",
      "Epoch 406: train_loss=0.3642, val_loss=0.3523, val_acc=0.8850\n",
      "Epoch 407: train_loss=0.3635, val_loss=0.3521, val_acc=0.8850\n",
      "Epoch 408: train_loss=0.3605, val_loss=0.3520, val_acc=0.8850\n",
      "Epoch 409: train_loss=0.3628, val_loss=0.3519, val_acc=0.8850\n",
      "Epoch 410: train_loss=0.3625, val_loss=0.3518, val_acc=0.8850\n",
      "Epoch 411: train_loss=0.3629, val_loss=0.3517, val_acc=0.8850\n",
      "Epoch 412: train_loss=0.3622, val_loss=0.3516, val_acc=0.8850\n",
      "Epoch 413: train_loss=0.3599, val_loss=0.3514, val_acc=0.8850\n",
      "Epoch 414: train_loss=0.3598, val_loss=0.3513, val_acc=0.8850\n",
      "Epoch 415: train_loss=0.3612, val_loss=0.3512, val_acc=0.8850\n",
      "Epoch 416: train_loss=0.3595, val_loss=0.3511, val_acc=0.8850\n",
      "Epoch 417: train_loss=0.3616, val_loss=0.3510, val_acc=0.8850\n",
      "Epoch 418: train_loss=0.3620, val_loss=0.3509, val_acc=0.8850\n",
      "Epoch 419: train_loss=0.3602, val_loss=0.3507, val_acc=0.8850\n",
      "Epoch 420: train_loss=0.3618, val_loss=0.3506, val_acc=0.8850\n",
      "Epoch 421: train_loss=0.3624, val_loss=0.3505, val_acc=0.8850\n",
      "Epoch 422: train_loss=0.3593, val_loss=0.3504, val_acc=0.8850\n",
      "Epoch 423: train_loss=0.3607, val_loss=0.3503, val_acc=0.8850\n",
      "Epoch 424: train_loss=0.3629, val_loss=0.3502, val_acc=0.8850\n",
      "Epoch 425: train_loss=0.3582, val_loss=0.3501, val_acc=0.8850\n",
      "Epoch 426: train_loss=0.3619, val_loss=0.3500, val_acc=0.8850\n",
      "Epoch 427: train_loss=0.3619, val_loss=0.3498, val_acc=0.8850\n",
      "Epoch 428: train_loss=0.3592, val_loss=0.3497, val_acc=0.8850\n",
      "Epoch 429: train_loss=0.3626, val_loss=0.3496, val_acc=0.8850\n",
      "Epoch 430: train_loss=0.3580, val_loss=0.3495, val_acc=0.8850\n",
      "Epoch 431: train_loss=0.3582, val_loss=0.3494, val_acc=0.8850\n",
      "Epoch 432: train_loss=0.3582, val_loss=0.3493, val_acc=0.8850\n",
      "Epoch 433: train_loss=0.3583, val_loss=0.3492, val_acc=0.8850\n",
      "Epoch 434: train_loss=0.3567, val_loss=0.3491, val_acc=0.8850\n",
      "Epoch 435: train_loss=0.3583, val_loss=0.3489, val_acc=0.8850\n",
      "Epoch 436: train_loss=0.3592, val_loss=0.3488, val_acc=0.8850\n",
      "Epoch 437: train_loss=0.3584, val_loss=0.3487, val_acc=0.8850\n",
      "Epoch 438: train_loss=0.3578, val_loss=0.3486, val_acc=0.8850\n",
      "Epoch 439: train_loss=0.3601, val_loss=0.3485, val_acc=0.8850\n",
      "Epoch 440: train_loss=0.3570, val_loss=0.3484, val_acc=0.8850\n",
      "Epoch 441: train_loss=0.3571, val_loss=0.3483, val_acc=0.8850\n",
      "Epoch 442: train_loss=0.3595, val_loss=0.3482, val_acc=0.8850\n",
      "Epoch 443: train_loss=0.3590, val_loss=0.3481, val_acc=0.8850\n",
      "Epoch 444: train_loss=0.3574, val_loss=0.3480, val_acc=0.8850\n",
      "Epoch 445: train_loss=0.3564, val_loss=0.3478, val_acc=0.8850\n",
      "Epoch 446: train_loss=0.3601, val_loss=0.3477, val_acc=0.8850\n",
      "Epoch 447: train_loss=0.3590, val_loss=0.3476, val_acc=0.8850\n",
      "Epoch 448: train_loss=0.3590, val_loss=0.3475, val_acc=0.8850\n",
      "Epoch 449: train_loss=0.3582, val_loss=0.3474, val_acc=0.8850\n",
      "Epoch 450: train_loss=0.3566, val_loss=0.3473, val_acc=0.8850\n",
      "Epoch 451: train_loss=0.3558, val_loss=0.3472, val_acc=0.8850\n",
      "Epoch 452: train_loss=0.3591, val_loss=0.3471, val_acc=0.8850\n",
      "Epoch 453: train_loss=0.3542, val_loss=0.3470, val_acc=0.8850\n",
      "Epoch 454: train_loss=0.3587, val_loss=0.3469, val_acc=0.8850\n",
      "Epoch 455: train_loss=0.3571, val_loss=0.3468, val_acc=0.8850\n",
      "Epoch 456: train_loss=0.3562, val_loss=0.3467, val_acc=0.8850\n",
      "Epoch 457: train_loss=0.3570, val_loss=0.3466, val_acc=0.8850\n",
      "Epoch 458: train_loss=0.3532, val_loss=0.3465, val_acc=0.8850\n",
      "Epoch 459: train_loss=0.3544, val_loss=0.3464, val_acc=0.8850\n",
      "Epoch 460: train_loss=0.3559, val_loss=0.3463, val_acc=0.8850\n",
      "Epoch 461: train_loss=0.3566, val_loss=0.3462, val_acc=0.8850\n",
      "Epoch 462: train_loss=0.3563, val_loss=0.3460, val_acc=0.8850\n",
      "Epoch 463: train_loss=0.3522, val_loss=0.3459, val_acc=0.8850\n",
      "Epoch 464: train_loss=0.3550, val_loss=0.3458, val_acc=0.8850\n",
      "Epoch 465: train_loss=0.3563, val_loss=0.3457, val_acc=0.8850\n",
      "Epoch 466: train_loss=0.3534, val_loss=0.3456, val_acc=0.8850\n",
      "Epoch 467: train_loss=0.3552, val_loss=0.3455, val_acc=0.8850\n",
      "Epoch 468: train_loss=0.3537, val_loss=0.3454, val_acc=0.8850\n",
      "Epoch 469: train_loss=0.3540, val_loss=0.3453, val_acc=0.8850\n",
      "Epoch 470: train_loss=0.3537, val_loss=0.3452, val_acc=0.8850\n",
      "Epoch 471: train_loss=0.3532, val_loss=0.3451, val_acc=0.8850\n",
      "Epoch 472: train_loss=0.3550, val_loss=0.3450, val_acc=0.8850\n",
      "Epoch 473: train_loss=0.3521, val_loss=0.3449, val_acc=0.8850\n",
      "Epoch 474: train_loss=0.3550, val_loss=0.3448, val_acc=0.8850\n",
      "Epoch 475: train_loss=0.3521, val_loss=0.3447, val_acc=0.8850\n",
      "Epoch 476: train_loss=0.3535, val_loss=0.3446, val_acc=0.8850\n",
      "Epoch 477: train_loss=0.3509, val_loss=0.3445, val_acc=0.8850\n",
      "Epoch 478: train_loss=0.3525, val_loss=0.3444, val_acc=0.8850\n",
      "Epoch 479: train_loss=0.3528, val_loss=0.3443, val_acc=0.8850\n",
      "Epoch 480: train_loss=0.3537, val_loss=0.3442, val_acc=0.8850\n",
      "Epoch 481: train_loss=0.3533, val_loss=0.3441, val_acc=0.8850\n",
      "Epoch 482: train_loss=0.3518, val_loss=0.3440, val_acc=0.8850\n",
      "Epoch 483: train_loss=0.3508, val_loss=0.3439, val_acc=0.8850\n",
      "Epoch 484: train_loss=0.3540, val_loss=0.3438, val_acc=0.8850\n",
      "Epoch 485: train_loss=0.3521, val_loss=0.3437, val_acc=0.8850\n",
      "Epoch 486: train_loss=0.3528, val_loss=0.3436, val_acc=0.8850\n",
      "Epoch 487: train_loss=0.3535, val_loss=0.3435, val_acc=0.8850\n",
      "Epoch 488: train_loss=0.3537, val_loss=0.3434, val_acc=0.8850\n",
      "Epoch 489: train_loss=0.3544, val_loss=0.3433, val_acc=0.8850\n",
      "Epoch 490: train_loss=0.3505, val_loss=0.3432, val_acc=0.8850\n",
      "Epoch 491: train_loss=0.3538, val_loss=0.3431, val_acc=0.8850\n",
      "Epoch 492: train_loss=0.3523, val_loss=0.3430, val_acc=0.8850\n",
      "Epoch 493: train_loss=0.3518, val_loss=0.3429, val_acc=0.8850\n",
      "Epoch 494: train_loss=0.3498, val_loss=0.3428, val_acc=0.8850\n",
      "Epoch 495: train_loss=0.3523, val_loss=0.3427, val_acc=0.8850\n",
      "Epoch 496: train_loss=0.3499, val_loss=0.3426, val_acc=0.8850\n",
      "Epoch 497: train_loss=0.3516, val_loss=0.3425, val_acc=0.8850\n",
      "Epoch 498: train_loss=0.3506, val_loss=0.3424, val_acc=0.8850\n",
      "Epoch 499: train_loss=0.3526, val_loss=0.3423, val_acc=0.8850\n",
      "Epoch 500: train_loss=0.3510, val_loss=0.3422, val_acc=0.8850\n",
      "Epoch 501: train_loss=0.3495, val_loss=0.3421, val_acc=0.8850\n",
      "Epoch 502: train_loss=0.3498, val_loss=0.3420, val_acc=0.8850\n",
      "Epoch 503: train_loss=0.3492, val_loss=0.3419, val_acc=0.8850\n",
      "Epoch 504: train_loss=0.3508, val_loss=0.3418, val_acc=0.8850\n",
      "Epoch 505: train_loss=0.3500, val_loss=0.3417, val_acc=0.8850\n",
      "Epoch 506: train_loss=0.3507, val_loss=0.3416, val_acc=0.8850\n",
      "Epoch 507: train_loss=0.3486, val_loss=0.3415, val_acc=0.8850\n",
      "Epoch 508: train_loss=0.3521, val_loss=0.3414, val_acc=0.8850\n",
      "Epoch 509: train_loss=0.3498, val_loss=0.3413, val_acc=0.8850\n",
      "Epoch 510: train_loss=0.3481, val_loss=0.3412, val_acc=0.8850\n",
      "Epoch 511: train_loss=0.3500, val_loss=0.3411, val_acc=0.8850\n",
      "Epoch 512: train_loss=0.3518, val_loss=0.3410, val_acc=0.8850\n",
      "Epoch 513: train_loss=0.3518, val_loss=0.3409, val_acc=0.8850\n",
      "Epoch 514: train_loss=0.3486, val_loss=0.3408, val_acc=0.8850\n",
      "Epoch 515: train_loss=0.3487, val_loss=0.3407, val_acc=0.8850\n",
      "Epoch 516: train_loss=0.3478, val_loss=0.3406, val_acc=0.8850\n",
      "Epoch 517: train_loss=0.3476, val_loss=0.3405, val_acc=0.8850\n",
      "Epoch 518: train_loss=0.3490, val_loss=0.3404, val_acc=0.8850\n",
      "Epoch 519: train_loss=0.3481, val_loss=0.3403, val_acc=0.8850\n",
      "Epoch 520: train_loss=0.3459, val_loss=0.3402, val_acc=0.8850\n",
      "Epoch 521: train_loss=0.3465, val_loss=0.3401, val_acc=0.8850\n",
      "Epoch 522: train_loss=0.3486, val_loss=0.3400, val_acc=0.8850\n",
      "Epoch 523: train_loss=0.3460, val_loss=0.3399, val_acc=0.8850\n",
      "Epoch 524: train_loss=0.3472, val_loss=0.3399, val_acc=0.8850\n",
      "Epoch 525: train_loss=0.3462, val_loss=0.3398, val_acc=0.8850\n",
      "Epoch 526: train_loss=0.3482, val_loss=0.3397, val_acc=0.8850\n",
      "Epoch 527: train_loss=0.3471, val_loss=0.3396, val_acc=0.8850\n",
      "Epoch 528: train_loss=0.3507, val_loss=0.3395, val_acc=0.8850\n",
      "Epoch 529: train_loss=0.3451, val_loss=0.3394, val_acc=0.8850\n",
      "Epoch 530: train_loss=0.3465, val_loss=0.3393, val_acc=0.8850\n",
      "Epoch 531: train_loss=0.3450, val_loss=0.3392, val_acc=0.8850\n",
      "Epoch 532: train_loss=0.3472, val_loss=0.3391, val_acc=0.8850\n",
      "Epoch 533: train_loss=0.3487, val_loss=0.3390, val_acc=0.8850\n",
      "Epoch 534: train_loss=0.3492, val_loss=0.3389, val_acc=0.8850\n",
      "Epoch 535: train_loss=0.3454, val_loss=0.3388, val_acc=0.8850\n",
      "Epoch 536: train_loss=0.3466, val_loss=0.3387, val_acc=0.8850\n",
      "Epoch 537: train_loss=0.3474, val_loss=0.3386, val_acc=0.8850\n",
      "Epoch 538: train_loss=0.3475, val_loss=0.3385, val_acc=0.8850\n",
      "Epoch 539: train_loss=0.3461, val_loss=0.3384, val_acc=0.8850\n",
      "Epoch 540: train_loss=0.3460, val_loss=0.3383, val_acc=0.8850\n",
      "Epoch 541: train_loss=0.3435, val_loss=0.3383, val_acc=0.8850\n",
      "Epoch 542: train_loss=0.3450, val_loss=0.3382, val_acc=0.8850\n",
      "Epoch 543: train_loss=0.3477, val_loss=0.3381, val_acc=0.8850\n",
      "Epoch 544: train_loss=0.3426, val_loss=0.3380, val_acc=0.8850\n",
      "Epoch 545: train_loss=0.3465, val_loss=0.3379, val_acc=0.8850\n",
      "Epoch 546: train_loss=0.3452, val_loss=0.3378, val_acc=0.8850\n",
      "Epoch 547: train_loss=0.3453, val_loss=0.3377, val_acc=0.8850\n",
      "Epoch 548: train_loss=0.3446, val_loss=0.3376, val_acc=0.8850\n",
      "Epoch 549: train_loss=0.3464, val_loss=0.3375, val_acc=0.8850\n",
      "Epoch 550: train_loss=0.3440, val_loss=0.3374, val_acc=0.8850\n",
      "Epoch 551: train_loss=0.3465, val_loss=0.3373, val_acc=0.8850\n",
      "Epoch 552: train_loss=0.3456, val_loss=0.3372, val_acc=0.8850\n",
      "Epoch 553: train_loss=0.3458, val_loss=0.3371, val_acc=0.8850\n",
      "Epoch 554: train_loss=0.3429, val_loss=0.3370, val_acc=0.8850\n",
      "Epoch 555: train_loss=0.3439, val_loss=0.3370, val_acc=0.8850\n",
      "Epoch 556: train_loss=0.3444, val_loss=0.3369, val_acc=0.8850\n",
      "Epoch 557: train_loss=0.3443, val_loss=0.3368, val_acc=0.8850\n",
      "Epoch 558: train_loss=0.3430, val_loss=0.3367, val_acc=0.8850\n",
      "Epoch 559: train_loss=0.3463, val_loss=0.3366, val_acc=0.8850\n",
      "Epoch 560: train_loss=0.3421, val_loss=0.3365, val_acc=0.8850\n",
      "Epoch 561: train_loss=0.3435, val_loss=0.3364, val_acc=0.8850\n",
      "Epoch 562: train_loss=0.3426, val_loss=0.3363, val_acc=0.8850\n",
      "Epoch 563: train_loss=0.3428, val_loss=0.3362, val_acc=0.8850\n",
      "Epoch 564: train_loss=0.3438, val_loss=0.3361, val_acc=0.8850\n",
      "Epoch 565: train_loss=0.3419, val_loss=0.3360, val_acc=0.8850\n",
      "Epoch 566: train_loss=0.3421, val_loss=0.3359, val_acc=0.8850\n",
      "Epoch 567: train_loss=0.3431, val_loss=0.3359, val_acc=0.8850\n",
      "Epoch 568: train_loss=0.3430, val_loss=0.3358, val_acc=0.8850\n",
      "Epoch 569: train_loss=0.3441, val_loss=0.3357, val_acc=0.8850\n",
      "Epoch 570: train_loss=0.3405, val_loss=0.3356, val_acc=0.8850\n",
      "Epoch 571: train_loss=0.3430, val_loss=0.3355, val_acc=0.8850\n",
      "Epoch 572: train_loss=0.3408, val_loss=0.3354, val_acc=0.8850\n",
      "Epoch 573: train_loss=0.3421, val_loss=0.3353, val_acc=0.8850\n",
      "Epoch 574: train_loss=0.3419, val_loss=0.3352, val_acc=0.8850\n",
      "Epoch 575: train_loss=0.3431, val_loss=0.3351, val_acc=0.8850\n",
      "Epoch 576: train_loss=0.3433, val_loss=0.3350, val_acc=0.8850\n",
      "Epoch 577: train_loss=0.3434, val_loss=0.3350, val_acc=0.8850\n",
      "Epoch 578: train_loss=0.3408, val_loss=0.3349, val_acc=0.8850\n",
      "Epoch 579: train_loss=0.3413, val_loss=0.3348, val_acc=0.8850\n",
      "Epoch 580: train_loss=0.3432, val_loss=0.3347, val_acc=0.8850\n",
      "Epoch 581: train_loss=0.3432, val_loss=0.3346, val_acc=0.8850\n",
      "Epoch 582: train_loss=0.3405, val_loss=0.3345, val_acc=0.8850\n",
      "Epoch 583: train_loss=0.3415, val_loss=0.3344, val_acc=0.8850\n",
      "Epoch 584: train_loss=0.3375, val_loss=0.3343, val_acc=0.8850\n",
      "Epoch 585: train_loss=0.3396, val_loss=0.3342, val_acc=0.8850\n",
      "Epoch 586: train_loss=0.3453, val_loss=0.3342, val_acc=0.8850\n",
      "Epoch 587: train_loss=0.3422, val_loss=0.3341, val_acc=0.8850\n",
      "Epoch 588: train_loss=0.3401, val_loss=0.3340, val_acc=0.8850\n",
      "Epoch 589: train_loss=0.3414, val_loss=0.3339, val_acc=0.8850\n",
      "Epoch 590: train_loss=0.3432, val_loss=0.3338, val_acc=0.8850\n",
      "Epoch 591: train_loss=0.3394, val_loss=0.3337, val_acc=0.8850\n",
      "Epoch 592: train_loss=0.3379, val_loss=0.3336, val_acc=0.8850\n",
      "Epoch 593: train_loss=0.3397, val_loss=0.3335, val_acc=0.8850\n",
      "Epoch 594: train_loss=0.3400, val_loss=0.3334, val_acc=0.8850\n",
      "Epoch 595: train_loss=0.3388, val_loss=0.3334, val_acc=0.8850\n",
      "Epoch 596: train_loss=0.3410, val_loss=0.3333, val_acc=0.8850\n",
      "Epoch 597: train_loss=0.3370, val_loss=0.3332, val_acc=0.8850\n",
      "Epoch 598: train_loss=0.3402, val_loss=0.3331, val_acc=0.8850\n",
      "Epoch 599: train_loss=0.3374, val_loss=0.3330, val_acc=0.8850\n",
      "Epoch 600: train_loss=0.3408, val_loss=0.3329, val_acc=0.8850\n",
      "Epoch 601: train_loss=0.3402, val_loss=0.3328, val_acc=0.8850\n",
      "Epoch 602: train_loss=0.3382, val_loss=0.3327, val_acc=0.8850\n",
      "Epoch 603: train_loss=0.3384, val_loss=0.3326, val_acc=0.8850\n",
      "Epoch 604: train_loss=0.3399, val_loss=0.3325, val_acc=0.8850\n",
      "Epoch 605: train_loss=0.3383, val_loss=0.3325, val_acc=0.8850\n",
      "Epoch 606: train_loss=0.3414, val_loss=0.3324, val_acc=0.8850\n",
      "Epoch 607: train_loss=0.3389, val_loss=0.3323, val_acc=0.8850\n",
      "Epoch 608: train_loss=0.3398, val_loss=0.3322, val_acc=0.8850\n",
      "Epoch 609: train_loss=0.3392, val_loss=0.3321, val_acc=0.8850\n",
      "Epoch 610: train_loss=0.3403, val_loss=0.3320, val_acc=0.8850\n",
      "Epoch 611: train_loss=0.3380, val_loss=0.3319, val_acc=0.8850\n",
      "Epoch 612: train_loss=0.3392, val_loss=0.3318, val_acc=0.8850\n",
      "Epoch 613: train_loss=0.3404, val_loss=0.3318, val_acc=0.8850\n",
      "Epoch 614: train_loss=0.3399, val_loss=0.3317, val_acc=0.8850\n",
      "Epoch 615: train_loss=0.3354, val_loss=0.3316, val_acc=0.8850\n",
      "Epoch 616: train_loss=0.3378, val_loss=0.3315, val_acc=0.8850\n",
      "Epoch 617: train_loss=0.3401, val_loss=0.3314, val_acc=0.8850\n",
      "Epoch 618: train_loss=0.3360, val_loss=0.3313, val_acc=0.8850\n",
      "Epoch 619: train_loss=0.3403, val_loss=0.3312, val_acc=0.8850\n",
      "Epoch 620: train_loss=0.3353, val_loss=0.3311, val_acc=0.8850\n",
      "Epoch 621: train_loss=0.3378, val_loss=0.3311, val_acc=0.8850\n",
      "Epoch 622: train_loss=0.3351, val_loss=0.3310, val_acc=0.8850\n",
      "Epoch 623: train_loss=0.3358, val_loss=0.3309, val_acc=0.8850\n",
      "Epoch 624: train_loss=0.3370, val_loss=0.3308, val_acc=0.8850\n",
      "Epoch 625: train_loss=0.3379, val_loss=0.3307, val_acc=0.8850\n",
      "Epoch 626: train_loss=0.3376, val_loss=0.3306, val_acc=0.8850\n",
      "Epoch 627: train_loss=0.3365, val_loss=0.3305, val_acc=0.8850\n",
      "Epoch 628: train_loss=0.3361, val_loss=0.3304, val_acc=0.8850\n",
      "Epoch 629: train_loss=0.3344, val_loss=0.3304, val_acc=0.8850\n",
      "Epoch 630: train_loss=0.3360, val_loss=0.3303, val_acc=0.8850\n",
      "Epoch 631: train_loss=0.3357, val_loss=0.3302, val_acc=0.8850\n",
      "Epoch 632: train_loss=0.3355, val_loss=0.3301, val_acc=0.8850\n",
      "Epoch 633: train_loss=0.3362, val_loss=0.3300, val_acc=0.8850\n",
      "Epoch 634: train_loss=0.3359, val_loss=0.3299, val_acc=0.8850\n",
      "Epoch 635: train_loss=0.3349, val_loss=0.3298, val_acc=0.8850\n",
      "Epoch 636: train_loss=0.3342, val_loss=0.3298, val_acc=0.8850\n",
      "Epoch 637: train_loss=0.3350, val_loss=0.3297, val_acc=0.8850\n",
      "Epoch 638: train_loss=0.3362, val_loss=0.3296, val_acc=0.8850\n",
      "Epoch 639: train_loss=0.3332, val_loss=0.3295, val_acc=0.8850\n",
      "Epoch 640: train_loss=0.3360, val_loss=0.3294, val_acc=0.8850\n",
      "Epoch 641: train_loss=0.3343, val_loss=0.3293, val_acc=0.8850\n",
      "Epoch 642: train_loss=0.3380, val_loss=0.3292, val_acc=0.8850\n",
      "Epoch 643: train_loss=0.3351, val_loss=0.3292, val_acc=0.8850\n",
      "Epoch 644: train_loss=0.3368, val_loss=0.3291, val_acc=0.8850\n",
      "Epoch 645: train_loss=0.3321, val_loss=0.3290, val_acc=0.8850\n",
      "Epoch 646: train_loss=0.3332, val_loss=0.3289, val_acc=0.8850\n",
      "Epoch 647: train_loss=0.3327, val_loss=0.3288, val_acc=0.8850\n",
      "Epoch 648: train_loss=0.3346, val_loss=0.3287, val_acc=0.8850\n",
      "Epoch 649: train_loss=0.3325, val_loss=0.3287, val_acc=0.8850\n",
      "Epoch 650: train_loss=0.3355, val_loss=0.3286, val_acc=0.8850\n",
      "Epoch 651: train_loss=0.3344, val_loss=0.3285, val_acc=0.8850\n",
      "Epoch 652: train_loss=0.3335, val_loss=0.3284, val_acc=0.8850\n",
      "Epoch 653: train_loss=0.3335, val_loss=0.3283, val_acc=0.8850\n",
      "Epoch 654: train_loss=0.3321, val_loss=0.3282, val_acc=0.8850\n",
      "Epoch 655: train_loss=0.3341, val_loss=0.3281, val_acc=0.8850\n",
      "Epoch 656: train_loss=0.3347, val_loss=0.3281, val_acc=0.8850\n",
      "Epoch 657: train_loss=0.3332, val_loss=0.3280, val_acc=0.8850\n",
      "Epoch 658: train_loss=0.3337, val_loss=0.3279, val_acc=0.8850\n",
      "Epoch 659: train_loss=0.3344, val_loss=0.3278, val_acc=0.8850\n",
      "Epoch 660: train_loss=0.3293, val_loss=0.3277, val_acc=0.8850\n",
      "Epoch 661: train_loss=0.3329, val_loss=0.3276, val_acc=0.8850\n",
      "Epoch 662: train_loss=0.3330, val_loss=0.3276, val_acc=0.8850\n",
      "Epoch 663: train_loss=0.3338, val_loss=0.3275, val_acc=0.8850\n",
      "Epoch 664: train_loss=0.3316, val_loss=0.3274, val_acc=0.8850\n",
      "Epoch 665: train_loss=0.3301, val_loss=0.3273, val_acc=0.8850\n",
      "Epoch 666: train_loss=0.3306, val_loss=0.3272, val_acc=0.8850\n",
      "Epoch 667: train_loss=0.3321, val_loss=0.3271, val_acc=0.8850\n",
      "Epoch 668: train_loss=0.3319, val_loss=0.3270, val_acc=0.8850\n",
      "Epoch 669: train_loss=0.3306, val_loss=0.3270, val_acc=0.8850\n",
      "Epoch 670: train_loss=0.3317, val_loss=0.3269, val_acc=0.8850\n",
      "Epoch 671: train_loss=0.3316, val_loss=0.3268, val_acc=0.8850\n",
      "Epoch 672: train_loss=0.3332, val_loss=0.3267, val_acc=0.8850\n",
      "Epoch 673: train_loss=0.3352, val_loss=0.3266, val_acc=0.8850\n",
      "Epoch 674: train_loss=0.3324, val_loss=0.3265, val_acc=0.8850\n",
      "Epoch 675: train_loss=0.3304, val_loss=0.3265, val_acc=0.8850\n",
      "Epoch 676: train_loss=0.3303, val_loss=0.3264, val_acc=0.8850\n",
      "Epoch 677: train_loss=0.3306, val_loss=0.3263, val_acc=0.8850\n",
      "Epoch 678: train_loss=0.3302, val_loss=0.3262, val_acc=0.8850\n",
      "Epoch 679: train_loss=0.3301, val_loss=0.3261, val_acc=0.8850\n",
      "Epoch 680: train_loss=0.3319, val_loss=0.3260, val_acc=0.8850\n",
      "Epoch 681: train_loss=0.3317, val_loss=0.3260, val_acc=0.8850\n",
      "Epoch 682: train_loss=0.3320, val_loss=0.3259, val_acc=0.8850\n",
      "Epoch 683: train_loss=0.3284, val_loss=0.3258, val_acc=0.8850\n",
      "Epoch 684: train_loss=0.3295, val_loss=0.3257, val_acc=0.8850\n",
      "Epoch 685: train_loss=0.3305, val_loss=0.3256, val_acc=0.8850\n",
      "Epoch 686: train_loss=0.3289, val_loss=0.3255, val_acc=0.8850\n",
      "Epoch 687: train_loss=0.3314, val_loss=0.3255, val_acc=0.8850\n",
      "Epoch 688: train_loss=0.3311, val_loss=0.3254, val_acc=0.8850\n",
      "Epoch 689: train_loss=0.3308, val_loss=0.3253, val_acc=0.8850\n",
      "Epoch 690: train_loss=0.3282, val_loss=0.3252, val_acc=0.8850\n",
      "Epoch 691: train_loss=0.3280, val_loss=0.3251, val_acc=0.8850\n",
      "Epoch 692: train_loss=0.3299, val_loss=0.3250, val_acc=0.8850\n",
      "Epoch 693: train_loss=0.3287, val_loss=0.3250, val_acc=0.8850\n",
      "Epoch 694: train_loss=0.3284, val_loss=0.3249, val_acc=0.8850\n",
      "Epoch 695: train_loss=0.3302, val_loss=0.3248, val_acc=0.8850\n",
      "Epoch 696: train_loss=0.3282, val_loss=0.3247, val_acc=0.8850\n",
      "Epoch 697: train_loss=0.3308, val_loss=0.3246, val_acc=0.8850\n",
      "Epoch 698: train_loss=0.3299, val_loss=0.3245, val_acc=0.8850\n",
      "Epoch 699: train_loss=0.3301, val_loss=0.3245, val_acc=0.8850\n",
      "Epoch 700: train_loss=0.3272, val_loss=0.3244, val_acc=0.8850\n",
      "Epoch 701: train_loss=0.3303, val_loss=0.3243, val_acc=0.8850\n",
      "Epoch 702: train_loss=0.3273, val_loss=0.3242, val_acc=0.8850\n",
      "Epoch 703: train_loss=0.3265, val_loss=0.3241, val_acc=0.8850\n",
      "Epoch 704: train_loss=0.3281, val_loss=0.3240, val_acc=0.8850\n",
      "Epoch 705: train_loss=0.3293, val_loss=0.3240, val_acc=0.8850\n",
      "Epoch 706: train_loss=0.3262, val_loss=0.3239, val_acc=0.8850\n",
      "Epoch 707: train_loss=0.3291, val_loss=0.3238, val_acc=0.8850\n",
      "Epoch 708: train_loss=0.3269, val_loss=0.3237, val_acc=0.8850\n",
      "Epoch 709: train_loss=0.3276, val_loss=0.3236, val_acc=0.8850\n",
      "Epoch 710: train_loss=0.3271, val_loss=0.3236, val_acc=0.8850\n",
      "Epoch 711: train_loss=0.3259, val_loss=0.3235, val_acc=0.8850\n",
      "Epoch 712: train_loss=0.3303, val_loss=0.3234, val_acc=0.8850\n",
      "Epoch 713: train_loss=0.3266, val_loss=0.3233, val_acc=0.8850\n",
      "Epoch 714: train_loss=0.3279, val_loss=0.3232, val_acc=0.8850\n",
      "Epoch 715: train_loss=0.3281, val_loss=0.3231, val_acc=0.8850\n",
      "Epoch 716: train_loss=0.3271, val_loss=0.3231, val_acc=0.8850\n",
      "Epoch 717: train_loss=0.3271, val_loss=0.3230, val_acc=0.8850\n",
      "Epoch 718: train_loss=0.3254, val_loss=0.3229, val_acc=0.8850\n",
      "Epoch 719: train_loss=0.3278, val_loss=0.3228, val_acc=0.8850\n",
      "Epoch 720: train_loss=0.3252, val_loss=0.3227, val_acc=0.8850\n",
      "Epoch 721: train_loss=0.3260, val_loss=0.3226, val_acc=0.8850\n",
      "Epoch 722: train_loss=0.3261, val_loss=0.3226, val_acc=0.8850\n",
      "Epoch 723: train_loss=0.3257, val_loss=0.3225, val_acc=0.8850\n",
      "Epoch 724: train_loss=0.3235, val_loss=0.3224, val_acc=0.8850\n",
      "Epoch 725: train_loss=0.3240, val_loss=0.3223, val_acc=0.8850\n",
      "Epoch 726: train_loss=0.3253, val_loss=0.3222, val_acc=0.8850\n",
      "Epoch 727: train_loss=0.3259, val_loss=0.3222, val_acc=0.8850\n",
      "Epoch 728: train_loss=0.3252, val_loss=0.3221, val_acc=0.8850\n",
      "Epoch 729: train_loss=0.3257, val_loss=0.3220, val_acc=0.8850\n",
      "Epoch 730: train_loss=0.3274, val_loss=0.3219, val_acc=0.8850\n",
      "Epoch 731: train_loss=0.3241, val_loss=0.3218, val_acc=0.8850\n",
      "Epoch 732: train_loss=0.3229, val_loss=0.3217, val_acc=0.8850\n",
      "Epoch 733: train_loss=0.3232, val_loss=0.3217, val_acc=0.8850\n",
      "Epoch 734: train_loss=0.3260, val_loss=0.3216, val_acc=0.8850\n",
      "Epoch 735: train_loss=0.3265, val_loss=0.3215, val_acc=0.8850\n",
      "Epoch 736: train_loss=0.3233, val_loss=0.3214, val_acc=0.8850\n",
      "Epoch 737: train_loss=0.3238, val_loss=0.3213, val_acc=0.8850\n",
      "Epoch 738: train_loss=0.3233, val_loss=0.3213, val_acc=0.8850\n",
      "Epoch 739: train_loss=0.3248, val_loss=0.3212, val_acc=0.8850\n",
      "Epoch 740: train_loss=0.3262, val_loss=0.3211, val_acc=0.8850\n",
      "Epoch 741: train_loss=0.3253, val_loss=0.3210, val_acc=0.8850\n",
      "Epoch 742: train_loss=0.3247, val_loss=0.3209, val_acc=0.8850\n",
      "Epoch 743: train_loss=0.3253, val_loss=0.3209, val_acc=0.8850\n",
      "Epoch 744: train_loss=0.3265, val_loss=0.3208, val_acc=0.8850\n",
      "Epoch 745: train_loss=0.3238, val_loss=0.3207, val_acc=0.8850\n",
      "Epoch 746: train_loss=0.3258, val_loss=0.3206, val_acc=0.8850\n",
      "Epoch 747: train_loss=0.3278, val_loss=0.3205, val_acc=0.8850\n",
      "Epoch 748: train_loss=0.3233, val_loss=0.3205, val_acc=0.8850\n",
      "Epoch 749: train_loss=0.3226, val_loss=0.3204, val_acc=0.8850\n",
      "Epoch 750: train_loss=0.3235, val_loss=0.3203, val_acc=0.8850\n",
      "Epoch 751: train_loss=0.3242, val_loss=0.3202, val_acc=0.8850\n",
      "Epoch 752: train_loss=0.3200, val_loss=0.3201, val_acc=0.8850\n",
      "Epoch 753: train_loss=0.3218, val_loss=0.3201, val_acc=0.8850\n",
      "Epoch 754: train_loss=0.3215, val_loss=0.3200, val_acc=0.8850\n",
      "Epoch 755: train_loss=0.3225, val_loss=0.3199, val_acc=0.8850\n",
      "Epoch 756: train_loss=0.3187, val_loss=0.3198, val_acc=0.8850\n",
      "Epoch 757: train_loss=0.3243, val_loss=0.3197, val_acc=0.8850\n",
      "Epoch 758: train_loss=0.3232, val_loss=0.3197, val_acc=0.8850\n",
      "Epoch 759: train_loss=0.3233, val_loss=0.3196, val_acc=0.8850\n",
      "Epoch 760: train_loss=0.3239, val_loss=0.3195, val_acc=0.8850\n",
      "Epoch 761: train_loss=0.3222, val_loss=0.3194, val_acc=0.8850\n",
      "Epoch 762: train_loss=0.3199, val_loss=0.3193, val_acc=0.8850\n",
      "Epoch 763: train_loss=0.3195, val_loss=0.3193, val_acc=0.8850\n",
      "Epoch 764: train_loss=0.3215, val_loss=0.3192, val_acc=0.8850\n",
      "Epoch 765: train_loss=0.3241, val_loss=0.3191, val_acc=0.8850\n",
      "Epoch 766: train_loss=0.3227, val_loss=0.3190, val_acc=0.8850\n",
      "Epoch 767: train_loss=0.3228, val_loss=0.3190, val_acc=0.8850\n",
      "Epoch 768: train_loss=0.3216, val_loss=0.3189, val_acc=0.8850\n",
      "Epoch 769: train_loss=0.3209, val_loss=0.3188, val_acc=0.8850\n",
      "Epoch 770: train_loss=0.3214, val_loss=0.3187, val_acc=0.8850\n",
      "Epoch 771: train_loss=0.3236, val_loss=0.3186, val_acc=0.8850\n",
      "Epoch 772: train_loss=0.3225, val_loss=0.3186, val_acc=0.8850\n",
      "Epoch 773: train_loss=0.3228, val_loss=0.3185, val_acc=0.8850\n",
      "Epoch 774: train_loss=0.3243, val_loss=0.3184, val_acc=0.8850\n",
      "Epoch 775: train_loss=0.3201, val_loss=0.3183, val_acc=0.8850\n",
      "Epoch 776: train_loss=0.3209, val_loss=0.3183, val_acc=0.8850\n",
      "Epoch 777: train_loss=0.3261, val_loss=0.3182, val_acc=0.8850\n",
      "Epoch 778: train_loss=0.3203, val_loss=0.3181, val_acc=0.8850\n",
      "Epoch 779: train_loss=0.3193, val_loss=0.3180, val_acc=0.8850\n",
      "Epoch 780: train_loss=0.3219, val_loss=0.3179, val_acc=0.8850\n",
      "Epoch 781: train_loss=0.3208, val_loss=0.3179, val_acc=0.8850\n",
      "Epoch 782: train_loss=0.3210, val_loss=0.3178, val_acc=0.8850\n",
      "Epoch 783: train_loss=0.3185, val_loss=0.3177, val_acc=0.8850\n",
      "Epoch 784: train_loss=0.3198, val_loss=0.3176, val_acc=0.8850\n",
      "Epoch 785: train_loss=0.3185, val_loss=0.3175, val_acc=0.8850\n",
      "Epoch 786: train_loss=0.3216, val_loss=0.3175, val_acc=0.8850\n",
      "Epoch 787: train_loss=0.3196, val_loss=0.3174, val_acc=0.8850\n",
      "Epoch 788: train_loss=0.3205, val_loss=0.3173, val_acc=0.8850\n",
      "Epoch 789: train_loss=0.3178, val_loss=0.3172, val_acc=0.8850\n",
      "Epoch 790: train_loss=0.3201, val_loss=0.3172, val_acc=0.8850\n",
      "Epoch 791: train_loss=0.3194, val_loss=0.3171, val_acc=0.8850\n",
      "Epoch 792: train_loss=0.3202, val_loss=0.3170, val_acc=0.8850\n",
      "Epoch 793: train_loss=0.3188, val_loss=0.3169, val_acc=0.8850\n",
      "Epoch 794: train_loss=0.3211, val_loss=0.3169, val_acc=0.8850\n",
      "Epoch 795: train_loss=0.3199, val_loss=0.3168, val_acc=0.8850\n",
      "Epoch 796: train_loss=0.3193, val_loss=0.3167, val_acc=0.8850\n",
      "Epoch 797: train_loss=0.3183, val_loss=0.3166, val_acc=0.8850\n",
      "Epoch 798: train_loss=0.3204, val_loss=0.3165, val_acc=0.8850\n",
      "Epoch 799: train_loss=0.3153, val_loss=0.3165, val_acc=0.8850\n",
      "Epoch 800: train_loss=0.3180, val_loss=0.3164, val_acc=0.8850\n",
      "Epoch 801: train_loss=0.3190, val_loss=0.3163, val_acc=0.8850\n",
      "Epoch 802: train_loss=0.3173, val_loss=0.3162, val_acc=0.8850\n",
      "Epoch 803: train_loss=0.3214, val_loss=0.3162, val_acc=0.8850\n",
      "Epoch 804: train_loss=0.3165, val_loss=0.3161, val_acc=0.8850\n",
      "Epoch 805: train_loss=0.3200, val_loss=0.3160, val_acc=0.8850\n",
      "Epoch 806: train_loss=0.3181, val_loss=0.3159, val_acc=0.8850\n",
      "Epoch 807: train_loss=0.3189, val_loss=0.3159, val_acc=0.8850\n",
      "Epoch 808: train_loss=0.3216, val_loss=0.3158, val_acc=0.8850\n",
      "Epoch 809: train_loss=0.3150, val_loss=0.3157, val_acc=0.8850\n",
      "Epoch 810: train_loss=0.3173, val_loss=0.3156, val_acc=0.8850\n",
      "Epoch 811: train_loss=0.3181, val_loss=0.3155, val_acc=0.8850\n",
      "Epoch 812: train_loss=0.3164, val_loss=0.3155, val_acc=0.8850\n",
      "Epoch 813: train_loss=0.3181, val_loss=0.3154, val_acc=0.8850\n",
      "Epoch 814: train_loss=0.3161, val_loss=0.3153, val_acc=0.8850\n",
      "Epoch 815: train_loss=0.3157, val_loss=0.3152, val_acc=0.8850\n",
      "Epoch 816: train_loss=0.3152, val_loss=0.3152, val_acc=0.8850\n",
      "Epoch 817: train_loss=0.3165, val_loss=0.3151, val_acc=0.8850\n",
      "Epoch 818: train_loss=0.3145, val_loss=0.3150, val_acc=0.8850\n",
      "Epoch 819: train_loss=0.3168, val_loss=0.3149, val_acc=0.8850\n",
      "Epoch 820: train_loss=0.3174, val_loss=0.3149, val_acc=0.8850\n",
      "Epoch 821: train_loss=0.3170, val_loss=0.3148, val_acc=0.8850\n",
      "Epoch 822: train_loss=0.3159, val_loss=0.3147, val_acc=0.8850\n",
      "Epoch 823: train_loss=0.3153, val_loss=0.3146, val_acc=0.8850\n",
      "Epoch 824: train_loss=0.3162, val_loss=0.3146, val_acc=0.8850\n",
      "Epoch 825: train_loss=0.3149, val_loss=0.3145, val_acc=0.8850\n",
      "Epoch 826: train_loss=0.3142, val_loss=0.3144, val_acc=0.8850\n",
      "Epoch 827: train_loss=0.3154, val_loss=0.3143, val_acc=0.8850\n",
      "Epoch 828: train_loss=0.3187, val_loss=0.3143, val_acc=0.8850\n",
      "Epoch 829: train_loss=0.3166, val_loss=0.3142, val_acc=0.8850\n",
      "Epoch 830: train_loss=0.3140, val_loss=0.3141, val_acc=0.8850\n",
      "Epoch 831: train_loss=0.3169, val_loss=0.3140, val_acc=0.8850\n",
      "Epoch 832: train_loss=0.3157, val_loss=0.3140, val_acc=0.8850\n",
      "Epoch 833: train_loss=0.3160, val_loss=0.3139, val_acc=0.8850\n",
      "Epoch 834: train_loss=0.3151, val_loss=0.3138, val_acc=0.8850\n",
      "Epoch 835: train_loss=0.3152, val_loss=0.3137, val_acc=0.8850\n",
      "Epoch 836: train_loss=0.3146, val_loss=0.3137, val_acc=0.8850\n",
      "Epoch 837: train_loss=0.3150, val_loss=0.3136, val_acc=0.8850\n",
      "Epoch 838: train_loss=0.3158, val_loss=0.3135, val_acc=0.8850\n",
      "Epoch 839: train_loss=0.3156, val_loss=0.3134, val_acc=0.8850\n",
      "Epoch 840: train_loss=0.3123, val_loss=0.3134, val_acc=0.8850\n",
      "Epoch 841: train_loss=0.3138, val_loss=0.3133, val_acc=0.8850\n",
      "Epoch 842: train_loss=0.3158, val_loss=0.3132, val_acc=0.8850\n",
      "Epoch 843: train_loss=0.3151, val_loss=0.3131, val_acc=0.8850\n",
      "Epoch 844: train_loss=0.3149, val_loss=0.3131, val_acc=0.8850\n",
      "Epoch 845: train_loss=0.3151, val_loss=0.3130, val_acc=0.8850\n",
      "Epoch 846: train_loss=0.3136, val_loss=0.3129, val_acc=0.8850\n",
      "Epoch 847: train_loss=0.3140, val_loss=0.3128, val_acc=0.8850\n",
      "Epoch 848: train_loss=0.3153, val_loss=0.3128, val_acc=0.8850\n",
      "Epoch 849: train_loss=0.3155, val_loss=0.3127, val_acc=0.8850\n",
      "Epoch 850: train_loss=0.3129, val_loss=0.3126, val_acc=0.8850\n",
      "Epoch 851: train_loss=0.3145, val_loss=0.3126, val_acc=0.8850\n",
      "Epoch 852: train_loss=0.3115, val_loss=0.3125, val_acc=0.8850\n",
      "Epoch 853: train_loss=0.3139, val_loss=0.3124, val_acc=0.8850\n",
      "Epoch 854: train_loss=0.3161, val_loss=0.3123, val_acc=0.8850\n",
      "Epoch 855: train_loss=0.3164, val_loss=0.3123, val_acc=0.8850\n",
      "Epoch 856: train_loss=0.3146, val_loss=0.3122, val_acc=0.8850\n",
      "Epoch 857: train_loss=0.3123, val_loss=0.3121, val_acc=0.8850\n",
      "Epoch 858: train_loss=0.3152, val_loss=0.3120, val_acc=0.8850\n",
      "Epoch 859: train_loss=0.3130, val_loss=0.3120, val_acc=0.8850\n",
      "Epoch 860: train_loss=0.3117, val_loss=0.3119, val_acc=0.8850\n",
      "Epoch 861: train_loss=0.3118, val_loss=0.3118, val_acc=0.8850\n",
      "Epoch 862: train_loss=0.3122, val_loss=0.3117, val_acc=0.8850\n",
      "Epoch 863: train_loss=0.3134, val_loss=0.3117, val_acc=0.8850\n",
      "Epoch 864: train_loss=0.3127, val_loss=0.3116, val_acc=0.8850\n",
      "Epoch 865: train_loss=0.3109, val_loss=0.3115, val_acc=0.8850\n",
      "Epoch 866: train_loss=0.3115, val_loss=0.3114, val_acc=0.8850\n",
      "Epoch 867: train_loss=0.3148, val_loss=0.3114, val_acc=0.8850\n",
      "Epoch 868: train_loss=0.3121, val_loss=0.3113, val_acc=0.8850\n",
      "Epoch 869: train_loss=0.3117, val_loss=0.3112, val_acc=0.8850\n",
      "Epoch 870: train_loss=0.3071, val_loss=0.3112, val_acc=0.8850\n",
      "Epoch 871: train_loss=0.3144, val_loss=0.3111, val_acc=0.8850\n",
      "Epoch 872: train_loss=0.3114, val_loss=0.3110, val_acc=0.8850\n",
      "Epoch 873: train_loss=0.3130, val_loss=0.3109, val_acc=0.8850\n",
      "Epoch 874: train_loss=0.3110, val_loss=0.3109, val_acc=0.8850\n",
      "Epoch 875: train_loss=0.3109, val_loss=0.3108, val_acc=0.8850\n",
      "Epoch 876: train_loss=0.3104, val_loss=0.3107, val_acc=0.8850\n",
      "Epoch 877: train_loss=0.3108, val_loss=0.3107, val_acc=0.8850\n",
      "Epoch 878: train_loss=0.3144, val_loss=0.3106, val_acc=0.8850\n",
      "Epoch 879: train_loss=0.3107, val_loss=0.3105, val_acc=0.8850\n",
      "Epoch 880: train_loss=0.3106, val_loss=0.3104, val_acc=0.8850\n",
      "Epoch 881: train_loss=0.3132, val_loss=0.3104, val_acc=0.8850\n",
      "Epoch 882: train_loss=0.3110, val_loss=0.3103, val_acc=0.8850\n",
      "Epoch 883: train_loss=0.3129, val_loss=0.3102, val_acc=0.8850\n",
      "Epoch 884: train_loss=0.3105, val_loss=0.3101, val_acc=0.8850\n",
      "Epoch 885: train_loss=0.3102, val_loss=0.3101, val_acc=0.8850\n",
      "Epoch 886: train_loss=0.3106, val_loss=0.3100, val_acc=0.8850\n",
      "Epoch 887: train_loss=0.3117, val_loss=0.3099, val_acc=0.8850\n",
      "Epoch 888: train_loss=0.3112, val_loss=0.3099, val_acc=0.8850\n",
      "Epoch 889: train_loss=0.3093, val_loss=0.3098, val_acc=0.8850\n",
      "Epoch 890: train_loss=0.3109, val_loss=0.3097, val_acc=0.8850\n",
      "Epoch 891: train_loss=0.3077, val_loss=0.3096, val_acc=0.8850\n",
      "Epoch 892: train_loss=0.3085, val_loss=0.3096, val_acc=0.8850\n",
      "Epoch 893: train_loss=0.3084, val_loss=0.3095, val_acc=0.8850\n",
      "Epoch 894: train_loss=0.3092, val_loss=0.3094, val_acc=0.8850\n",
      "Epoch 895: train_loss=0.3093, val_loss=0.3094, val_acc=0.8850\n",
      "Epoch 896: train_loss=0.3078, val_loss=0.3093, val_acc=0.8850\n",
      "Epoch 897: train_loss=0.3085, val_loss=0.3092, val_acc=0.8850\n",
      "Epoch 898: train_loss=0.3094, val_loss=0.3091, val_acc=0.8850\n",
      "Epoch 899: train_loss=0.3073, val_loss=0.3091, val_acc=0.8850\n",
      "Epoch 900: train_loss=0.3113, val_loss=0.3090, val_acc=0.8850\n",
      "Epoch 901: train_loss=0.3072, val_loss=0.3089, val_acc=0.8850\n",
      "Epoch 902: train_loss=0.3078, val_loss=0.3089, val_acc=0.8850\n",
      "Epoch 903: train_loss=0.3073, val_loss=0.3088, val_acc=0.8850\n",
      "Epoch 904: train_loss=0.3058, val_loss=0.3087, val_acc=0.8850\n",
      "Epoch 905: train_loss=0.3093, val_loss=0.3086, val_acc=0.8850\n",
      "Epoch 906: train_loss=0.3082, val_loss=0.3086, val_acc=0.8850\n",
      "Epoch 907: train_loss=0.3107, val_loss=0.3085, val_acc=0.8850\n",
      "Epoch 908: train_loss=0.3086, val_loss=0.3084, val_acc=0.8850\n",
      "Epoch 909: train_loss=0.3061, val_loss=0.3084, val_acc=0.8850\n",
      "Epoch 910: train_loss=0.3111, val_loss=0.3083, val_acc=0.8850\n",
      "Epoch 911: train_loss=0.3060, val_loss=0.3082, val_acc=0.8850\n",
      "Epoch 912: train_loss=0.3079, val_loss=0.3081, val_acc=0.8850\n",
      "Epoch 913: train_loss=0.3081, val_loss=0.3081, val_acc=0.8850\n",
      "Epoch 914: train_loss=0.3090, val_loss=0.3080, val_acc=0.8850\n",
      "Epoch 915: train_loss=0.3077, val_loss=0.3079, val_acc=0.8850\n",
      "Epoch 916: train_loss=0.3095, val_loss=0.3079, val_acc=0.8850\n",
      "Epoch 917: train_loss=0.3076, val_loss=0.3078, val_acc=0.8850\n",
      "Epoch 918: train_loss=0.3076, val_loss=0.3077, val_acc=0.8850\n",
      "Epoch 919: train_loss=0.3079, val_loss=0.3077, val_acc=0.8850\n",
      "Epoch 920: train_loss=0.3068, val_loss=0.3076, val_acc=0.8850\n",
      "Epoch 921: train_loss=0.3081, val_loss=0.3075, val_acc=0.8850\n",
      "Epoch 922: train_loss=0.3064, val_loss=0.3074, val_acc=0.8850\n",
      "Epoch 923: train_loss=0.3069, val_loss=0.3074, val_acc=0.8850\n",
      "Epoch 924: train_loss=0.3046, val_loss=0.3073, val_acc=0.8850\n",
      "Epoch 925: train_loss=0.3055, val_loss=0.3072, val_acc=0.8850\n",
      "Epoch 926: train_loss=0.3043, val_loss=0.3072, val_acc=0.8850\n",
      "Epoch 927: train_loss=0.3088, val_loss=0.3071, val_acc=0.8850\n",
      "Epoch 928: train_loss=0.3060, val_loss=0.3070, val_acc=0.8850\n",
      "Epoch 929: train_loss=0.3101, val_loss=0.3070, val_acc=0.8850\n",
      "Epoch 930: train_loss=0.3064, val_loss=0.3069, val_acc=0.8850\n",
      "Epoch 931: train_loss=0.3074, val_loss=0.3068, val_acc=0.8850\n",
      "Epoch 932: train_loss=0.3052, val_loss=0.3067, val_acc=0.8850\n",
      "Epoch 933: train_loss=0.3031, val_loss=0.3067, val_acc=0.8850\n",
      "Epoch 934: train_loss=0.3061, val_loss=0.3066, val_acc=0.8850\n",
      "Epoch 935: train_loss=0.3047, val_loss=0.3065, val_acc=0.8850\n",
      "Epoch 936: train_loss=0.3075, val_loss=0.3065, val_acc=0.8850\n",
      "Epoch 937: train_loss=0.3038, val_loss=0.3064, val_acc=0.8850\n",
      "Epoch 938: train_loss=0.3056, val_loss=0.3063, val_acc=0.8850\n",
      "Epoch 939: train_loss=0.3046, val_loss=0.3063, val_acc=0.8850\n",
      "Epoch 940: train_loss=0.3083, val_loss=0.3062, val_acc=0.8850\n",
      "Epoch 941: train_loss=0.3021, val_loss=0.3061, val_acc=0.8850\n",
      "Epoch 942: train_loss=0.3024, val_loss=0.3061, val_acc=0.8850\n",
      "Epoch 943: train_loss=0.3056, val_loss=0.3060, val_acc=0.8850\n",
      "Epoch 944: train_loss=0.3041, val_loss=0.3059, val_acc=0.8850\n",
      "Epoch 945: train_loss=0.3040, val_loss=0.3058, val_acc=0.8850\n",
      "Epoch 946: train_loss=0.3033, val_loss=0.3058, val_acc=0.8850\n",
      "Epoch 947: train_loss=0.3032, val_loss=0.3057, val_acc=0.8850\n",
      "Epoch 948: train_loss=0.3057, val_loss=0.3056, val_acc=0.8850\n",
      "Epoch 949: train_loss=0.3037, val_loss=0.3056, val_acc=0.8850\n",
      "Epoch 950: train_loss=0.3045, val_loss=0.3055, val_acc=0.8850\n",
      "Epoch 951: train_loss=0.3045, val_loss=0.3054, val_acc=0.8850\n",
      "Epoch 952: train_loss=0.3042, val_loss=0.3054, val_acc=0.8850\n",
      "Epoch 953: train_loss=0.3049, val_loss=0.3053, val_acc=0.8850\n",
      "Epoch 954: train_loss=0.3044, val_loss=0.3052, val_acc=0.8850\n",
      "Epoch 955: train_loss=0.3035, val_loss=0.3052, val_acc=0.8850\n",
      "Epoch 956: train_loss=0.3029, val_loss=0.3051, val_acc=0.8850\n",
      "Epoch 957: train_loss=0.3027, val_loss=0.3050, val_acc=0.8850\n",
      "Epoch 958: train_loss=0.3044, val_loss=0.3050, val_acc=0.8850\n",
      "Epoch 959: train_loss=0.3045, val_loss=0.3049, val_acc=0.8850\n",
      "Epoch 960: train_loss=0.3046, val_loss=0.3048, val_acc=0.8850\n",
      "Epoch 961: train_loss=0.3039, val_loss=0.3048, val_acc=0.8850\n",
      "Epoch 962: train_loss=0.3050, val_loss=0.3047, val_acc=0.8850\n",
      "Epoch 963: train_loss=0.3037, val_loss=0.3046, val_acc=0.8850\n",
      "Epoch 964: train_loss=0.3025, val_loss=0.3046, val_acc=0.8850\n",
      "Epoch 965: train_loss=0.3040, val_loss=0.3045, val_acc=0.8850\n",
      "Epoch 966: train_loss=0.3042, val_loss=0.3044, val_acc=0.8850\n",
      "Epoch 967: train_loss=0.3054, val_loss=0.3044, val_acc=0.8850\n",
      "Epoch 968: train_loss=0.3017, val_loss=0.3043, val_acc=0.8850\n",
      "Epoch 969: train_loss=0.3030, val_loss=0.3042, val_acc=0.8850\n",
      "Epoch 970: train_loss=0.3029, val_loss=0.3042, val_acc=0.8850\n",
      "Epoch 971: train_loss=0.3028, val_loss=0.3041, val_acc=0.8850\n",
      "Epoch 972: train_loss=0.3016, val_loss=0.3040, val_acc=0.8850\n",
      "Epoch 973: train_loss=0.3043, val_loss=0.3040, val_acc=0.8850\n",
      "Epoch 974: train_loss=0.3031, val_loss=0.3039, val_acc=0.8850\n",
      "Epoch 975: train_loss=0.3022, val_loss=0.3038, val_acc=0.8850\n",
      "Epoch 976: train_loss=0.3008, val_loss=0.3038, val_acc=0.8850\n",
      "Epoch 977: train_loss=0.3008, val_loss=0.3037, val_acc=0.8850\n",
      "Epoch 978: train_loss=0.3025, val_loss=0.3036, val_acc=0.8850\n",
      "Epoch 979: train_loss=0.3039, val_loss=0.3036, val_acc=0.8850\n",
      "Epoch 980: train_loss=0.3034, val_loss=0.3035, val_acc=0.8850\n",
      "Epoch 981: train_loss=0.3032, val_loss=0.3034, val_acc=0.8850\n",
      "Epoch 982: train_loss=0.3013, val_loss=0.3034, val_acc=0.8850\n",
      "Epoch 983: train_loss=0.3021, val_loss=0.3033, val_acc=0.8850\n",
      "Epoch 984: train_loss=0.2997, val_loss=0.3032, val_acc=0.8850\n",
      "Epoch 985: train_loss=0.3012, val_loss=0.3032, val_acc=0.8850\n",
      "Epoch 986: train_loss=0.3011, val_loss=0.3031, val_acc=0.8850\n",
      "Epoch 987: train_loss=0.3011, val_loss=0.3030, val_acc=0.8850\n",
      "Epoch 988: train_loss=0.3009, val_loss=0.3030, val_acc=0.8850\n",
      "Epoch 989: train_loss=0.3038, val_loss=0.3029, val_acc=0.8850\n",
      "Epoch 990: train_loss=0.2973, val_loss=0.3028, val_acc=0.8850\n",
      "Epoch 991: train_loss=0.3023, val_loss=0.3028, val_acc=0.8850\n",
      "Epoch 992: train_loss=0.3026, val_loss=0.3027, val_acc=0.8850\n",
      "Epoch 993: train_loss=0.3010, val_loss=0.3026, val_acc=0.8850\n",
      "Epoch 994: train_loss=0.3016, val_loss=0.3026, val_acc=0.8850\n",
      "Epoch 995: train_loss=0.3038, val_loss=0.3025, val_acc=0.8850\n",
      "Epoch 996: train_loss=0.2991, val_loss=0.3024, val_acc=0.8850\n",
      "Epoch 997: train_loss=0.2987, val_loss=0.3024, val_acc=0.8850\n",
      "Epoch 998: train_loss=0.2989, val_loss=0.3023, val_acc=0.8850\n",
      "Epoch 999: train_loss=0.2988, val_loss=0.3022, val_acc=0.8850\n",
      "Epoch 1000: train_loss=0.2984, val_loss=0.3022, val_acc=0.8850\n",
      "Epoch 1001: train_loss=0.2977, val_loss=0.3021, val_acc=0.8850\n",
      "Epoch 1002: train_loss=0.3005, val_loss=0.3020, val_acc=0.8850\n",
      "Epoch 1003: train_loss=0.3002, val_loss=0.3020, val_acc=0.8850\n",
      "Epoch 1004: train_loss=0.2983, val_loss=0.3019, val_acc=0.8850\n",
      "Epoch 1005: train_loss=0.2983, val_loss=0.3018, val_acc=0.8850\n",
      "Epoch 1006: train_loss=0.2973, val_loss=0.3018, val_acc=0.8850\n",
      "Epoch 1007: train_loss=0.2968, val_loss=0.3017, val_acc=0.8850\n",
      "Epoch 1008: train_loss=0.3003, val_loss=0.3016, val_acc=0.8850\n",
      "Epoch 1009: train_loss=0.3009, val_loss=0.3016, val_acc=0.8850\n",
      "Epoch 1010: train_loss=0.2984, val_loss=0.3015, val_acc=0.8850\n",
      "Epoch 1011: train_loss=0.3003, val_loss=0.3014, val_acc=0.8850\n",
      "Epoch 1012: train_loss=0.2975, val_loss=0.3014, val_acc=0.8850\n",
      "Epoch 1013: train_loss=0.2991, val_loss=0.3013, val_acc=0.8850\n",
      "Epoch 1014: train_loss=0.2986, val_loss=0.3012, val_acc=0.8850\n",
      "Epoch 1015: train_loss=0.2998, val_loss=0.3012, val_acc=0.8850\n",
      "Epoch 1016: train_loss=0.2985, val_loss=0.3011, val_acc=0.8850\n",
      "Epoch 1017: train_loss=0.2992, val_loss=0.3011, val_acc=0.8850\n",
      "Epoch 1018: train_loss=0.2995, val_loss=0.3010, val_acc=0.8850\n",
      "Epoch 1019: train_loss=0.2986, val_loss=0.3009, val_acc=0.8850\n",
      "Epoch 1020: train_loss=0.2972, val_loss=0.3009, val_acc=0.8850\n",
      "Epoch 1021: train_loss=0.2990, val_loss=0.3008, val_acc=0.8850\n",
      "Epoch 1022: train_loss=0.2985, val_loss=0.3007, val_acc=0.8850\n",
      "Epoch 1023: train_loss=0.2985, val_loss=0.3007, val_acc=0.8850\n",
      "Epoch 1024: train_loss=0.2970, val_loss=0.3006, val_acc=0.8850\n",
      "Epoch 1025: train_loss=0.2989, val_loss=0.3005, val_acc=0.8850\n",
      "Epoch 1026: train_loss=0.2997, val_loss=0.3005, val_acc=0.8850\n",
      "Epoch 1027: train_loss=0.2990, val_loss=0.3004, val_acc=0.8850\n",
      "Epoch 1028: train_loss=0.2971, val_loss=0.3003, val_acc=0.8850\n",
      "Epoch 1029: train_loss=0.2960, val_loss=0.3003, val_acc=0.8850\n",
      "Epoch 1030: train_loss=0.2993, val_loss=0.3002, val_acc=0.8850\n",
      "Epoch 1031: train_loss=0.2970, val_loss=0.3001, val_acc=0.8850\n",
      "Epoch 1032: train_loss=0.2976, val_loss=0.3001, val_acc=0.8850\n",
      "Epoch 1033: train_loss=0.2997, val_loss=0.3000, val_acc=0.8850\n",
      "Epoch 1034: train_loss=0.2976, val_loss=0.3000, val_acc=0.8850\n",
      "Epoch 1035: train_loss=0.2931, val_loss=0.2999, val_acc=0.8850\n",
      "Epoch 1036: train_loss=0.2983, val_loss=0.2998, val_acc=0.8850\n",
      "Epoch 1037: train_loss=0.2971, val_loss=0.2998, val_acc=0.8850\n",
      "Epoch 1038: train_loss=0.2991, val_loss=0.2997, val_acc=0.8850\n",
      "Epoch 1039: train_loss=0.2962, val_loss=0.2996, val_acc=0.8850\n",
      "Epoch 1040: train_loss=0.2982, val_loss=0.2996, val_acc=0.8850\n",
      "Epoch 1041: train_loss=0.2934, val_loss=0.2995, val_acc=0.8850\n",
      "Epoch 1042: train_loss=0.2961, val_loss=0.2994, val_acc=0.8850\n",
      "Epoch 1043: train_loss=0.2964, val_loss=0.2994, val_acc=0.8850\n",
      "Epoch 1044: train_loss=0.2963, val_loss=0.2993, val_acc=0.8850\n",
      "Epoch 1045: train_loss=0.2949, val_loss=0.2993, val_acc=0.8850\n",
      "Epoch 1046: train_loss=0.2975, val_loss=0.2992, val_acc=0.8850\n",
      "Epoch 1047: train_loss=0.2941, val_loss=0.2991, val_acc=0.8850\n",
      "Epoch 1048: train_loss=0.2957, val_loss=0.2991, val_acc=0.8850\n",
      "Epoch 1049: train_loss=0.2947, val_loss=0.2990, val_acc=0.8850\n",
      "Epoch 1050: train_loss=0.2952, val_loss=0.2989, val_acc=0.8850\n",
      "Epoch 1051: train_loss=0.2972, val_loss=0.2989, val_acc=0.8850\n",
      "Epoch 1052: train_loss=0.2939, val_loss=0.2988, val_acc=0.8850\n",
      "Epoch 1053: train_loss=0.2961, val_loss=0.2987, val_acc=0.8850\n",
      "Epoch 1054: train_loss=0.2976, val_loss=0.2987, val_acc=0.8850\n",
      "Epoch 1055: train_loss=0.2955, val_loss=0.2986, val_acc=0.8850\n",
      "Epoch 1056: train_loss=0.2969, val_loss=0.2985, val_acc=0.8850\n",
      "Epoch 1057: train_loss=0.2939, val_loss=0.2985, val_acc=0.8850\n",
      "Epoch 1058: train_loss=0.2926, val_loss=0.2984, val_acc=0.8850\n",
      "Epoch 1059: train_loss=0.2955, val_loss=0.2983, val_acc=0.8850\n",
      "Epoch 1060: train_loss=0.2992, val_loss=0.2983, val_acc=0.8850\n",
      "Epoch 1061: train_loss=0.2932, val_loss=0.2982, val_acc=0.8850\n",
      "Epoch 1062: train_loss=0.2954, val_loss=0.2982, val_acc=0.8850\n",
      "Epoch 1063: train_loss=0.2967, val_loss=0.2981, val_acc=0.8850\n",
      "Epoch 1064: train_loss=0.2966, val_loss=0.2980, val_acc=0.8850\n",
      "Epoch 1065: train_loss=0.2952, val_loss=0.2980, val_acc=0.8850\n",
      "Epoch 1066: train_loss=0.2939, val_loss=0.2979, val_acc=0.8850\n",
      "Epoch 1067: train_loss=0.2968, val_loss=0.2978, val_acc=0.8850\n",
      "Epoch 1068: train_loss=0.2952, val_loss=0.2978, val_acc=0.8850\n",
      "Epoch 1069: train_loss=0.2931, val_loss=0.2977, val_acc=0.8850\n",
      "Epoch 1070: train_loss=0.2937, val_loss=0.2977, val_acc=0.8850\n",
      "Epoch 1071: train_loss=0.2950, val_loss=0.2976, val_acc=0.8850\n",
      "Epoch 1072: train_loss=0.2945, val_loss=0.2975, val_acc=0.8850\n",
      "Epoch 1073: train_loss=0.2949, val_loss=0.2975, val_acc=0.8850\n",
      "Epoch 1074: train_loss=0.2946, val_loss=0.2974, val_acc=0.8850\n",
      "Epoch 1075: train_loss=0.2952, val_loss=0.2974, val_acc=0.8850\n",
      "Epoch 1076: train_loss=0.2928, val_loss=0.2973, val_acc=0.8850\n",
      "Epoch 1077: train_loss=0.2923, val_loss=0.2972, val_acc=0.8850\n",
      "Epoch 1078: train_loss=0.2943, val_loss=0.2972, val_acc=0.8850\n",
      "Epoch 1079: train_loss=0.2924, val_loss=0.2971, val_acc=0.8850\n",
      "Epoch 1080: train_loss=0.2947, val_loss=0.2970, val_acc=0.8850\n",
      "Epoch 1081: train_loss=0.2913, val_loss=0.2970, val_acc=0.8850\n",
      "Epoch 1082: train_loss=0.2928, val_loss=0.2969, val_acc=0.8850\n",
      "Epoch 1083: train_loss=0.2908, val_loss=0.2969, val_acc=0.8850\n",
      "Epoch 1084: train_loss=0.2896, val_loss=0.2968, val_acc=0.8850\n",
      "Epoch 1085: train_loss=0.2916, val_loss=0.2967, val_acc=0.8850\n",
      "Epoch 1086: train_loss=0.2914, val_loss=0.2967, val_acc=0.8850\n",
      "Epoch 1087: train_loss=0.2923, val_loss=0.2966, val_acc=0.8850\n",
      "Epoch 1088: train_loss=0.2937, val_loss=0.2966, val_acc=0.8850\n",
      "Epoch 1089: train_loss=0.2914, val_loss=0.2965, val_acc=0.8850\n",
      "Epoch 1090: train_loss=0.2920, val_loss=0.2964, val_acc=0.8850\n",
      "Epoch 1091: train_loss=0.2955, val_loss=0.2964, val_acc=0.8850\n",
      "Epoch 1092: train_loss=0.2924, val_loss=0.2963, val_acc=0.8850\n",
      "Epoch 1093: train_loss=0.2925, val_loss=0.2962, val_acc=0.8850\n",
      "Epoch 1094: train_loss=0.2919, val_loss=0.2962, val_acc=0.8850\n",
      "Epoch 1095: train_loss=0.2972, val_loss=0.2961, val_acc=0.8850\n",
      "Epoch 1096: train_loss=0.2914, val_loss=0.2961, val_acc=0.8850\n",
      "Epoch 1097: train_loss=0.2917, val_loss=0.2960, val_acc=0.8850\n",
      "Epoch 1098: train_loss=0.2928, val_loss=0.2959, val_acc=0.8850\n",
      "Epoch 1099: train_loss=0.2899, val_loss=0.2959, val_acc=0.8850\n",
      "Epoch 1100: train_loss=0.2901, val_loss=0.2958, val_acc=0.8850\n",
      "Epoch 1101: train_loss=0.2916, val_loss=0.2958, val_acc=0.8850\n",
      "Epoch 1102: train_loss=0.2917, val_loss=0.2957, val_acc=0.8850\n",
      "Epoch 1103: train_loss=0.2925, val_loss=0.2956, val_acc=0.8850\n",
      "Epoch 1104: train_loss=0.2951, val_loss=0.2956, val_acc=0.8850\n",
      "Epoch 1105: train_loss=0.2890, val_loss=0.2955, val_acc=0.8850\n",
      "Epoch 1106: train_loss=0.2883, val_loss=0.2955, val_acc=0.8850\n",
      "Epoch 1107: train_loss=0.2926, val_loss=0.2954, val_acc=0.8850\n",
      "Epoch 1108: train_loss=0.2926, val_loss=0.2953, val_acc=0.8850\n",
      "Epoch 1109: train_loss=0.2916, val_loss=0.2953, val_acc=0.8850\n",
      "Epoch 1110: train_loss=0.2917, val_loss=0.2952, val_acc=0.8850\n",
      "Epoch 1111: train_loss=0.2877, val_loss=0.2952, val_acc=0.8850\n",
      "Epoch 1112: train_loss=0.2933, val_loss=0.2951, val_acc=0.8850\n",
      "Epoch 1113: train_loss=0.2923, val_loss=0.2950, val_acc=0.8850\n",
      "Epoch 1114: train_loss=0.2917, val_loss=0.2950, val_acc=0.8850\n",
      "Epoch 1115: train_loss=0.2900, val_loss=0.2949, val_acc=0.8850\n",
      "Epoch 1116: train_loss=0.2894, val_loss=0.2949, val_acc=0.8850\n",
      "Epoch 1117: train_loss=0.2913, val_loss=0.2948, val_acc=0.8850\n",
      "Epoch 1118: train_loss=0.2914, val_loss=0.2947, val_acc=0.8850\n",
      "Epoch 1119: train_loss=0.2877, val_loss=0.2947, val_acc=0.8850\n",
      "Epoch 1120: train_loss=0.2883, val_loss=0.2946, val_acc=0.8850\n",
      "Epoch 1121: train_loss=0.2896, val_loss=0.2946, val_acc=0.8850\n",
      "Epoch 1122: train_loss=0.2887, val_loss=0.2945, val_acc=0.8850\n",
      "Epoch 1123: train_loss=0.2876, val_loss=0.2945, val_acc=0.8850\n",
      "Epoch 1124: train_loss=0.2905, val_loss=0.2944, val_acc=0.8850\n",
      "Epoch 1125: train_loss=0.2920, val_loss=0.2943, val_acc=0.8850\n",
      "Epoch 1126: train_loss=0.2916, val_loss=0.2943, val_acc=0.8850\n",
      "Epoch 1127: train_loss=0.2867, val_loss=0.2942, val_acc=0.8850\n",
      "Epoch 1128: train_loss=0.2906, val_loss=0.2942, val_acc=0.8850\n",
      "Epoch 1129: train_loss=0.2905, val_loss=0.2941, val_acc=0.8850\n",
      "Epoch 1130: train_loss=0.2907, val_loss=0.2940, val_acc=0.8850\n",
      "Epoch 1131: train_loss=0.2919, val_loss=0.2940, val_acc=0.8850\n",
      "Epoch 1132: train_loss=0.2856, val_loss=0.2939, val_acc=0.8850\n",
      "Epoch 1133: train_loss=0.2895, val_loss=0.2939, val_acc=0.8850\n",
      "Epoch 1134: train_loss=0.2905, val_loss=0.2938, val_acc=0.8850\n",
      "Epoch 1135: train_loss=0.2930, val_loss=0.2938, val_acc=0.8850\n",
      "Epoch 1136: train_loss=0.2894, val_loss=0.2937, val_acc=0.8850\n",
      "Epoch 1137: train_loss=0.2889, val_loss=0.2936, val_acc=0.8850\n",
      "Epoch 1138: train_loss=0.2881, val_loss=0.2936, val_acc=0.8850\n",
      "Epoch 1139: train_loss=0.2899, val_loss=0.2935, val_acc=0.8850\n",
      "Epoch 1140: train_loss=0.2903, val_loss=0.2935, val_acc=0.8850\n",
      "Epoch 1141: train_loss=0.2904, val_loss=0.2934, val_acc=0.8850\n",
      "Epoch 1142: train_loss=0.2889, val_loss=0.2934, val_acc=0.8850\n",
      "Epoch 1143: train_loss=0.2887, val_loss=0.2933, val_acc=0.8850\n",
      "Epoch 1144: train_loss=0.2889, val_loss=0.2932, val_acc=0.8850\n",
      "Epoch 1145: train_loss=0.2881, val_loss=0.2932, val_acc=0.8850\n",
      "Epoch 1146: train_loss=0.2911, val_loss=0.2931, val_acc=0.8850\n",
      "Epoch 1147: train_loss=0.2887, val_loss=0.2931, val_acc=0.8850\n",
      "Epoch 1148: train_loss=0.2872, val_loss=0.2930, val_acc=0.8850\n",
      "Epoch 1149: train_loss=0.2888, val_loss=0.2930, val_acc=0.8850\n",
      "Epoch 1150: train_loss=0.2888, val_loss=0.2929, val_acc=0.8850\n",
      "Epoch 1151: train_loss=0.2893, val_loss=0.2928, val_acc=0.8850\n",
      "Epoch 1152: train_loss=0.2882, val_loss=0.2928, val_acc=0.8850\n",
      "Epoch 1153: train_loss=0.2890, val_loss=0.2927, val_acc=0.8850\n",
      "Epoch 1154: train_loss=0.2865, val_loss=0.2927, val_acc=0.8850\n",
      "Epoch 1155: train_loss=0.2869, val_loss=0.2926, val_acc=0.8850\n",
      "Epoch 1156: train_loss=0.2863, val_loss=0.2926, val_acc=0.8850\n",
      "Epoch 1157: train_loss=0.2897, val_loss=0.2925, val_acc=0.8850\n",
      "Epoch 1158: train_loss=0.2904, val_loss=0.2925, val_acc=0.8850\n",
      "Epoch 1159: train_loss=0.2885, val_loss=0.2924, val_acc=0.8850\n",
      "Epoch 1160: train_loss=0.2866, val_loss=0.2923, val_acc=0.8850\n",
      "Epoch 1161: train_loss=0.2868, val_loss=0.2923, val_acc=0.8850\n",
      "Epoch 1162: train_loss=0.2855, val_loss=0.2922, val_acc=0.8850\n",
      "Epoch 1163: train_loss=0.2871, val_loss=0.2922, val_acc=0.8850\n",
      "Epoch 1164: train_loss=0.2865, val_loss=0.2921, val_acc=0.8850\n",
      "Epoch 1165: train_loss=0.2869, val_loss=0.2920, val_acc=0.8850\n",
      "Epoch 1166: train_loss=0.2892, val_loss=0.2920, val_acc=0.8850\n",
      "Epoch 1167: train_loss=0.2826, val_loss=0.2919, val_acc=0.8850\n",
      "Epoch 1168: train_loss=0.2868, val_loss=0.2919, val_acc=0.8850\n",
      "Epoch 1169: train_loss=0.2835, val_loss=0.2918, val_acc=0.8850\n",
      "Epoch 1170: train_loss=0.2847, val_loss=0.2918, val_acc=0.8850\n",
      "Epoch 1171: train_loss=0.2861, val_loss=0.2917, val_acc=0.8850\n",
      "Epoch 1172: train_loss=0.2876, val_loss=0.2917, val_acc=0.8850\n",
      "Epoch 1173: train_loss=0.2816, val_loss=0.2916, val_acc=0.8850\n",
      "Epoch 1174: train_loss=0.2847, val_loss=0.2915, val_acc=0.8850\n",
      "Epoch 1175: train_loss=0.2864, val_loss=0.2915, val_acc=0.8850\n",
      "Epoch 1176: train_loss=0.2875, val_loss=0.2914, val_acc=0.8850\n",
      "Epoch 1177: train_loss=0.2886, val_loss=0.2914, val_acc=0.8850\n",
      "Epoch 1178: train_loss=0.2843, val_loss=0.2913, val_acc=0.8850\n",
      "Epoch 1179: train_loss=0.2862, val_loss=0.2913, val_acc=0.8850\n",
      "Epoch 1180: train_loss=0.2836, val_loss=0.2912, val_acc=0.8850\n",
      "Epoch 1181: train_loss=0.2845, val_loss=0.2912, val_acc=0.8850\n",
      "Epoch 1182: train_loss=0.2815, val_loss=0.2911, val_acc=0.8850\n",
      "Epoch 1183: train_loss=0.2849, val_loss=0.2910, val_acc=0.8850\n",
      "Epoch 1184: train_loss=0.2862, val_loss=0.2910, val_acc=0.8850\n",
      "Epoch 1185: train_loss=0.2862, val_loss=0.2909, val_acc=0.8850\n",
      "Epoch 1186: train_loss=0.2834, val_loss=0.2909, val_acc=0.8850\n",
      "Epoch 1187: train_loss=0.2836, val_loss=0.2908, val_acc=0.8850\n",
      "Epoch 1188: train_loss=0.2866, val_loss=0.2908, val_acc=0.8850\n",
      "Epoch 1189: train_loss=0.2873, val_loss=0.2907, val_acc=0.8850\n",
      "Epoch 1190: train_loss=0.2852, val_loss=0.2907, val_acc=0.8850\n",
      "Epoch 1191: train_loss=0.2866, val_loss=0.2906, val_acc=0.8850\n",
      "Epoch 1192: train_loss=0.2832, val_loss=0.2905, val_acc=0.8850\n",
      "Epoch 1193: train_loss=0.2838, val_loss=0.2905, val_acc=0.8850\n",
      "Epoch 1194: train_loss=0.2832, val_loss=0.2904, val_acc=0.8850\n",
      "Epoch 1195: train_loss=0.2834, val_loss=0.2904, val_acc=0.8850\n",
      "Epoch 1196: train_loss=0.2875, val_loss=0.2903, val_acc=0.8850\n",
      "Epoch 1197: train_loss=0.2834, val_loss=0.2903, val_acc=0.8850\n",
      "Epoch 1198: train_loss=0.2848, val_loss=0.2902, val_acc=0.8850\n",
      "Epoch 1199: train_loss=0.2815, val_loss=0.2902, val_acc=0.8850\n",
      "Epoch 1200: train_loss=0.2812, val_loss=0.2901, val_acc=0.8850\n",
      "Epoch 1201: train_loss=0.2834, val_loss=0.2901, val_acc=0.8850\n",
      "Epoch 1202: train_loss=0.2815, val_loss=0.2900, val_acc=0.8850\n",
      "Epoch 1203: train_loss=0.2839, val_loss=0.2899, val_acc=0.8850\n",
      "Epoch 1204: train_loss=0.2843, val_loss=0.2899, val_acc=0.8850\n",
      "Epoch 1205: train_loss=0.2841, val_loss=0.2898, val_acc=0.8850\n",
      "Epoch 1206: train_loss=0.2806, val_loss=0.2898, val_acc=0.8850\n",
      "Epoch 1207: train_loss=0.2819, val_loss=0.2897, val_acc=0.8850\n",
      "Epoch 1208: train_loss=0.2832, val_loss=0.2897, val_acc=0.8850\n",
      "Epoch 1209: train_loss=0.2860, val_loss=0.2896, val_acc=0.8850\n",
      "Epoch 1210: train_loss=0.2845, val_loss=0.2896, val_acc=0.8850\n",
      "Epoch 1211: train_loss=0.2804, val_loss=0.2895, val_acc=0.8850\n",
      "Epoch 1212: train_loss=0.2835, val_loss=0.2895, val_acc=0.8850\n",
      "Epoch 1213: train_loss=0.2835, val_loss=0.2894, val_acc=0.8850\n",
      "Epoch 1214: train_loss=0.2818, val_loss=0.2893, val_acc=0.8850\n",
      "Epoch 1215: train_loss=0.2824, val_loss=0.2893, val_acc=0.8850\n",
      "Epoch 1216: train_loss=0.2822, val_loss=0.2892, val_acc=0.8850\n",
      "Epoch 1217: train_loss=0.2839, val_loss=0.2892, val_acc=0.8850\n",
      "Epoch 1218: train_loss=0.2818, val_loss=0.2891, val_acc=0.8850\n",
      "Epoch 1219: train_loss=0.2818, val_loss=0.2891, val_acc=0.8850\n",
      "Epoch 1220: train_loss=0.2824, val_loss=0.2890, val_acc=0.8850\n",
      "Epoch 1221: train_loss=0.2853, val_loss=0.2890, val_acc=0.8850\n",
      "Epoch 1222: train_loss=0.2835, val_loss=0.2889, val_acc=0.8850\n",
      "Epoch 1223: train_loss=0.2825, val_loss=0.2889, val_acc=0.8850\n",
      "Epoch 1224: train_loss=0.2835, val_loss=0.2888, val_acc=0.8850\n",
      "Epoch 1225: train_loss=0.2838, val_loss=0.2888, val_acc=0.8850\n",
      "Epoch 1226: train_loss=0.2847, val_loss=0.2887, val_acc=0.8850\n",
      "Epoch 1227: train_loss=0.2832, val_loss=0.2887, val_acc=0.8850\n",
      "Epoch 1228: train_loss=0.2804, val_loss=0.2886, val_acc=0.8850\n",
      "Epoch 1229: train_loss=0.2803, val_loss=0.2886, val_acc=0.8850\n",
      "Epoch 1230: train_loss=0.2823, val_loss=0.2885, val_acc=0.8850\n",
      "Epoch 1231: train_loss=0.2827, val_loss=0.2884, val_acc=0.8850\n",
      "Epoch 1232: train_loss=0.2796, val_loss=0.2884, val_acc=0.8850\n",
      "Epoch 1233: train_loss=0.2809, val_loss=0.2883, val_acc=0.8850\n",
      "Epoch 1234: train_loss=0.2828, val_loss=0.2883, val_acc=0.8850\n",
      "Epoch 1235: train_loss=0.2794, val_loss=0.2882, val_acc=0.8850\n",
      "Epoch 1236: train_loss=0.2812, val_loss=0.2882, val_acc=0.8850\n",
      "Epoch 1237: train_loss=0.2785, val_loss=0.2881, val_acc=0.8850\n",
      "Epoch 1238: train_loss=0.2842, val_loss=0.2881, val_acc=0.8850\n",
      "Epoch 1239: train_loss=0.2808, val_loss=0.2880, val_acc=0.8850\n",
      "Epoch 1240: train_loss=0.2815, val_loss=0.2880, val_acc=0.8850\n",
      "Epoch 1241: train_loss=0.2811, val_loss=0.2879, val_acc=0.8850\n",
      "Epoch 1242: train_loss=0.2820, val_loss=0.2879, val_acc=0.8850\n",
      "Epoch 1243: train_loss=0.2796, val_loss=0.2878, val_acc=0.8850\n",
      "Epoch 1244: train_loss=0.2787, val_loss=0.2878, val_acc=0.8850\n",
      "Epoch 1245: train_loss=0.2816, val_loss=0.2877, val_acc=0.8850\n",
      "Epoch 1246: train_loss=0.2765, val_loss=0.2876, val_acc=0.8850\n",
      "Epoch 1247: train_loss=0.2817, val_loss=0.2876, val_acc=0.8850\n",
      "Epoch 1248: train_loss=0.2804, val_loss=0.2875, val_acc=0.8850\n",
      "Epoch 1249: train_loss=0.2793, val_loss=0.2875, val_acc=0.8850\n",
      "Epoch 1250: train_loss=0.2785, val_loss=0.2874, val_acc=0.8850\n",
      "Epoch 1251: train_loss=0.2814, val_loss=0.2874, val_acc=0.8850\n",
      "Epoch 1252: train_loss=0.2797, val_loss=0.2873, val_acc=0.8850\n",
      "Epoch 1253: train_loss=0.2816, val_loss=0.2873, val_acc=0.8850\n",
      "Epoch 1254: train_loss=0.2799, val_loss=0.2872, val_acc=0.8850\n",
      "Epoch 1255: train_loss=0.2778, val_loss=0.2872, val_acc=0.8850\n",
      "Epoch 1256: train_loss=0.2786, val_loss=0.2871, val_acc=0.8850\n",
      "Epoch 1257: train_loss=0.2792, val_loss=0.2871, val_acc=0.8850\n",
      "Epoch 1258: train_loss=0.2819, val_loss=0.2870, val_acc=0.8850\n",
      "Epoch 1259: train_loss=0.2783, val_loss=0.2870, val_acc=0.8850\n",
      "Epoch 1260: train_loss=0.2789, val_loss=0.2869, val_acc=0.8850\n",
      "Epoch 1261: train_loss=0.2796, val_loss=0.2869, val_acc=0.8850\n",
      "Epoch 1262: train_loss=0.2825, val_loss=0.2868, val_acc=0.8850\n",
      "Epoch 1263: train_loss=0.2785, val_loss=0.2868, val_acc=0.8850\n",
      "Epoch 1264: train_loss=0.2809, val_loss=0.2867, val_acc=0.8850\n",
      "Epoch 1265: train_loss=0.2796, val_loss=0.2867, val_acc=0.8850\n",
      "Epoch 1266: train_loss=0.2778, val_loss=0.2866, val_acc=0.8850\n",
      "Epoch 1267: train_loss=0.2801, val_loss=0.2865, val_acc=0.8850\n",
      "Epoch 1268: train_loss=0.2795, val_loss=0.2865, val_acc=0.8850\n",
      "Epoch 1269: train_loss=0.2808, val_loss=0.2864, val_acc=0.8850\n",
      "Epoch 1270: train_loss=0.2756, val_loss=0.2864, val_acc=0.8850\n",
      "Epoch 1271: train_loss=0.2777, val_loss=0.2863, val_acc=0.8850\n",
      "Epoch 1272: train_loss=0.2793, val_loss=0.2863, val_acc=0.8850\n",
      "Epoch 1273: train_loss=0.2782, val_loss=0.2862, val_acc=0.8850\n",
      "Epoch 1274: train_loss=0.2779, val_loss=0.2862, val_acc=0.8850\n",
      "Epoch 1275: train_loss=0.2799, val_loss=0.2861, val_acc=0.8850\n",
      "Epoch 1276: train_loss=0.2765, val_loss=0.2861, val_acc=0.8850\n",
      "Epoch 1277: train_loss=0.2803, val_loss=0.2860, val_acc=0.8850\n",
      "Epoch 1278: train_loss=0.2745, val_loss=0.2860, val_acc=0.8850\n",
      "Epoch 1279: train_loss=0.2802, val_loss=0.2859, val_acc=0.8850\n",
      "Epoch 1280: train_loss=0.2811, val_loss=0.2859, val_acc=0.8850\n",
      "Epoch 1281: train_loss=0.2792, val_loss=0.2858, val_acc=0.8850\n",
      "Epoch 1282: train_loss=0.2758, val_loss=0.2858, val_acc=0.8850\n",
      "Epoch 1283: train_loss=0.2793, val_loss=0.2857, val_acc=0.8850\n",
      "Epoch 1284: train_loss=0.2789, val_loss=0.2857, val_acc=0.8850\n",
      "Epoch 1285: train_loss=0.2777, val_loss=0.2856, val_acc=0.8850\n",
      "Epoch 1286: train_loss=0.2739, val_loss=0.2856, val_acc=0.8850\n",
      "Epoch 1287: train_loss=0.2761, val_loss=0.2855, val_acc=0.8850\n",
      "Epoch 1288: train_loss=0.2797, val_loss=0.2855, val_acc=0.8850\n",
      "Epoch 1289: train_loss=0.2756, val_loss=0.2854, val_acc=0.8850\n",
      "Epoch 1290: train_loss=0.2792, val_loss=0.2854, val_acc=0.8850\n",
      "Epoch 1291: train_loss=0.2772, val_loss=0.2853, val_acc=0.8850\n",
      "Epoch 1292: train_loss=0.2763, val_loss=0.2853, val_acc=0.8850\n",
      "Epoch 1293: train_loss=0.2792, val_loss=0.2852, val_acc=0.8850\n",
      "Epoch 1294: train_loss=0.2769, val_loss=0.2852, val_acc=0.8850\n",
      "Epoch 1295: train_loss=0.2766, val_loss=0.2851, val_acc=0.8850\n",
      "Epoch 1296: train_loss=0.2786, val_loss=0.2851, val_acc=0.8850\n",
      "Epoch 1297: train_loss=0.2771, val_loss=0.2850, val_acc=0.8850\n",
      "Epoch 1298: train_loss=0.2752, val_loss=0.2850, val_acc=0.8850\n",
      "Epoch 1299: train_loss=0.2774, val_loss=0.2849, val_acc=0.8850\n",
      "Epoch 1300: train_loss=0.2772, val_loss=0.2849, val_acc=0.8850\n",
      "Epoch 1301: train_loss=0.2789, val_loss=0.2848, val_acc=0.8850\n",
      "Epoch 1302: train_loss=0.2763, val_loss=0.2848, val_acc=0.8850\n",
      "Epoch 1303: train_loss=0.2729, val_loss=0.2847, val_acc=0.8850\n",
      "Epoch 1304: train_loss=0.2740, val_loss=0.2847, val_acc=0.8850\n",
      "Epoch 1305: train_loss=0.2769, val_loss=0.2846, val_acc=0.8850\n",
      "Epoch 1306: train_loss=0.2774, val_loss=0.2846, val_acc=0.8850\n",
      "Epoch 1307: train_loss=0.2735, val_loss=0.2845, val_acc=0.8850\n",
      "Epoch 1308: train_loss=0.2786, val_loss=0.2845, val_acc=0.8850\n",
      "Epoch 1309: train_loss=0.2731, val_loss=0.2844, val_acc=0.8850\n",
      "Epoch 1310: train_loss=0.2781, val_loss=0.2844, val_acc=0.8850\n",
      "Epoch 1311: train_loss=0.2766, val_loss=0.2843, val_acc=0.8850\n",
      "Epoch 1312: train_loss=0.2744, val_loss=0.2843, val_acc=0.8850\n",
      "Epoch 1313: train_loss=0.2773, val_loss=0.2842, val_acc=0.8850\n",
      "Epoch 1314: train_loss=0.2736, val_loss=0.2842, val_acc=0.8850\n",
      "Epoch 1315: train_loss=0.2743, val_loss=0.2841, val_acc=0.8850\n",
      "Epoch 1316: train_loss=0.2757, val_loss=0.2841, val_acc=0.8850\n",
      "Epoch 1317: train_loss=0.2769, val_loss=0.2840, val_acc=0.8850\n",
      "Epoch 1318: train_loss=0.2754, val_loss=0.2840, val_acc=0.8850\n",
      "Epoch 1319: train_loss=0.2774, val_loss=0.2839, val_acc=0.8850\n",
      "Epoch 1320: train_loss=0.2734, val_loss=0.2839, val_acc=0.8850\n",
      "Epoch 1321: train_loss=0.2760, val_loss=0.2838, val_acc=0.8850\n",
      "Epoch 1322: train_loss=0.2736, val_loss=0.2838, val_acc=0.8850\n",
      "Epoch 1323: train_loss=0.2791, val_loss=0.2837, val_acc=0.8850\n",
      "Epoch 1324: train_loss=0.2749, val_loss=0.2837, val_acc=0.8850\n",
      "Epoch 1325: train_loss=0.2758, val_loss=0.2836, val_acc=0.8850\n",
      "Epoch 1326: train_loss=0.2715, val_loss=0.2836, val_acc=0.8850\n",
      "Epoch 1327: train_loss=0.2789, val_loss=0.2835, val_acc=0.8850\n",
      "Epoch 1328: train_loss=0.2742, val_loss=0.2835, val_acc=0.8850\n",
      "Epoch 1329: train_loss=0.2738, val_loss=0.2834, val_acc=0.8850\n",
      "Epoch 1330: train_loss=0.2743, val_loss=0.2834, val_acc=0.8850\n",
      "Epoch 1331: train_loss=0.2756, val_loss=0.2833, val_acc=0.8850\n",
      "Epoch 1332: train_loss=0.2738, val_loss=0.2833, val_acc=0.8850\n",
      "Epoch 1333: train_loss=0.2763, val_loss=0.2832, val_acc=0.8850\n",
      "Epoch 1334: train_loss=0.2761, val_loss=0.2832, val_acc=0.8850\n",
      "Epoch 1335: train_loss=0.2749, val_loss=0.2831, val_acc=0.8850\n",
      "Epoch 1336: train_loss=0.2748, val_loss=0.2831, val_acc=0.8850\n",
      "Epoch 1337: train_loss=0.2753, val_loss=0.2830, val_acc=0.8850\n",
      "Epoch 1338: train_loss=0.2747, val_loss=0.2830, val_acc=0.8850\n",
      "Epoch 1339: train_loss=0.2725, val_loss=0.2829, val_acc=0.8850\n",
      "Epoch 1340: train_loss=0.2719, val_loss=0.2829, val_acc=0.8850\n",
      "Epoch 1341: train_loss=0.2753, val_loss=0.2828, val_acc=0.8850\n",
      "Epoch 1342: train_loss=0.2715, val_loss=0.2828, val_acc=0.8850\n",
      "Epoch 1343: train_loss=0.2751, val_loss=0.2827, val_acc=0.8850\n",
      "Epoch 1344: train_loss=0.2732, val_loss=0.2827, val_acc=0.8850\n",
      "Epoch 1345: train_loss=0.2701, val_loss=0.2826, val_acc=0.8850\n",
      "Epoch 1346: train_loss=0.2748, val_loss=0.2826, val_acc=0.8850\n",
      "Epoch 1347: train_loss=0.2725, val_loss=0.2825, val_acc=0.8850\n",
      "Epoch 1348: train_loss=0.2749, val_loss=0.2825, val_acc=0.8850\n",
      "Epoch 1349: train_loss=0.2751, val_loss=0.2825, val_acc=0.8850\n",
      "Epoch 1350: train_loss=0.2739, val_loss=0.2824, val_acc=0.8850\n",
      "Epoch 1351: train_loss=0.2730, val_loss=0.2824, val_acc=0.8850\n",
      "Epoch 1352: train_loss=0.2755, val_loss=0.2823, val_acc=0.8850\n",
      "Epoch 1353: train_loss=0.2753, val_loss=0.2823, val_acc=0.8850\n",
      "Epoch 1354: train_loss=0.2734, val_loss=0.2822, val_acc=0.8850\n",
      "Epoch 1355: train_loss=0.2724, val_loss=0.2822, val_acc=0.8850\n",
      "Epoch 1356: train_loss=0.2738, val_loss=0.2821, val_acc=0.8850\n",
      "Epoch 1357: train_loss=0.2734, val_loss=0.2821, val_acc=0.8850\n",
      "Epoch 1358: train_loss=0.2697, val_loss=0.2820, val_acc=0.8850\n",
      "Epoch 1359: train_loss=0.2759, val_loss=0.2820, val_acc=0.8850\n",
      "Epoch 1360: train_loss=0.2722, val_loss=0.2819, val_acc=0.8850\n",
      "Epoch 1361: train_loss=0.2751, val_loss=0.2819, val_acc=0.8850\n",
      "Epoch 1362: train_loss=0.2745, val_loss=0.2818, val_acc=0.8850\n",
      "Epoch 1363: train_loss=0.2736, val_loss=0.2818, val_acc=0.8850\n",
      "Epoch 1364: train_loss=0.2724, val_loss=0.2817, val_acc=0.8850\n",
      "Epoch 1365: train_loss=0.2716, val_loss=0.2817, val_acc=0.8850\n",
      "Epoch 1366: train_loss=0.2725, val_loss=0.2817, val_acc=0.8850\n",
      "Epoch 1367: train_loss=0.2745, val_loss=0.2816, val_acc=0.8850\n",
      "Epoch 1368: train_loss=0.2708, val_loss=0.2816, val_acc=0.8850\n",
      "Epoch 1369: train_loss=0.2735, val_loss=0.2815, val_acc=0.8850\n",
      "Epoch 1370: train_loss=0.2682, val_loss=0.2815, val_acc=0.8850\n",
      "Epoch 1371: train_loss=0.2723, val_loss=0.2814, val_acc=0.8850\n",
      "Epoch 1372: train_loss=0.2705, val_loss=0.2814, val_acc=0.8850\n",
      "Epoch 1373: train_loss=0.2712, val_loss=0.2813, val_acc=0.8850\n",
      "Epoch 1374: train_loss=0.2730, val_loss=0.2813, val_acc=0.8850\n",
      "Epoch 1375: train_loss=0.2696, val_loss=0.2812, val_acc=0.8850\n",
      "Epoch 1376: train_loss=0.2710, val_loss=0.2812, val_acc=0.8850\n",
      "Epoch 1377: train_loss=0.2728, val_loss=0.2811, val_acc=0.8850\n",
      "Epoch 1378: train_loss=0.2692, val_loss=0.2811, val_acc=0.8850\n",
      "Epoch 1379: train_loss=0.2720, val_loss=0.2810, val_acc=0.8850\n",
      "Epoch 1380: train_loss=0.2703, val_loss=0.2810, val_acc=0.8850\n",
      "Epoch 1381: train_loss=0.2718, val_loss=0.2810, val_acc=0.8850\n",
      "Epoch 1382: train_loss=0.2718, val_loss=0.2809, val_acc=0.8850\n",
      "Epoch 1383: train_loss=0.2705, val_loss=0.2809, val_acc=0.8850\n",
      "Epoch 1384: train_loss=0.2719, val_loss=0.2808, val_acc=0.8850\n",
      "Epoch 1385: train_loss=0.2695, val_loss=0.2808, val_acc=0.8850\n",
      "Epoch 1386: train_loss=0.2705, val_loss=0.2807, val_acc=0.8850\n",
      "Epoch 1387: train_loss=0.2712, val_loss=0.2807, val_acc=0.8850\n",
      "Epoch 1388: train_loss=0.2713, val_loss=0.2806, val_acc=0.8850\n",
      "Epoch 1389: train_loss=0.2719, val_loss=0.2806, val_acc=0.8850\n",
      "Epoch 1390: train_loss=0.2678, val_loss=0.2805, val_acc=0.8850\n",
      "Epoch 1391: train_loss=0.2716, val_loss=0.2805, val_acc=0.8850\n",
      "Epoch 1392: train_loss=0.2694, val_loss=0.2805, val_acc=0.8850\n",
      "Epoch 1393: train_loss=0.2725, val_loss=0.2804, val_acc=0.8850\n",
      "Epoch 1394: train_loss=0.2700, val_loss=0.2804, val_acc=0.8850\n",
      "Epoch 1395: train_loss=0.2715, val_loss=0.2803, val_acc=0.8850\n",
      "Epoch 1396: train_loss=0.2696, val_loss=0.2803, val_acc=0.8850\n",
      "Epoch 1397: train_loss=0.2690, val_loss=0.2802, val_acc=0.8850\n",
      "Epoch 1398: train_loss=0.2677, val_loss=0.2802, val_acc=0.8850\n",
      "Epoch 1399: train_loss=0.2696, val_loss=0.2801, val_acc=0.8850\n",
      "Epoch 1400: train_loss=0.2719, val_loss=0.2801, val_acc=0.8850\n",
      "Epoch 1401: train_loss=0.2719, val_loss=0.2800, val_acc=0.8850\n",
      "Epoch 1402: train_loss=0.2688, val_loss=0.2800, val_acc=0.8850\n",
      "Epoch 1403: train_loss=0.2698, val_loss=0.2800, val_acc=0.8850\n",
      "Epoch 1404: train_loss=0.2717, val_loss=0.2799, val_acc=0.8850\n",
      "Epoch 1405: train_loss=0.2684, val_loss=0.2799, val_acc=0.8850\n",
      "Epoch 1406: train_loss=0.2675, val_loss=0.2798, val_acc=0.8850\n",
      "Epoch 1407: train_loss=0.2696, val_loss=0.2798, val_acc=0.8850\n",
      "Epoch 1408: train_loss=0.2711, val_loss=0.2797, val_acc=0.8850\n",
      "Epoch 1409: train_loss=0.2703, val_loss=0.2797, val_acc=0.8850\n",
      "Epoch 1410: train_loss=0.2704, val_loss=0.2796, val_acc=0.8850\n",
      "Epoch 1411: train_loss=0.2700, val_loss=0.2796, val_acc=0.8850\n",
      "Epoch 1412: train_loss=0.2684, val_loss=0.2796, val_acc=0.8850\n",
      "Epoch 1413: train_loss=0.2666, val_loss=0.2795, val_acc=0.8850\n",
      "Epoch 1414: train_loss=0.2674, val_loss=0.2795, val_acc=0.8850\n",
      "Epoch 1415: train_loss=0.2692, val_loss=0.2794, val_acc=0.8850\n",
      "Epoch 1416: train_loss=0.2686, val_loss=0.2794, val_acc=0.8850\n",
      "Epoch 1417: train_loss=0.2690, val_loss=0.2793, val_acc=0.8850\n",
      "Epoch 1418: train_loss=0.2646, val_loss=0.2793, val_acc=0.8850\n",
      "Epoch 1419: train_loss=0.2699, val_loss=0.2792, val_acc=0.8850\n",
      "Epoch 1420: train_loss=0.2701, val_loss=0.2792, val_acc=0.8850\n",
      "Epoch 1421: train_loss=0.2674, val_loss=0.2792, val_acc=0.8850\n",
      "Epoch 1422: train_loss=0.2699, val_loss=0.2791, val_acc=0.8850\n",
      "Epoch 1423: train_loss=0.2689, val_loss=0.2791, val_acc=0.8850\n",
      "Epoch 1424: train_loss=0.2693, val_loss=0.2790, val_acc=0.8850\n",
      "Epoch 1425: train_loss=0.2638, val_loss=0.2790, val_acc=0.8850\n",
      "Epoch 1426: train_loss=0.2683, val_loss=0.2789, val_acc=0.8850\n",
      "Epoch 1427: train_loss=0.2675, val_loss=0.2789, val_acc=0.8850\n",
      "Epoch 1428: train_loss=0.2684, val_loss=0.2789, val_acc=0.8850\n",
      "Epoch 1429: train_loss=0.2695, val_loss=0.2788, val_acc=0.8850\n",
      "Epoch 1430: train_loss=0.2666, val_loss=0.2788, val_acc=0.8850\n",
      "Epoch 1431: train_loss=0.2688, val_loss=0.2787, val_acc=0.8850\n",
      "Epoch 1432: train_loss=0.2679, val_loss=0.2787, val_acc=0.8850\n",
      "Epoch 1433: train_loss=0.2678, val_loss=0.2786, val_acc=0.8850\n",
      "Epoch 1434: train_loss=0.2685, val_loss=0.2786, val_acc=0.8850\n",
      "Epoch 1435: train_loss=0.2677, val_loss=0.2786, val_acc=0.8850\n",
      "Epoch 1436: train_loss=0.2677, val_loss=0.2785, val_acc=0.8850\n",
      "Epoch 1437: train_loss=0.2661, val_loss=0.2785, val_acc=0.8850\n",
      "Epoch 1438: train_loss=0.2651, val_loss=0.2784, val_acc=0.8850\n",
      "Epoch 1439: train_loss=0.2677, val_loss=0.2784, val_acc=0.8850\n",
      "Epoch 1440: train_loss=0.2662, val_loss=0.2783, val_acc=0.8850\n",
      "Epoch 1441: train_loss=0.2685, val_loss=0.2783, val_acc=0.8850\n",
      "Epoch 1442: train_loss=0.2671, val_loss=0.2783, val_acc=0.8850\n",
      "Epoch 1443: train_loss=0.2686, val_loss=0.2782, val_acc=0.8850\n",
      "Epoch 1444: train_loss=0.2696, val_loss=0.2782, val_acc=0.8850\n",
      "Epoch 1445: train_loss=0.2675, val_loss=0.2781, val_acc=0.8850\n",
      "Epoch 1446: train_loss=0.2648, val_loss=0.2781, val_acc=0.8850\n",
      "Epoch 1447: train_loss=0.2652, val_loss=0.2780, val_acc=0.8850\n",
      "Epoch 1448: train_loss=0.2686, val_loss=0.2780, val_acc=0.8850\n",
      "Epoch 1449: train_loss=0.2665, val_loss=0.2780, val_acc=0.8850\n",
      "Epoch 1450: train_loss=0.2657, val_loss=0.2779, val_acc=0.8850\n",
      "Epoch 1451: train_loss=0.2665, val_loss=0.2779, val_acc=0.8850\n",
      "Epoch 1452: train_loss=0.2684, val_loss=0.2778, val_acc=0.8850\n",
      "Epoch 1453: train_loss=0.2677, val_loss=0.2778, val_acc=0.8850\n",
      "Epoch 1454: train_loss=0.2666, val_loss=0.2777, val_acc=0.8850\n",
      "Epoch 1455: train_loss=0.2663, val_loss=0.2777, val_acc=0.8850\n",
      "Epoch 1456: train_loss=0.2670, val_loss=0.2777, val_acc=0.8850\n",
      "Epoch 1457: train_loss=0.2681, val_loss=0.2776, val_acc=0.8850\n",
      "Epoch 1458: train_loss=0.2684, val_loss=0.2776, val_acc=0.8850\n",
      "Epoch 1459: train_loss=0.2638, val_loss=0.2775, val_acc=0.8850\n",
      "Epoch 1460: train_loss=0.2635, val_loss=0.2775, val_acc=0.8850\n",
      "Epoch 1461: train_loss=0.2657, val_loss=0.2775, val_acc=0.8850\n",
      "Epoch 1462: train_loss=0.2677, val_loss=0.2774, val_acc=0.8864\n",
      "Epoch 1463: train_loss=0.2678, val_loss=0.2774, val_acc=0.8850\n",
      "Epoch 1464: train_loss=0.2689, val_loss=0.2773, val_acc=0.8850\n",
      "Epoch 1465: train_loss=0.2629, val_loss=0.2773, val_acc=0.8850\n",
      "Epoch 1466: train_loss=0.2653, val_loss=0.2773, val_acc=0.8850\n",
      "Epoch 1467: train_loss=0.2674, val_loss=0.2772, val_acc=0.8850\n",
      "Epoch 1468: train_loss=0.2636, val_loss=0.2772, val_acc=0.8850\n",
      "Epoch 1469: train_loss=0.2637, val_loss=0.2771, val_acc=0.8850\n",
      "Epoch 1470: train_loss=0.2618, val_loss=0.2771, val_acc=0.8850\n",
      "Epoch 1471: train_loss=0.2620, val_loss=0.2770, val_acc=0.8850\n",
      "Epoch 1472: train_loss=0.2685, val_loss=0.2770, val_acc=0.8850\n",
      "Epoch 1473: train_loss=0.2666, val_loss=0.2770, val_acc=0.8850\n",
      "Epoch 1474: train_loss=0.2654, val_loss=0.2769, val_acc=0.8850\n",
      "Epoch 1475: train_loss=0.2672, val_loss=0.2769, val_acc=0.8850\n",
      "Epoch 1476: train_loss=0.2672, val_loss=0.2768, val_acc=0.8850\n",
      "Epoch 1477: train_loss=0.2658, val_loss=0.2768, val_acc=0.8850\n",
      "Epoch 1478: train_loss=0.2642, val_loss=0.2768, val_acc=0.8850\n",
      "Epoch 1479: train_loss=0.2652, val_loss=0.2767, val_acc=0.8850\n",
      "Epoch 1480: train_loss=0.2632, val_loss=0.2767, val_acc=0.8850\n",
      "Epoch 1481: train_loss=0.2651, val_loss=0.2766, val_acc=0.8850\n",
      "Epoch 1482: train_loss=0.2656, val_loss=0.2766, val_acc=0.8850\n",
      "Epoch 1483: train_loss=0.2671, val_loss=0.2766, val_acc=0.8850\n",
      "Epoch 1484: train_loss=0.2655, val_loss=0.2765, val_acc=0.8850\n",
      "Epoch 1485: train_loss=0.2652, val_loss=0.2765, val_acc=0.8850\n",
      "Epoch 1486: train_loss=0.2601, val_loss=0.2765, val_acc=0.8850\n",
      "Epoch 1487: train_loss=0.2653, val_loss=0.2764, val_acc=0.8850\n",
      "Epoch 1488: train_loss=0.2691, val_loss=0.2764, val_acc=0.8850\n",
      "Epoch 1489: train_loss=0.2648, val_loss=0.2763, val_acc=0.8850\n",
      "Epoch 1490: train_loss=0.2657, val_loss=0.2763, val_acc=0.8850\n",
      "Epoch 1491: train_loss=0.2637, val_loss=0.2763, val_acc=0.8850\n",
      "Epoch 1492: train_loss=0.2633, val_loss=0.2762, val_acc=0.8850\n",
      "Epoch 1493: train_loss=0.2656, val_loss=0.2762, val_acc=0.8850\n",
      "Epoch 1494: train_loss=0.2654, val_loss=0.2761, val_acc=0.8850\n",
      "Epoch 1495: train_loss=0.2645, val_loss=0.2761, val_acc=0.8850\n",
      "Epoch 1496: train_loss=0.2657, val_loss=0.2761, val_acc=0.8850\n",
      "Epoch 1497: train_loss=0.2659, val_loss=0.2760, val_acc=0.8850\n",
      "Epoch 1498: train_loss=0.2642, val_loss=0.2760, val_acc=0.8850\n",
      "Epoch 1499: train_loss=0.2650, val_loss=0.2759, val_acc=0.8850\n",
      "Epoch 1500: train_loss=0.2607, val_loss=0.2759, val_acc=0.8850\n",
      "Epoch 1501: train_loss=0.2625, val_loss=0.2759, val_acc=0.8850\n",
      "Epoch 1502: train_loss=0.2618, val_loss=0.2758, val_acc=0.8850\n",
      "Epoch 1503: train_loss=0.2624, val_loss=0.2758, val_acc=0.8850\n",
      "Epoch 1504: train_loss=0.2596, val_loss=0.2757, val_acc=0.8850\n",
      "Epoch 1505: train_loss=0.2623, val_loss=0.2757, val_acc=0.8850\n",
      "Epoch 1506: train_loss=0.2638, val_loss=0.2757, val_acc=0.8850\n",
      "Epoch 1507: train_loss=0.2659, val_loss=0.2756, val_acc=0.8850\n",
      "Epoch 1508: train_loss=0.2646, val_loss=0.2756, val_acc=0.8850\n",
      "Epoch 1509: train_loss=0.2646, val_loss=0.2755, val_acc=0.8850\n",
      "Epoch 1510: train_loss=0.2646, val_loss=0.2755, val_acc=0.8850\n",
      "Epoch 1511: train_loss=0.2624, val_loss=0.2755, val_acc=0.8850\n",
      "Epoch 1512: train_loss=0.2630, val_loss=0.2754, val_acc=0.8850\n",
      "Epoch 1513: train_loss=0.2616, val_loss=0.2754, val_acc=0.8850\n",
      "Epoch 1514: train_loss=0.2626, val_loss=0.2754, val_acc=0.8850\n",
      "Epoch 1515: train_loss=0.2626, val_loss=0.2753, val_acc=0.8850\n",
      "Epoch 1516: train_loss=0.2650, val_loss=0.2753, val_acc=0.8850\n",
      "Epoch 1517: train_loss=0.2613, val_loss=0.2752, val_acc=0.8850\n",
      "Epoch 1518: train_loss=0.2606, val_loss=0.2752, val_acc=0.8850\n",
      "Epoch 1519: train_loss=0.2601, val_loss=0.2752, val_acc=0.8850\n",
      "Epoch 1520: train_loss=0.2637, val_loss=0.2751, val_acc=0.8864\n",
      "Epoch 1521: train_loss=0.2608, val_loss=0.2751, val_acc=0.8864\n",
      "Epoch 1522: train_loss=0.2642, val_loss=0.2750, val_acc=0.8879\n",
      "Epoch 1523: train_loss=0.2630, val_loss=0.2750, val_acc=0.8879\n",
      "Epoch 1524: train_loss=0.2665, val_loss=0.2750, val_acc=0.8879\n",
      "Epoch 1525: train_loss=0.2601, val_loss=0.2749, val_acc=0.8879\n",
      "Epoch 1526: train_loss=0.2625, val_loss=0.2749, val_acc=0.8879\n",
      "Epoch 1527: train_loss=0.2641, val_loss=0.2749, val_acc=0.8879\n",
      "Epoch 1528: train_loss=0.2633, val_loss=0.2748, val_acc=0.8879\n",
      "Epoch 1529: train_loss=0.2612, val_loss=0.2748, val_acc=0.8879\n",
      "Epoch 1530: train_loss=0.2607, val_loss=0.2747, val_acc=0.8879\n",
      "Epoch 1531: train_loss=0.2624, val_loss=0.2747, val_acc=0.8879\n",
      "Epoch 1532: train_loss=0.2632, val_loss=0.2747, val_acc=0.8879\n",
      "Epoch 1533: train_loss=0.2616, val_loss=0.2746, val_acc=0.8879\n",
      "Epoch 1534: train_loss=0.2627, val_loss=0.2746, val_acc=0.8879\n",
      "Epoch 1535: train_loss=0.2616, val_loss=0.2745, val_acc=0.8879\n",
      "Epoch 1536: train_loss=0.2599, val_loss=0.2745, val_acc=0.8879\n",
      "Epoch 1537: train_loss=0.2604, val_loss=0.2745, val_acc=0.8879\n",
      "Epoch 1538: train_loss=0.2630, val_loss=0.2744, val_acc=0.8879\n",
      "Epoch 1539: train_loss=0.2652, val_loss=0.2744, val_acc=0.8879\n",
      "Epoch 1540: train_loss=0.2617, val_loss=0.2744, val_acc=0.8879\n",
      "Epoch 1541: train_loss=0.2612, val_loss=0.2743, val_acc=0.8879\n",
      "Epoch 1542: train_loss=0.2588, val_loss=0.2743, val_acc=0.8879\n",
      "Epoch 1543: train_loss=0.2639, val_loss=0.2743, val_acc=0.8879\n",
      "Epoch 1544: train_loss=0.2597, val_loss=0.2742, val_acc=0.8879\n",
      "Epoch 1545: train_loss=0.2621, val_loss=0.2742, val_acc=0.8879\n",
      "Epoch 1546: train_loss=0.2598, val_loss=0.2741, val_acc=0.8879\n",
      "Epoch 1547: train_loss=0.2612, val_loss=0.2741, val_acc=0.8879\n",
      "Epoch 1548: train_loss=0.2626, val_loss=0.2741, val_acc=0.8879\n",
      "Epoch 1549: train_loss=0.2571, val_loss=0.2740, val_acc=0.8879\n",
      "Epoch 1550: train_loss=0.2588, val_loss=0.2740, val_acc=0.8879\n",
      "Epoch 1551: train_loss=0.2613, val_loss=0.2740, val_acc=0.8879\n",
      "Epoch 1552: train_loss=0.2595, val_loss=0.2739, val_acc=0.8879\n",
      "Epoch 1553: train_loss=0.2612, val_loss=0.2739, val_acc=0.8879\n",
      "Epoch 1554: train_loss=0.2617, val_loss=0.2738, val_acc=0.8879\n",
      "Epoch 1555: train_loss=0.2604, val_loss=0.2738, val_acc=0.8879\n",
      "Epoch 1556: train_loss=0.2612, val_loss=0.2738, val_acc=0.8879\n",
      "Epoch 1557: train_loss=0.2599, val_loss=0.2737, val_acc=0.8879\n",
      "Epoch 1558: train_loss=0.2612, val_loss=0.2737, val_acc=0.8879\n",
      "Epoch 1559: train_loss=0.2603, val_loss=0.2737, val_acc=0.8879\n",
      "Epoch 1560: train_loss=0.2633, val_loss=0.2736, val_acc=0.8879\n",
      "Epoch 1561: train_loss=0.2612, val_loss=0.2736, val_acc=0.8879\n",
      "Epoch 1562: train_loss=0.2618, val_loss=0.2735, val_acc=0.8879\n",
      "Epoch 1563: train_loss=0.2621, val_loss=0.2735, val_acc=0.8879\n",
      "Epoch 1564: train_loss=0.2576, val_loss=0.2735, val_acc=0.8879\n",
      "Epoch 1565: train_loss=0.2610, val_loss=0.2734, val_acc=0.8879\n",
      "Epoch 1566: train_loss=0.2617, val_loss=0.2734, val_acc=0.8879\n",
      "Epoch 1567: train_loss=0.2631, val_loss=0.2734, val_acc=0.8879\n",
      "Epoch 1568: train_loss=0.2606, val_loss=0.2733, val_acc=0.8879\n",
      "Epoch 1569: train_loss=0.2615, val_loss=0.2733, val_acc=0.8879\n",
      "Epoch 1570: train_loss=0.2591, val_loss=0.2733, val_acc=0.8879\n",
      "Epoch 1571: train_loss=0.2596, val_loss=0.2732, val_acc=0.8879\n",
      "Epoch 1572: train_loss=0.2609, val_loss=0.2732, val_acc=0.8879\n",
      "Epoch 1573: train_loss=0.2580, val_loss=0.2732, val_acc=0.8879\n",
      "Epoch 1574: train_loss=0.2613, val_loss=0.2731, val_acc=0.8879\n",
      "Epoch 1575: train_loss=0.2608, val_loss=0.2731, val_acc=0.8879\n",
      "Epoch 1576: train_loss=0.2599, val_loss=0.2731, val_acc=0.8879\n",
      "Epoch 1577: train_loss=0.2592, val_loss=0.2730, val_acc=0.8879\n",
      "Epoch 1578: train_loss=0.2615, val_loss=0.2730, val_acc=0.8879\n",
      "Epoch 1579: train_loss=0.2601, val_loss=0.2729, val_acc=0.8879\n",
      "Epoch 1580: train_loss=0.2621, val_loss=0.2729, val_acc=0.8879\n",
      "Epoch 1581: train_loss=0.2555, val_loss=0.2729, val_acc=0.8864\n",
      "Epoch 1582: train_loss=0.2617, val_loss=0.2728, val_acc=0.8864\n",
      "Epoch 1583: train_loss=0.2590, val_loss=0.2728, val_acc=0.8864\n",
      "Epoch 1584: train_loss=0.2584, val_loss=0.2728, val_acc=0.8864\n",
      "Epoch 1585: train_loss=0.2634, val_loss=0.2727, val_acc=0.8864\n",
      "Epoch 1586: train_loss=0.2577, val_loss=0.2727, val_acc=0.8850\n",
      "Epoch 1587: train_loss=0.2594, val_loss=0.2727, val_acc=0.8850\n",
      "Epoch 1588: train_loss=0.2608, val_loss=0.2726, val_acc=0.8850\n",
      "Epoch 1589: train_loss=0.2618, val_loss=0.2726, val_acc=0.8850\n",
      "Epoch 1590: train_loss=0.2629, val_loss=0.2726, val_acc=0.8850\n",
      "Epoch 1591: train_loss=0.2631, val_loss=0.2725, val_acc=0.8850\n",
      "Epoch 1592: train_loss=0.2603, val_loss=0.2725, val_acc=0.8850\n",
      "Epoch 1593: train_loss=0.2586, val_loss=0.2725, val_acc=0.8850\n",
      "Epoch 1594: train_loss=0.2574, val_loss=0.2724, val_acc=0.8850\n",
      "Epoch 1595: train_loss=0.2593, val_loss=0.2724, val_acc=0.8850\n",
      "Epoch 1596: train_loss=0.2578, val_loss=0.2724, val_acc=0.8850\n",
      "Epoch 1597: train_loss=0.2576, val_loss=0.2723, val_acc=0.8850\n",
      "Epoch 1598: train_loss=0.2604, val_loss=0.2723, val_acc=0.8850\n",
      "Epoch 1599: train_loss=0.2581, val_loss=0.2723, val_acc=0.8850\n",
      "Epoch 1600: train_loss=0.2579, val_loss=0.2722, val_acc=0.8850\n",
      "Epoch 1601: train_loss=0.2589, val_loss=0.2722, val_acc=0.8850\n",
      "Epoch 1602: train_loss=0.2615, val_loss=0.2722, val_acc=0.8850\n",
      "Epoch 1603: train_loss=0.2617, val_loss=0.2721, val_acc=0.8864\n",
      "Epoch 1604: train_loss=0.2586, val_loss=0.2721, val_acc=0.8864\n",
      "Epoch 1605: train_loss=0.2600, val_loss=0.2721, val_acc=0.8864\n",
      "Epoch 1606: train_loss=0.2588, val_loss=0.2720, val_acc=0.8864\n",
      "Epoch 1607: train_loss=0.2566, val_loss=0.2720, val_acc=0.8864\n",
      "Epoch 1608: train_loss=0.2559, val_loss=0.2720, val_acc=0.8864\n",
      "Epoch 1609: train_loss=0.2570, val_loss=0.2719, val_acc=0.8864\n",
      "Epoch 1610: train_loss=0.2608, val_loss=0.2719, val_acc=0.8864\n",
      "Epoch 1611: train_loss=0.2617, val_loss=0.2719, val_acc=0.8864\n",
      "Epoch 1612: train_loss=0.2536, val_loss=0.2718, val_acc=0.8864\n",
      "Epoch 1613: train_loss=0.2600, val_loss=0.2718, val_acc=0.8864\n",
      "Epoch 1614: train_loss=0.2568, val_loss=0.2718, val_acc=0.8864\n",
      "Epoch 1615: train_loss=0.2625, val_loss=0.2717, val_acc=0.8864\n",
      "Epoch 1616: train_loss=0.2591, val_loss=0.2717, val_acc=0.8864\n",
      "Epoch 1617: train_loss=0.2554, val_loss=0.2717, val_acc=0.8864\n",
      "Epoch 1618: train_loss=0.2597, val_loss=0.2716, val_acc=0.8864\n",
      "Epoch 1619: train_loss=0.2570, val_loss=0.2716, val_acc=0.8864\n",
      "Epoch 1620: train_loss=0.2573, val_loss=0.2716, val_acc=0.8864\n",
      "Epoch 1621: train_loss=0.2591, val_loss=0.2715, val_acc=0.8864\n",
      "Epoch 1622: train_loss=0.2560, val_loss=0.2715, val_acc=0.8864\n",
      "Epoch 1623: train_loss=0.2528, val_loss=0.2715, val_acc=0.8864\n",
      "Epoch 1624: train_loss=0.2579, val_loss=0.2715, val_acc=0.8864\n",
      "Epoch 1625: train_loss=0.2587, val_loss=0.2714, val_acc=0.8864\n",
      "Epoch 1626: train_loss=0.2593, val_loss=0.2714, val_acc=0.8864\n",
      "Epoch 1627: train_loss=0.2607, val_loss=0.2714, val_acc=0.8864\n",
      "Epoch 1628: train_loss=0.2550, val_loss=0.2713, val_acc=0.8879\n",
      "Epoch 1629: train_loss=0.2587, val_loss=0.2713, val_acc=0.8879\n",
      "Epoch 1630: train_loss=0.2560, val_loss=0.2713, val_acc=0.8879\n",
      "Epoch 1631: train_loss=0.2578, val_loss=0.2712, val_acc=0.8879\n",
      "Epoch 1632: train_loss=0.2583, val_loss=0.2712, val_acc=0.8879\n",
      "Epoch 1633: train_loss=0.2553, val_loss=0.2712, val_acc=0.8879\n",
      "Epoch 1634: train_loss=0.2564, val_loss=0.2711, val_acc=0.8879\n",
      "Epoch 1635: train_loss=0.2552, val_loss=0.2711, val_acc=0.8879\n",
      "Epoch 1636: train_loss=0.2567, val_loss=0.2711, val_acc=0.8879\n",
      "Epoch 1637: train_loss=0.2555, val_loss=0.2711, val_acc=0.8879\n",
      "Epoch 1638: train_loss=0.2555, val_loss=0.2710, val_acc=0.8879\n",
      "Epoch 1639: train_loss=0.2589, val_loss=0.2710, val_acc=0.8864\n",
      "Epoch 1640: train_loss=0.2587, val_loss=0.2710, val_acc=0.8864\n",
      "Epoch 1641: train_loss=0.2563, val_loss=0.2709, val_acc=0.8864\n",
      "Epoch 1642: train_loss=0.2562, val_loss=0.2709, val_acc=0.8864\n",
      "Epoch 1643: train_loss=0.2535, val_loss=0.2709, val_acc=0.8864\n",
      "Epoch 1644: train_loss=0.2586, val_loss=0.2708, val_acc=0.8864\n",
      "Epoch 1645: train_loss=0.2531, val_loss=0.2708, val_acc=0.8864\n",
      "Epoch 1646: train_loss=0.2550, val_loss=0.2708, val_acc=0.8864\n",
      "Epoch 1647: train_loss=0.2560, val_loss=0.2707, val_acc=0.8864\n",
      "Epoch 1648: train_loss=0.2570, val_loss=0.2707, val_acc=0.8864\n",
      "Epoch 1649: train_loss=0.2526, val_loss=0.2707, val_acc=0.8864\n",
      "Epoch 1650: train_loss=0.2557, val_loss=0.2707, val_acc=0.8864\n",
      "Epoch 1651: train_loss=0.2547, val_loss=0.2706, val_acc=0.8864\n",
      "Epoch 1652: train_loss=0.2565, val_loss=0.2706, val_acc=0.8864\n",
      "Epoch 1653: train_loss=0.2539, val_loss=0.2706, val_acc=0.8864\n",
      "Epoch 1654: train_loss=0.2555, val_loss=0.2705, val_acc=0.8864\n",
      "Epoch 1655: train_loss=0.2550, val_loss=0.2705, val_acc=0.8864\n",
      "Epoch 1656: train_loss=0.2512, val_loss=0.2705, val_acc=0.8850\n",
      "Epoch 1657: train_loss=0.2540, val_loss=0.2704, val_acc=0.8850\n",
      "Epoch 1658: train_loss=0.2561, val_loss=0.2704, val_acc=0.8850\n",
      "Epoch 1659: train_loss=0.2566, val_loss=0.2704, val_acc=0.8850\n",
      "Epoch 1660: train_loss=0.2553, val_loss=0.2704, val_acc=0.8850\n",
      "Epoch 1661: train_loss=0.2573, val_loss=0.2703, val_acc=0.8850\n",
      "Epoch 1662: train_loss=0.2565, val_loss=0.2703, val_acc=0.8850\n",
      "Epoch 1663: train_loss=0.2566, val_loss=0.2703, val_acc=0.8850\n",
      "Epoch 1664: train_loss=0.2555, val_loss=0.2702, val_acc=0.8850\n",
      "Epoch 1665: train_loss=0.2570, val_loss=0.2702, val_acc=0.8850\n",
      "Epoch 1666: train_loss=0.2571, val_loss=0.2702, val_acc=0.8850\n",
      "Epoch 1667: train_loss=0.2557, val_loss=0.2701, val_acc=0.8850\n",
      "Epoch 1668: train_loss=0.2592, val_loss=0.2701, val_acc=0.8850\n",
      "Epoch 1669: train_loss=0.2553, val_loss=0.2701, val_acc=0.8850\n",
      "Epoch 1670: train_loss=0.2552, val_loss=0.2701, val_acc=0.8850\n",
      "Epoch 1671: train_loss=0.2564, val_loss=0.2700, val_acc=0.8850\n",
      "Epoch 1672: train_loss=0.2559, val_loss=0.2700, val_acc=0.8850\n",
      "Epoch 1673: train_loss=0.2545, val_loss=0.2700, val_acc=0.8850\n",
      "Epoch 1674: train_loss=0.2571, val_loss=0.2699, val_acc=0.8850\n",
      "Epoch 1675: train_loss=0.2544, val_loss=0.2699, val_acc=0.8850\n",
      "Epoch 1676: train_loss=0.2565, val_loss=0.2699, val_acc=0.8850\n",
      "Epoch 1677: train_loss=0.2541, val_loss=0.2699, val_acc=0.8850\n",
      "Epoch 1678: train_loss=0.2537, val_loss=0.2698, val_acc=0.8850\n",
      "Epoch 1679: train_loss=0.2537, val_loss=0.2698, val_acc=0.8850\n",
      "Epoch 1680: train_loss=0.2544, val_loss=0.2698, val_acc=0.8850\n",
      "Epoch 1681: train_loss=0.2573, val_loss=0.2697, val_acc=0.8850\n",
      "Epoch 1682: train_loss=0.2546, val_loss=0.2697, val_acc=0.8850\n",
      "Epoch 1683: train_loss=0.2546, val_loss=0.2697, val_acc=0.8850\n",
      "Epoch 1684: train_loss=0.2552, val_loss=0.2697, val_acc=0.8850\n",
      "Epoch 1685: train_loss=0.2573, val_loss=0.2696, val_acc=0.8850\n",
      "Epoch 1686: train_loss=0.2551, val_loss=0.2696, val_acc=0.8850\n",
      "Epoch 1687: train_loss=0.2561, val_loss=0.2696, val_acc=0.8850\n",
      "Epoch 1688: train_loss=0.2543, val_loss=0.2695, val_acc=0.8850\n",
      "Epoch 1689: train_loss=0.2531, val_loss=0.2695, val_acc=0.8850\n",
      "Epoch 1690: train_loss=0.2524, val_loss=0.2695, val_acc=0.8850\n",
      "Epoch 1691: train_loss=0.2530, val_loss=0.2695, val_acc=0.8850\n",
      "Epoch 1692: train_loss=0.2531, val_loss=0.2694, val_acc=0.8850\n",
      "Epoch 1693: train_loss=0.2584, val_loss=0.2694, val_acc=0.8850\n",
      "Epoch 1694: train_loss=0.2570, val_loss=0.2694, val_acc=0.8850\n",
      "Epoch 1695: train_loss=0.2528, val_loss=0.2693, val_acc=0.8850\n",
      "Epoch 1696: train_loss=0.2544, val_loss=0.2693, val_acc=0.8850\n",
      "Epoch 1697: train_loss=0.2532, val_loss=0.2693, val_acc=0.8850\n",
      "Epoch 1698: train_loss=0.2533, val_loss=0.2693, val_acc=0.8850\n",
      "Epoch 1699: train_loss=0.2538, val_loss=0.2692, val_acc=0.8850\n",
      "Epoch 1700: train_loss=0.2505, val_loss=0.2692, val_acc=0.8850\n",
      "Epoch 1701: train_loss=0.2566, val_loss=0.2692, val_acc=0.8850\n",
      "Epoch 1702: train_loss=0.2537, val_loss=0.2692, val_acc=0.8850\n",
      "Epoch 1703: train_loss=0.2524, val_loss=0.2691, val_acc=0.8850\n",
      "Epoch 1704: train_loss=0.2540, val_loss=0.2691, val_acc=0.8850\n",
      "Epoch 1705: train_loss=0.2514, val_loss=0.2691, val_acc=0.8864\n",
      "Epoch 1706: train_loss=0.2546, val_loss=0.2690, val_acc=0.8864\n",
      "Epoch 1707: train_loss=0.2502, val_loss=0.2690, val_acc=0.8864\n",
      "Epoch 1708: train_loss=0.2549, val_loss=0.2690, val_acc=0.8864\n",
      "Epoch 1709: train_loss=0.2527, val_loss=0.2690, val_acc=0.8864\n",
      "Epoch 1710: train_loss=0.2528, val_loss=0.2689, val_acc=0.8864\n",
      "Epoch 1711: train_loss=0.2532, val_loss=0.2689, val_acc=0.8864\n",
      "Epoch 1712: train_loss=0.2566, val_loss=0.2689, val_acc=0.8864\n",
      "Epoch 1713: train_loss=0.2537, val_loss=0.2689, val_acc=0.8864\n",
      "Epoch 1714: train_loss=0.2530, val_loss=0.2688, val_acc=0.8864\n",
      "Epoch 1715: train_loss=0.2529, val_loss=0.2688, val_acc=0.8864\n",
      "Epoch 1716: train_loss=0.2522, val_loss=0.2688, val_acc=0.8864\n",
      "Epoch 1717: train_loss=0.2562, val_loss=0.2687, val_acc=0.8864\n",
      "Epoch 1718: train_loss=0.2520, val_loss=0.2687, val_acc=0.8864\n",
      "Epoch 1719: train_loss=0.2535, val_loss=0.2687, val_acc=0.8864\n",
      "Epoch 1720: train_loss=0.2536, val_loss=0.2687, val_acc=0.8864\n",
      "Epoch 1721: train_loss=0.2530, val_loss=0.2686, val_acc=0.8864\n",
      "Epoch 1722: train_loss=0.2528, val_loss=0.2686, val_acc=0.8864\n",
      "Epoch 1723: train_loss=0.2547, val_loss=0.2686, val_acc=0.8864\n",
      "Epoch 1724: train_loss=0.2535, val_loss=0.2686, val_acc=0.8864\n",
      "Epoch 1725: train_loss=0.2550, val_loss=0.2685, val_acc=0.8864\n",
      "Epoch 1726: train_loss=0.2517, val_loss=0.2685, val_acc=0.8879\n",
      "Epoch 1727: train_loss=0.2549, val_loss=0.2685, val_acc=0.8864\n",
      "Epoch 1728: train_loss=0.2554, val_loss=0.2685, val_acc=0.8864\n",
      "Epoch 1729: train_loss=0.2518, val_loss=0.2684, val_acc=0.8864\n",
      "Epoch 1730: train_loss=0.2506, val_loss=0.2684, val_acc=0.8864\n",
      "Epoch 1731: train_loss=0.2513, val_loss=0.2684, val_acc=0.8864\n",
      "Epoch 1732: train_loss=0.2532, val_loss=0.2683, val_acc=0.8864\n",
      "Epoch 1733: train_loss=0.2551, val_loss=0.2683, val_acc=0.8864\n",
      "Epoch 1734: train_loss=0.2546, val_loss=0.2683, val_acc=0.8864\n",
      "Epoch 1735: train_loss=0.2507, val_loss=0.2683, val_acc=0.8864\n",
      "Epoch 1736: train_loss=0.2526, val_loss=0.2682, val_acc=0.8864\n",
      "Epoch 1737: train_loss=0.2503, val_loss=0.2682, val_acc=0.8864\n",
      "Epoch 1738: train_loss=0.2540, val_loss=0.2682, val_acc=0.8864\n",
      "Epoch 1739: train_loss=0.2486, val_loss=0.2682, val_acc=0.8864\n",
      "Epoch 1740: train_loss=0.2539, val_loss=0.2681, val_acc=0.8864\n",
      "Epoch 1741: train_loss=0.2525, val_loss=0.2681, val_acc=0.8864\n",
      "Epoch 1742: train_loss=0.2500, val_loss=0.2681, val_acc=0.8864\n",
      "Epoch 1743: train_loss=0.2525, val_loss=0.2681, val_acc=0.8864\n",
      "Epoch 1744: train_loss=0.2523, val_loss=0.2680, val_acc=0.8864\n",
      "Epoch 1745: train_loss=0.2559, val_loss=0.2680, val_acc=0.8850\n",
      "Epoch 1746: train_loss=0.2508, val_loss=0.2680, val_acc=0.8850\n",
      "Epoch 1747: train_loss=0.2509, val_loss=0.2680, val_acc=0.8850\n",
      "Epoch 1748: train_loss=0.2525, val_loss=0.2679, val_acc=0.8850\n",
      "Epoch 1749: train_loss=0.2521, val_loss=0.2679, val_acc=0.8850\n",
      "Epoch 1750: train_loss=0.2506, val_loss=0.2679, val_acc=0.8850\n",
      "Epoch 1751: train_loss=0.2503, val_loss=0.2679, val_acc=0.8864\n",
      "Epoch 1752: train_loss=0.2505, val_loss=0.2678, val_acc=0.8864\n",
      "Epoch 1753: train_loss=0.2492, val_loss=0.2678, val_acc=0.8864\n",
      "Epoch 1754: train_loss=0.2502, val_loss=0.2678, val_acc=0.8864\n",
      "Epoch 1755: train_loss=0.2536, val_loss=0.2678, val_acc=0.8864\n",
      "Epoch 1756: train_loss=0.2503, val_loss=0.2678, val_acc=0.8864\n",
      "Epoch 1757: train_loss=0.2533, val_loss=0.2677, val_acc=0.8864\n",
      "Epoch 1758: train_loss=0.2493, val_loss=0.2677, val_acc=0.8864\n",
      "Epoch 1759: train_loss=0.2517, val_loss=0.2677, val_acc=0.8864\n",
      "Epoch 1760: train_loss=0.2529, val_loss=0.2677, val_acc=0.8864\n",
      "Epoch 1761: train_loss=0.2481, val_loss=0.2676, val_acc=0.8864\n",
      "Epoch 1762: train_loss=0.2499, val_loss=0.2676, val_acc=0.8864\n",
      "Epoch 1763: train_loss=0.2536, val_loss=0.2676, val_acc=0.8864\n",
      "Epoch 1764: train_loss=0.2501, val_loss=0.2676, val_acc=0.8864\n",
      "Epoch 1765: train_loss=0.2492, val_loss=0.2675, val_acc=0.8864\n",
      "Epoch 1766: train_loss=0.2528, val_loss=0.2675, val_acc=0.8864\n",
      "Epoch 1767: train_loss=0.2506, val_loss=0.2675, val_acc=0.8864\n",
      "Epoch 1768: train_loss=0.2527, val_loss=0.2675, val_acc=0.8864\n",
      "Epoch 1769: train_loss=0.2538, val_loss=0.2674, val_acc=0.8864\n",
      "Epoch 1770: train_loss=0.2506, val_loss=0.2674, val_acc=0.8864\n",
      "Epoch 1771: train_loss=0.2550, val_loss=0.2674, val_acc=0.8864\n",
      "Epoch 1772: train_loss=0.2526, val_loss=0.2674, val_acc=0.8864\n",
      "Epoch 1773: train_loss=0.2499, val_loss=0.2673, val_acc=0.8864\n",
      "Epoch 1774: train_loss=0.2507, val_loss=0.2673, val_acc=0.8864\n",
      "Epoch 1775: train_loss=0.2510, val_loss=0.2673, val_acc=0.8864\n",
      "Epoch 1776: train_loss=0.2517, val_loss=0.2673, val_acc=0.8864\n",
      "Epoch 1777: train_loss=0.2490, val_loss=0.2672, val_acc=0.8864\n",
      "Epoch 1778: train_loss=0.2511, val_loss=0.2672, val_acc=0.8864\n",
      "Epoch 1779: train_loss=0.2482, val_loss=0.2672, val_acc=0.8879\n",
      "Epoch 1780: train_loss=0.2521, val_loss=0.2672, val_acc=0.8879\n",
      "Epoch 1781: train_loss=0.2487, val_loss=0.2671, val_acc=0.8879\n",
      "Epoch 1782: train_loss=0.2526, val_loss=0.2671, val_acc=0.8879\n",
      "Epoch 1783: train_loss=0.2492, val_loss=0.2671, val_acc=0.8879\n",
      "Epoch 1784: train_loss=0.2472, val_loss=0.2671, val_acc=0.8879\n",
      "Epoch 1785: train_loss=0.2485, val_loss=0.2671, val_acc=0.8879\n",
      "Epoch 1786: train_loss=0.2491, val_loss=0.2670, val_acc=0.8879\n",
      "Epoch 1787: train_loss=0.2502, val_loss=0.2670, val_acc=0.8879\n",
      "Epoch 1788: train_loss=0.2490, val_loss=0.2670, val_acc=0.8879\n",
      "Epoch 1789: train_loss=0.2489, val_loss=0.2670, val_acc=0.8879\n",
      "Epoch 1790: train_loss=0.2497, val_loss=0.2669, val_acc=0.8879\n",
      "Epoch 1791: train_loss=0.2494, val_loss=0.2669, val_acc=0.8879\n",
      "Epoch 1792: train_loss=0.2553, val_loss=0.2669, val_acc=0.8879\n",
      "Epoch 1793: train_loss=0.2511, val_loss=0.2669, val_acc=0.8879\n",
      "Epoch 1794: train_loss=0.2459, val_loss=0.2669, val_acc=0.8879\n",
      "Epoch 1795: train_loss=0.2486, val_loss=0.2668, val_acc=0.8879\n",
      "Epoch 1796: train_loss=0.2493, val_loss=0.2668, val_acc=0.8879\n",
      "Epoch 1797: train_loss=0.2480, val_loss=0.2668, val_acc=0.8879\n",
      "Epoch 1798: train_loss=0.2514, val_loss=0.2668, val_acc=0.8879\n",
      "Epoch 1799: train_loss=0.2490, val_loss=0.2667, val_acc=0.8879\n",
      "Epoch 1800: train_loss=0.2501, val_loss=0.2667, val_acc=0.8879\n",
      "Epoch 1801: train_loss=0.2490, val_loss=0.2667, val_acc=0.8879\n",
      "Epoch 1802: train_loss=0.2488, val_loss=0.2667, val_acc=0.8879\n",
      "Epoch 1803: train_loss=0.2519, val_loss=0.2666, val_acc=0.8879\n",
      "Epoch 1804: train_loss=0.2486, val_loss=0.2666, val_acc=0.8879\n",
      "Epoch 1805: train_loss=0.2494, val_loss=0.2666, val_acc=0.8879\n",
      "Epoch 1806: train_loss=0.2480, val_loss=0.2666, val_acc=0.8879\n",
      "Epoch 1807: train_loss=0.2529, val_loss=0.2665, val_acc=0.8879\n",
      "Epoch 1808: train_loss=0.2468, val_loss=0.2665, val_acc=0.8879\n",
      "Epoch 1809: train_loss=0.2503, val_loss=0.2665, val_acc=0.8879\n",
      "Epoch 1810: train_loss=0.2517, val_loss=0.2665, val_acc=0.8879\n",
      "Epoch 1811: train_loss=0.2484, val_loss=0.2665, val_acc=0.8879\n",
      "Epoch 1812: train_loss=0.2515, val_loss=0.2664, val_acc=0.8879\n",
      "Epoch 1813: train_loss=0.2460, val_loss=0.2664, val_acc=0.8879\n",
      "Epoch 1814: train_loss=0.2487, val_loss=0.2664, val_acc=0.8879\n",
      "Epoch 1815: train_loss=0.2488, val_loss=0.2664, val_acc=0.8879\n",
      "Epoch 1816: train_loss=0.2476, val_loss=0.2664, val_acc=0.8879\n",
      "Epoch 1817: train_loss=0.2487, val_loss=0.2663, val_acc=0.8879\n",
      "Epoch 1818: train_loss=0.2510, val_loss=0.2663, val_acc=0.8879\n",
      "Epoch 1819: train_loss=0.2484, val_loss=0.2663, val_acc=0.8879\n",
      "Epoch 1820: train_loss=0.2529, val_loss=0.2663, val_acc=0.8879\n",
      "Epoch 1821: train_loss=0.2525, val_loss=0.2662, val_acc=0.8879\n",
      "Epoch 1822: train_loss=0.2489, val_loss=0.2662, val_acc=0.8879\n",
      "Epoch 1823: train_loss=0.2527, val_loss=0.2662, val_acc=0.8879\n",
      "Epoch 1824: train_loss=0.2541, val_loss=0.2662, val_acc=0.8879\n",
      "Epoch 1825: train_loss=0.2473, val_loss=0.2662, val_acc=0.8879\n",
      "Epoch 1826: train_loss=0.2494, val_loss=0.2661, val_acc=0.8879\n",
      "Epoch 1827: train_loss=0.2520, val_loss=0.2661, val_acc=0.8879\n",
      "Epoch 1828: train_loss=0.2513, val_loss=0.2661, val_acc=0.8879\n",
      "Epoch 1829: train_loss=0.2450, val_loss=0.2661, val_acc=0.8879\n",
      "Epoch 1830: train_loss=0.2481, val_loss=0.2661, val_acc=0.8879\n",
      "Epoch 1831: train_loss=0.2490, val_loss=0.2660, val_acc=0.8879\n",
      "Epoch 1832: train_loss=0.2473, val_loss=0.2660, val_acc=0.8894\n",
      "Epoch 1833: train_loss=0.2458, val_loss=0.2660, val_acc=0.8894\n",
      "Epoch 1834: train_loss=0.2467, val_loss=0.2660, val_acc=0.8894\n",
      "Epoch 1835: train_loss=0.2431, val_loss=0.2659, val_acc=0.8894\n",
      "Epoch 1836: train_loss=0.2522, val_loss=0.2659, val_acc=0.8894\n",
      "Epoch 1837: train_loss=0.2485, val_loss=0.2659, val_acc=0.8894\n",
      "Epoch 1838: train_loss=0.2439, val_loss=0.2659, val_acc=0.8894\n",
      "Epoch 1839: train_loss=0.2480, val_loss=0.2659, val_acc=0.8894\n",
      "Epoch 1840: train_loss=0.2500, val_loss=0.2658, val_acc=0.8894\n",
      "Epoch 1841: train_loss=0.2472, val_loss=0.2658, val_acc=0.8894\n",
      "Epoch 1842: train_loss=0.2499, val_loss=0.2658, val_acc=0.8894\n",
      "Epoch 1843: train_loss=0.2466, val_loss=0.2658, val_acc=0.8894\n",
      "Epoch 1844: train_loss=0.2507, val_loss=0.2658, val_acc=0.8894\n",
      "Epoch 1845: train_loss=0.2493, val_loss=0.2657, val_acc=0.8894\n",
      "Epoch 1846: train_loss=0.2500, val_loss=0.2657, val_acc=0.8894\n",
      "Epoch 1847: train_loss=0.2493, val_loss=0.2657, val_acc=0.8894\n",
      "Epoch 1848: train_loss=0.2482, val_loss=0.2657, val_acc=0.8894\n",
      "Epoch 1849: train_loss=0.2469, val_loss=0.2657, val_acc=0.8894\n",
      "Epoch 1850: train_loss=0.2448, val_loss=0.2656, val_acc=0.8894\n",
      "Epoch 1851: train_loss=0.2520, val_loss=0.2656, val_acc=0.8894\n",
      "Epoch 1852: train_loss=0.2482, val_loss=0.2656, val_acc=0.8894\n",
      "Epoch 1853: train_loss=0.2479, val_loss=0.2656, val_acc=0.8894\n",
      "Epoch 1854: train_loss=0.2447, val_loss=0.2656, val_acc=0.8894\n",
      "Epoch 1855: train_loss=0.2462, val_loss=0.2655, val_acc=0.8894\n",
      "Epoch 1856: train_loss=0.2449, val_loss=0.2655, val_acc=0.8894\n",
      "Epoch 1857: train_loss=0.2516, val_loss=0.2655, val_acc=0.8894\n",
      "Epoch 1858: train_loss=0.2482, val_loss=0.2655, val_acc=0.8894\n",
      "Epoch 1859: train_loss=0.2510, val_loss=0.2655, val_acc=0.8894\n",
      "Epoch 1860: train_loss=0.2458, val_loss=0.2654, val_acc=0.8894\n",
      "Epoch 1861: train_loss=0.2483, val_loss=0.2654, val_acc=0.8894\n",
      "Epoch 1862: train_loss=0.2453, val_loss=0.2654, val_acc=0.8894\n",
      "Epoch 1863: train_loss=0.2491, val_loss=0.2654, val_acc=0.8894\n",
      "Epoch 1864: train_loss=0.2466, val_loss=0.2654, val_acc=0.8894\n",
      "Epoch 1865: train_loss=0.2479, val_loss=0.2653, val_acc=0.8894\n",
      "Epoch 1866: train_loss=0.2448, val_loss=0.2653, val_acc=0.8894\n",
      "Epoch 1867: train_loss=0.2488, val_loss=0.2653, val_acc=0.8894\n",
      "Epoch 1868: train_loss=0.2435, val_loss=0.2653, val_acc=0.8894\n",
      "Epoch 1869: train_loss=0.2461, val_loss=0.2653, val_acc=0.8894\n",
      "Epoch 1870: train_loss=0.2461, val_loss=0.2652, val_acc=0.8894\n",
      "Epoch 1871: train_loss=0.2437, val_loss=0.2652, val_acc=0.8894\n",
      "Epoch 1872: train_loss=0.2455, val_loss=0.2652, val_acc=0.8894\n",
      "Epoch 1873: train_loss=0.2481, val_loss=0.2652, val_acc=0.8894\n",
      "Epoch 1874: train_loss=0.2518, val_loss=0.2652, val_acc=0.8894\n",
      "Epoch 1875: train_loss=0.2425, val_loss=0.2651, val_acc=0.8909\n",
      "Epoch 1876: train_loss=0.2454, val_loss=0.2651, val_acc=0.8909\n",
      "Epoch 1877: train_loss=0.2460, val_loss=0.2651, val_acc=0.8909\n",
      "Epoch 1878: train_loss=0.2476, val_loss=0.2651, val_acc=0.8909\n",
      "Epoch 1879: train_loss=0.2471, val_loss=0.2651, val_acc=0.8909\n",
      "Epoch 1880: train_loss=0.2483, val_loss=0.2650, val_acc=0.8923\n",
      "Epoch 1881: train_loss=0.2444, val_loss=0.2650, val_acc=0.8923\n",
      "Epoch 1882: train_loss=0.2469, val_loss=0.2650, val_acc=0.8923\n",
      "Epoch 1883: train_loss=0.2441, val_loss=0.2650, val_acc=0.8923\n",
      "Epoch 1884: train_loss=0.2415, val_loss=0.2650, val_acc=0.8909\n",
      "Epoch 1885: train_loss=0.2488, val_loss=0.2650, val_acc=0.8909\n",
      "Epoch 1886: train_loss=0.2462, val_loss=0.2649, val_acc=0.8909\n",
      "Epoch 1887: train_loss=0.2460, val_loss=0.2649, val_acc=0.8894\n",
      "Epoch 1888: train_loss=0.2446, val_loss=0.2649, val_acc=0.8894\n",
      "Epoch 1889: train_loss=0.2474, val_loss=0.2649, val_acc=0.8894\n",
      "Epoch 1890: train_loss=0.2471, val_loss=0.2649, val_acc=0.8894\n",
      "Epoch 1891: train_loss=0.2430, val_loss=0.2648, val_acc=0.8894\n",
      "Epoch 1892: train_loss=0.2467, val_loss=0.2648, val_acc=0.8894\n",
      "Epoch 1893: train_loss=0.2498, val_loss=0.2648, val_acc=0.8909\n",
      "Epoch 1894: train_loss=0.2488, val_loss=0.2648, val_acc=0.8909\n",
      "Epoch 1895: train_loss=0.2481, val_loss=0.2648, val_acc=0.8909\n",
      "Epoch 1896: train_loss=0.2481, val_loss=0.2647, val_acc=0.8909\n",
      "Epoch 1897: train_loss=0.2450, val_loss=0.2647, val_acc=0.8909\n",
      "Epoch 1898: train_loss=0.2438, val_loss=0.2647, val_acc=0.8909\n",
      "Epoch 1899: train_loss=0.2447, val_loss=0.2647, val_acc=0.8909\n",
      "Epoch 1900: train_loss=0.2425, val_loss=0.2647, val_acc=0.8909\n",
      "Epoch 1901: train_loss=0.2485, val_loss=0.2646, val_acc=0.8909\n",
      "Epoch 1902: train_loss=0.2441, val_loss=0.2646, val_acc=0.8909\n",
      "Epoch 1903: train_loss=0.2490, val_loss=0.2646, val_acc=0.8909\n",
      "Epoch 1904: train_loss=0.2480, val_loss=0.2646, val_acc=0.8909\n",
      "Epoch 1905: train_loss=0.2487, val_loss=0.2646, val_acc=0.8909\n",
      "Epoch 1906: train_loss=0.2456, val_loss=0.2645, val_acc=0.8909\n",
      "Epoch 1907: train_loss=0.2474, val_loss=0.2645, val_acc=0.8909\n",
      "Epoch 1908: train_loss=0.2462, val_loss=0.2645, val_acc=0.8909\n",
      "Epoch 1909: train_loss=0.2427, val_loss=0.2645, val_acc=0.8909\n",
      "Epoch 1910: train_loss=0.2439, val_loss=0.2645, val_acc=0.8909\n",
      "Epoch 1911: train_loss=0.2482, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1912: train_loss=0.2455, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1913: train_loss=0.2434, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1914: train_loss=0.2459, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1915: train_loss=0.2485, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1916: train_loss=0.2450, val_loss=0.2644, val_acc=0.8909\n",
      "Epoch 1917: train_loss=0.2477, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1918: train_loss=0.2451, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1919: train_loss=0.2435, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1920: train_loss=0.2436, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1921: train_loss=0.2464, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1922: train_loss=0.2447, val_loss=0.2643, val_acc=0.8909\n",
      "Epoch 1923: train_loss=0.2454, val_loss=0.2642, val_acc=0.8909\n",
      "Epoch 1924: train_loss=0.2439, val_loss=0.2642, val_acc=0.8909\n",
      "Epoch 1925: train_loss=0.2491, val_loss=0.2642, val_acc=0.8909\n",
      "Epoch 1926: train_loss=0.2440, val_loss=0.2642, val_acc=0.8909\n",
      "Epoch 1927: train_loss=0.2478, val_loss=0.2642, val_acc=0.8909\n",
      "Epoch 1928: train_loss=0.2460, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1929: train_loss=0.2440, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1930: train_loss=0.2424, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1931: train_loss=0.2472, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1932: train_loss=0.2453, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1933: train_loss=0.2465, val_loss=0.2641, val_acc=0.8909\n",
      "Epoch 1934: train_loss=0.2489, val_loss=0.2640, val_acc=0.8909\n",
      "Epoch 1935: train_loss=0.2429, val_loss=0.2640, val_acc=0.8894\n",
      "Epoch 1936: train_loss=0.2436, val_loss=0.2640, val_acc=0.8894\n",
      "Epoch 1937: train_loss=0.2447, val_loss=0.2640, val_acc=0.8894\n",
      "Epoch 1938: train_loss=0.2475, val_loss=0.2640, val_acc=0.8894\n",
      "Epoch 1939: train_loss=0.2445, val_loss=0.2640, val_acc=0.8894\n",
      "Epoch 1940: train_loss=0.2458, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1941: train_loss=0.2449, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1942: train_loss=0.2463, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1943: train_loss=0.2453, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1944: train_loss=0.2438, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1945: train_loss=0.2435, val_loss=0.2639, val_acc=0.8894\n",
      "Epoch 1946: train_loss=0.2440, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1947: train_loss=0.2431, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1948: train_loss=0.2460, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1949: train_loss=0.2437, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1950: train_loss=0.2469, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1951: train_loss=0.2462, val_loss=0.2638, val_acc=0.8894\n",
      "Epoch 1952: train_loss=0.2461, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1953: train_loss=0.2441, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1954: train_loss=0.2389, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1955: train_loss=0.2401, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1956: train_loss=0.2458, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1957: train_loss=0.2450, val_loss=0.2637, val_acc=0.8894\n",
      "Epoch 1958: train_loss=0.2402, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1959: train_loss=0.2503, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1960: train_loss=0.2458, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1961: train_loss=0.2454, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1962: train_loss=0.2457, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1963: train_loss=0.2408, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1964: train_loss=0.2430, val_loss=0.2636, val_acc=0.8894\n",
      "Epoch 1965: train_loss=0.2439, val_loss=0.2635, val_acc=0.8894\n",
      "Epoch 1966: train_loss=0.2413, val_loss=0.2635, val_acc=0.8894\n",
      "Epoch 1967: train_loss=0.2410, val_loss=0.2635, val_acc=0.8894\n",
      "Epoch 1968: train_loss=0.2493, val_loss=0.2635, val_acc=0.8894\n",
      "Epoch 1969: train_loss=0.2434, val_loss=0.2635, val_acc=0.8894\n",
      "Epoch 1970: train_loss=0.2430, val_loss=0.2634, val_acc=0.8894\n",
      "Epoch 1971: train_loss=0.2479, val_loss=0.2634, val_acc=0.8894\n",
      "Epoch 1972: train_loss=0.2416, val_loss=0.2634, val_acc=0.8894\n",
      "Epoch 1973: train_loss=0.2448, val_loss=0.2634, val_acc=0.8909\n",
      "Epoch 1974: train_loss=0.2448, val_loss=0.2634, val_acc=0.8909\n",
      "Epoch 1975: train_loss=0.2472, val_loss=0.2634, val_acc=0.8909\n",
      "Epoch 1976: train_loss=0.2403, val_loss=0.2634, val_acc=0.8909\n",
      "Epoch 1977: train_loss=0.2442, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1978: train_loss=0.2454, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1979: train_loss=0.2463, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1980: train_loss=0.2440, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1981: train_loss=0.2407, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1982: train_loss=0.2445, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1983: train_loss=0.2445, val_loss=0.2633, val_acc=0.8909\n",
      "Epoch 1984: train_loss=0.2431, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1985: train_loss=0.2423, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1986: train_loss=0.2456, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1987: train_loss=0.2448, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1988: train_loss=0.2434, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1989: train_loss=0.2432, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1990: train_loss=0.2409, val_loss=0.2632, val_acc=0.8909\n",
      "Epoch 1991: train_loss=0.2432, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1992: train_loss=0.2441, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1993: train_loss=0.2412, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1994: train_loss=0.2406, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1995: train_loss=0.2454, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1996: train_loss=0.2447, val_loss=0.2631, val_acc=0.8909\n",
      "Epoch 1997: train_loss=0.2421, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 1998: train_loss=0.2432, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 1999: train_loss=0.2397, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 2000: train_loss=0.2431, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 2001: train_loss=0.2402, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 2002: train_loss=0.2387, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 2003: train_loss=0.2446, val_loss=0.2630, val_acc=0.8909\n",
      "Epoch 2004: train_loss=0.2457, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2005: train_loss=0.2433, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2006: train_loss=0.2440, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2007: train_loss=0.2436, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2008: train_loss=0.2424, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2009: train_loss=0.2452, val_loss=0.2629, val_acc=0.8909\n",
      "Epoch 2010: train_loss=0.2402, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2011: train_loss=0.2438, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2012: train_loss=0.2469, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2013: train_loss=0.2455, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2014: train_loss=0.2429, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2015: train_loss=0.2433, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2016: train_loss=0.2442, val_loss=0.2628, val_acc=0.8909\n",
      "Epoch 2017: train_loss=0.2438, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2018: train_loss=0.2413, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2019: train_loss=0.2398, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2020: train_loss=0.2420, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2021: train_loss=0.2424, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2022: train_loss=0.2425, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2023: train_loss=0.2421, val_loss=0.2627, val_acc=0.8909\n",
      "Epoch 2024: train_loss=0.2426, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2025: train_loss=0.2408, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2026: train_loss=0.2397, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2027: train_loss=0.2424, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2028: train_loss=0.2426, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2029: train_loss=0.2432, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2030: train_loss=0.2410, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2031: train_loss=0.2409, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2032: train_loss=0.2445, val_loss=0.2626, val_acc=0.8909\n",
      "Epoch 2033: train_loss=0.2416, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2034: train_loss=0.2434, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2035: train_loss=0.2427, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2036: train_loss=0.2403, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2037: train_loss=0.2454, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2038: train_loss=0.2433, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2039: train_loss=0.2388, val_loss=0.2625, val_acc=0.8909\n",
      "Epoch 2040: train_loss=0.2381, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2041: train_loss=0.2387, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2042: train_loss=0.2424, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2043: train_loss=0.2411, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2044: train_loss=0.2390, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2045: train_loss=0.2415, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2046: train_loss=0.2385, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2047: train_loss=0.2428, val_loss=0.2624, val_acc=0.8909\n",
      "Epoch 2048: train_loss=0.2467, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2049: train_loss=0.2441, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2050: train_loss=0.2407, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2051: train_loss=0.2408, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2052: train_loss=0.2374, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2053: train_loss=0.2384, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2054: train_loss=0.2436, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2055: train_loss=0.2404, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2056: train_loss=0.2394, val_loss=0.2623, val_acc=0.8909\n",
      "Epoch 2057: train_loss=0.2411, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2058: train_loss=0.2403, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2059: train_loss=0.2400, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2060: train_loss=0.2376, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2061: train_loss=0.2422, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2062: train_loss=0.2411, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2063: train_loss=0.2413, val_loss=0.2622, val_acc=0.8909\n",
      "Epoch 2064: train_loss=0.2378, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2065: train_loss=0.2391, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2066: train_loss=0.2430, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2067: train_loss=0.2437, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2068: train_loss=0.2393, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2069: train_loss=0.2402, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2070: train_loss=0.2406, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2071: train_loss=0.2389, val_loss=0.2621, val_acc=0.8909\n",
      "Epoch 2072: train_loss=0.2412, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2073: train_loss=0.2434, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2074: train_loss=0.2416, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2075: train_loss=0.2387, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2076: train_loss=0.2396, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2077: train_loss=0.2408, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2078: train_loss=0.2361, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2079: train_loss=0.2420, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2080: train_loss=0.2422, val_loss=0.2620, val_acc=0.8909\n",
      "Epoch 2081: train_loss=0.2456, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2082: train_loss=0.2403, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2083: train_loss=0.2382, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2084: train_loss=0.2369, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2085: train_loss=0.2420, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2086: train_loss=0.2376, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2087: train_loss=0.2395, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2088: train_loss=0.2387, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2089: train_loss=0.2407, val_loss=0.2619, val_acc=0.8909\n",
      "Epoch 2090: train_loss=0.2414, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2091: train_loss=0.2440, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2092: train_loss=0.2408, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2093: train_loss=0.2399, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2094: train_loss=0.2426, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2095: train_loss=0.2365, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2096: train_loss=0.2414, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2097: train_loss=0.2372, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2098: train_loss=0.2369, val_loss=0.2618, val_acc=0.8909\n",
      "Epoch 2099: train_loss=0.2413, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2100: train_loss=0.2386, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2101: train_loss=0.2423, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2102: train_loss=0.2431, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2103: train_loss=0.2439, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2104: train_loss=0.2405, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2105: train_loss=0.2388, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2106: train_loss=0.2401, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2107: train_loss=0.2349, val_loss=0.2617, val_acc=0.8909\n",
      "Epoch 2108: train_loss=0.2423, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2109: train_loss=0.2412, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2110: train_loss=0.2395, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2111: train_loss=0.2407, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2112: train_loss=0.2393, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2113: train_loss=0.2393, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2114: train_loss=0.2400, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2115: train_loss=0.2435, val_loss=0.2616, val_acc=0.8909\n",
      "Epoch 2116: train_loss=0.2397, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2117: train_loss=0.2408, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2118: train_loss=0.2396, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2119: train_loss=0.2425, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2120: train_loss=0.2407, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2121: train_loss=0.2359, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2122: train_loss=0.2378, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2123: train_loss=0.2394, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2124: train_loss=0.2356, val_loss=0.2615, val_acc=0.8909\n",
      "Epoch 2125: train_loss=0.2405, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2126: train_loss=0.2441, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2127: train_loss=0.2417, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2128: train_loss=0.2443, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2129: train_loss=0.2401, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2130: train_loss=0.2393, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2131: train_loss=0.2391, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2132: train_loss=0.2403, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2133: train_loss=0.2376, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2134: train_loss=0.2362, val_loss=0.2614, val_acc=0.8909\n",
      "Epoch 2135: train_loss=0.2378, val_loss=0.2613, val_acc=0.8909\n",
      "Epoch 2136: train_loss=0.2431, val_loss=0.2613, val_acc=0.8909\n",
      "Epoch 2137: train_loss=0.2410, val_loss=0.2613, val_acc=0.8909\n",
      "Epoch 2138: train_loss=0.2341, val_loss=0.2613, val_acc=0.8909\n",
      "Epoch 2139: train_loss=0.2414, val_loss=0.2613, val_acc=0.8909\n",
      "Epoch 2140: train_loss=0.2401, val_loss=0.2613, val_acc=0.8923\n",
      "Epoch 2141: train_loss=0.2419, val_loss=0.2613, val_acc=0.8923\n",
      "Epoch 2142: train_loss=0.2430, val_loss=0.2613, val_acc=0.8923\n",
      "Epoch 2143: train_loss=0.2397, val_loss=0.2612, val_acc=0.8923\n",
      "Epoch 2144: train_loss=0.2477, val_loss=0.2612, val_acc=0.8923\n",
      "Epoch 2145: train_loss=0.2387, val_loss=0.2612, val_acc=0.8923\n",
      "Epoch 2146: train_loss=0.2400, val_loss=0.2612, val_acc=0.8923\n",
      "Epoch 2147: train_loss=0.2378, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2148: train_loss=0.2382, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2149: train_loss=0.2410, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2150: train_loss=0.2397, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2151: train_loss=0.2363, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2152: train_loss=0.2396, val_loss=0.2612, val_acc=0.8909\n",
      "Epoch 2153: train_loss=0.2433, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2154: train_loss=0.2364, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2155: train_loss=0.2407, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2156: train_loss=0.2369, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2157: train_loss=0.2342, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2158: train_loss=0.2411, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2159: train_loss=0.2382, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2160: train_loss=0.2364, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2161: train_loss=0.2364, val_loss=0.2611, val_acc=0.8909\n",
      "Epoch 2162: train_loss=0.2412, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2163: train_loss=0.2370, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2164: train_loss=0.2373, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2165: train_loss=0.2353, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2166: train_loss=0.2401, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2167: train_loss=0.2397, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2168: train_loss=0.2393, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2169: train_loss=0.2361, val_loss=0.2610, val_acc=0.8923\n",
      "Epoch 2170: train_loss=0.2327, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2171: train_loss=0.2365, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2172: train_loss=0.2387, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2173: train_loss=0.2365, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2174: train_loss=0.2386, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2175: train_loss=0.2401, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2176: train_loss=0.2355, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2177: train_loss=0.2379, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2178: train_loss=0.2369, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2179: train_loss=0.2344, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2180: train_loss=0.2361, val_loss=0.2609, val_acc=0.8923\n",
      "Epoch 2181: train_loss=0.2363, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2182: train_loss=0.2407, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2183: train_loss=0.2372, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2184: train_loss=0.2423, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2185: train_loss=0.2399, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2186: train_loss=0.2400, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2187: train_loss=0.2411, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2188: train_loss=0.2372, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2189: train_loss=0.2343, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2190: train_loss=0.2394, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2191: train_loss=0.2399, val_loss=0.2608, val_acc=0.8923\n",
      "Epoch 2192: train_loss=0.2363, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2193: train_loss=0.2371, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2194: train_loss=0.2402, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2195: train_loss=0.2385, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2196: train_loss=0.2403, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2197: train_loss=0.2382, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2198: train_loss=0.2386, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2199: train_loss=0.2352, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2200: train_loss=0.2357, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2201: train_loss=0.2356, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2202: train_loss=0.2356, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2203: train_loss=0.2403, val_loss=0.2607, val_acc=0.8923\n",
      "Epoch 2204: train_loss=0.2339, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2205: train_loss=0.2406, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2206: train_loss=0.2348, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2207: train_loss=0.2409, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2208: train_loss=0.2334, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2209: train_loss=0.2379, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2210: train_loss=0.2396, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2211: train_loss=0.2355, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2212: train_loss=0.2369, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2213: train_loss=0.2315, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2214: train_loss=0.2348, val_loss=0.2606, val_acc=0.8923\n",
      "Epoch 2215: train_loss=0.2428, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2216: train_loss=0.2380, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2217: train_loss=0.2375, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2218: train_loss=0.2372, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2219: train_loss=0.2359, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2220: train_loss=0.2381, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2221: train_loss=0.2364, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2222: train_loss=0.2389, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2223: train_loss=0.2379, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2224: train_loss=0.2394, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2225: train_loss=0.2364, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2226: train_loss=0.2335, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2227: train_loss=0.2342, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2228: train_loss=0.2389, val_loss=0.2605, val_acc=0.8938\n",
      "Epoch 2229: train_loss=0.2372, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2230: train_loss=0.2367, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2231: train_loss=0.2343, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2232: train_loss=0.2365, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2233: train_loss=0.2352, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2234: train_loss=0.2389, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2235: train_loss=0.2397, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2236: train_loss=0.2359, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2237: train_loss=0.2394, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2238: train_loss=0.2339, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2239: train_loss=0.2382, val_loss=0.2604, val_acc=0.8938\n",
      "Epoch 2240: train_loss=0.2414, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2241: train_loss=0.2380, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2242: train_loss=0.2360, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2243: train_loss=0.2364, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2244: train_loss=0.2366, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2245: train_loss=0.2352, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2246: train_loss=0.2372, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2247: train_loss=0.2333, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2248: train_loss=0.2310, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2249: train_loss=0.2408, val_loss=0.2603, val_acc=0.8938\n",
      "Epoch 2250: train_loss=0.2401, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2251: train_loss=0.2355, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2252: train_loss=0.2389, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2253: train_loss=0.2386, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2254: train_loss=0.2363, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2255: train_loss=0.2366, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2256: train_loss=0.2377, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2257: train_loss=0.2346, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2258: train_loss=0.2381, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2259: train_loss=0.2356, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2260: train_loss=0.2339, val_loss=0.2602, val_acc=0.8938\n",
      "Epoch 2261: train_loss=0.2366, val_loss=0.2602, val_acc=0.8953\n",
      "Epoch 2262: train_loss=0.2385, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2263: train_loss=0.2343, val_loss=0.2602, val_acc=0.8953\n",
      "Epoch 2264: train_loss=0.2357, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2265: train_loss=0.2356, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2266: train_loss=0.2340, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2267: train_loss=0.2347, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2268: train_loss=0.2363, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2269: train_loss=0.2357, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2270: train_loss=0.2355, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2271: train_loss=0.2353, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2272: train_loss=0.2337, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2273: train_loss=0.2366, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2274: train_loss=0.2383, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2275: train_loss=0.2363, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2276: train_loss=0.2360, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2277: train_loss=0.2343, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2278: train_loss=0.2373, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2279: train_loss=0.2360, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2280: train_loss=0.2368, val_loss=0.2601, val_acc=0.8953\n",
      "Epoch 2281: train_loss=0.2301, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2282: train_loss=0.2363, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2283: train_loss=0.2342, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2284: train_loss=0.2364, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2285: train_loss=0.2381, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2286: train_loss=0.2398, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2287: train_loss=0.2392, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2288: train_loss=0.2350, val_loss=0.2600, val_acc=0.8953\n",
      "Epoch 2289: train_loss=0.2362, val_loss=0.2600, val_acc=0.8968\n",
      "Epoch 2290: train_loss=0.2357, val_loss=0.2600, val_acc=0.8968\n",
      "Epoch 2291: train_loss=0.2339, val_loss=0.2600, val_acc=0.8968\n",
      "Epoch 2292: train_loss=0.2329, val_loss=0.2600, val_acc=0.8968\n",
      "Epoch 2293: train_loss=0.2351, val_loss=0.2600, val_acc=0.8968\n",
      "Epoch 2294: train_loss=0.2346, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2295: train_loss=0.2361, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2296: train_loss=0.2359, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2297: train_loss=0.2376, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2298: train_loss=0.2314, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2299: train_loss=0.2363, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2300: train_loss=0.2388, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2301: train_loss=0.2368, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2302: train_loss=0.2378, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2303: train_loss=0.2385, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2304: train_loss=0.2357, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2305: train_loss=0.2351, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2306: train_loss=0.2391, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2307: train_loss=0.2337, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2308: train_loss=0.2401, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2309: train_loss=0.2361, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2310: train_loss=0.2290, val_loss=0.2599, val_acc=0.8968\n",
      "Epoch 2311: train_loss=0.2353, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2312: train_loss=0.2335, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2313: train_loss=0.2372, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2314: train_loss=0.2355, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2315: train_loss=0.2330, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2316: train_loss=0.2367, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2317: train_loss=0.2374, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2318: train_loss=0.2364, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2319: train_loss=0.2367, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2320: train_loss=0.2351, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2321: train_loss=0.2345, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2322: train_loss=0.2369, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2323: train_loss=0.2312, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2324: train_loss=0.2359, val_loss=0.2598, val_acc=0.8968\n",
      "Epoch 2325: train_loss=0.2365, val_loss=0.2598, val_acc=0.8982\n",
      "Epoch 2326: train_loss=0.2325, val_loss=0.2598, val_acc=0.8982\n",
      "Epoch 2327: train_loss=0.2356, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2328: train_loss=0.2397, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2329: train_loss=0.2391, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2330: train_loss=0.2345, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2331: train_loss=0.2370, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2332: train_loss=0.2363, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2333: train_loss=0.2325, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2334: train_loss=0.2365, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2335: train_loss=0.2342, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2336: train_loss=0.2389, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2337: train_loss=0.2369, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2338: train_loss=0.2357, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2339: train_loss=0.2354, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2340: train_loss=0.2384, val_loss=0.2597, val_acc=0.8982\n",
      "Epoch 2341: train_loss=0.2387, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2342: train_loss=0.2384, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2343: train_loss=0.2356, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2344: train_loss=0.2305, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2345: train_loss=0.2376, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2346: train_loss=0.2357, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2347: train_loss=0.2347, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2348: train_loss=0.2380, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2349: train_loss=0.2314, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2350: train_loss=0.2361, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2351: train_loss=0.2342, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2352: train_loss=0.2338, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2353: train_loss=0.2314, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2354: train_loss=0.2343, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2355: train_loss=0.2340, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2356: train_loss=0.2348, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2357: train_loss=0.2373, val_loss=0.2596, val_acc=0.8982\n",
      "Epoch 2358: train_loss=0.2382, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2359: train_loss=0.2348, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2360: train_loss=0.2341, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2361: train_loss=0.2388, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2362: train_loss=0.2376, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2363: train_loss=0.2356, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2364: train_loss=0.2308, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2365: train_loss=0.2370, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2366: train_loss=0.2360, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2367: train_loss=0.2324, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2368: train_loss=0.2381, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2369: train_loss=0.2377, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2370: train_loss=0.2288, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2371: train_loss=0.2319, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2372: train_loss=0.2341, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2373: train_loss=0.2367, val_loss=0.2595, val_acc=0.8982\n",
      "Epoch 2374: train_loss=0.2333, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2375: train_loss=0.2350, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2376: train_loss=0.2358, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2377: train_loss=0.2348, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2378: train_loss=0.2378, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2379: train_loss=0.2350, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2380: train_loss=0.2327, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2381: train_loss=0.2278, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2382: train_loss=0.2309, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2383: train_loss=0.2380, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2384: train_loss=0.2351, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2385: train_loss=0.2355, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2386: train_loss=0.2312, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2387: train_loss=0.2351, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2388: train_loss=0.2324, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2389: train_loss=0.2337, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2390: train_loss=0.2349, val_loss=0.2594, val_acc=0.8982\n",
      "Epoch 2391: train_loss=0.2339, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2392: train_loss=0.2382, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2393: train_loss=0.2358, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2394: train_loss=0.2330, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2395: train_loss=0.2348, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2396: train_loss=0.2337, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2397: train_loss=0.2325, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2398: train_loss=0.2351, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2399: train_loss=0.2339, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2400: train_loss=0.2321, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2401: train_loss=0.2310, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2402: train_loss=0.2318, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2403: train_loss=0.2357, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2404: train_loss=0.2349, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2405: train_loss=0.2340, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2406: train_loss=0.2361, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2407: train_loss=0.2314, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2408: train_loss=0.2366, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2409: train_loss=0.2351, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2410: train_loss=0.2357, val_loss=0.2593, val_acc=0.8982\n",
      "Epoch 2411: train_loss=0.2334, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2412: train_loss=0.2345, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2413: train_loss=0.2358, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2414: train_loss=0.2352, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2415: train_loss=0.2335, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2416: train_loss=0.2298, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2417: train_loss=0.2350, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2418: train_loss=0.2302, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2419: train_loss=0.2370, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2420: train_loss=0.2351, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2421: train_loss=0.2350, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2422: train_loss=0.2313, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2423: train_loss=0.2372, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2424: train_loss=0.2345, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2425: train_loss=0.2297, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2426: train_loss=0.2316, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2427: train_loss=0.2355, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2428: train_loss=0.2327, val_loss=0.2592, val_acc=0.8982\n",
      "Epoch 2429: train_loss=0.2316, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2430: train_loss=0.2331, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2431: train_loss=0.2337, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2432: train_loss=0.2340, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2433: train_loss=0.2309, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2434: train_loss=0.2316, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2435: train_loss=0.2396, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2436: train_loss=0.2357, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2437: train_loss=0.2337, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2438: train_loss=0.2311, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2439: train_loss=0.2291, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2440: train_loss=0.2305, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2441: train_loss=0.2319, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2442: train_loss=0.2367, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2443: train_loss=0.2339, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2444: train_loss=0.2372, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2445: train_loss=0.2347, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2446: train_loss=0.2350, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2447: train_loss=0.2304, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2448: train_loss=0.2339, val_loss=0.2591, val_acc=0.8982\n",
      "Epoch 2449: train_loss=0.2325, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2450: train_loss=0.2320, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2451: train_loss=0.2322, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2452: train_loss=0.2297, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2453: train_loss=0.2308, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2454: train_loss=0.2348, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2455: train_loss=0.2315, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2456: train_loss=0.2312, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2457: train_loss=0.2354, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2458: train_loss=0.2333, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2459: train_loss=0.2361, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2460: train_loss=0.2354, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2461: train_loss=0.2325, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2462: train_loss=0.2341, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2463: train_loss=0.2317, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2464: train_loss=0.2320, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2465: train_loss=0.2368, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2466: train_loss=0.2307, val_loss=0.2590, val_acc=0.8982\n",
      "Epoch 2467: train_loss=0.2347, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2468: train_loss=0.2343, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2469: train_loss=0.2348, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2470: train_loss=0.2318, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2471: train_loss=0.2357, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2472: train_loss=0.2293, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2473: train_loss=0.2346, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2474: train_loss=0.2317, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2475: train_loss=0.2348, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2476: train_loss=0.2298, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2477: train_loss=0.2291, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2478: train_loss=0.2299, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2479: train_loss=0.2360, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2480: train_loss=0.2341, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2481: train_loss=0.2313, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2482: train_loss=0.2316, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2483: train_loss=0.2300, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2484: train_loss=0.2334, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2485: train_loss=0.2331, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2486: train_loss=0.2316, val_loss=0.2589, val_acc=0.8982\n",
      "Epoch 2487: train_loss=0.2326, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2488: train_loss=0.2282, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2489: train_loss=0.2271, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2490: train_loss=0.2315, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2491: train_loss=0.2363, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2492: train_loss=0.2387, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2493: train_loss=0.2365, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2494: train_loss=0.2321, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2495: train_loss=0.2349, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2496: train_loss=0.2297, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2497: train_loss=0.2337, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2498: train_loss=0.2313, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2499: train_loss=0.2275, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2500: train_loss=0.2294, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2501: train_loss=0.2321, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2502: train_loss=0.2324, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2503: train_loss=0.2326, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2504: train_loss=0.2330, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2505: train_loss=0.2319, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2506: train_loss=0.2341, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2507: train_loss=0.2331, val_loss=0.2588, val_acc=0.8982\n",
      "Epoch 2508: train_loss=0.2346, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2509: train_loss=0.2333, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2510: train_loss=0.2325, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2511: train_loss=0.2316, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2512: train_loss=0.2284, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2513: train_loss=0.2323, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2514: train_loss=0.2342, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2515: train_loss=0.2331, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2516: train_loss=0.2318, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2517: train_loss=0.2331, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2518: train_loss=0.2341, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2519: train_loss=0.2357, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2520: train_loss=0.2309, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2521: train_loss=0.2313, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2522: train_loss=0.2335, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2523: train_loss=0.2346, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2524: train_loss=0.2304, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2525: train_loss=0.2287, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2526: train_loss=0.2328, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2527: train_loss=0.2333, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2528: train_loss=0.2295, val_loss=0.2587, val_acc=0.8982\n",
      "Epoch 2529: train_loss=0.2309, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2530: train_loss=0.2318, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2531: train_loss=0.2305, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2532: train_loss=0.2304, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2533: train_loss=0.2345, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2534: train_loss=0.2320, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2535: train_loss=0.2288, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2536: train_loss=0.2351, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2537: train_loss=0.2314, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2538: train_loss=0.2280, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2539: train_loss=0.2294, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2540: train_loss=0.2333, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2541: train_loss=0.2356, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2542: train_loss=0.2299, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2543: train_loss=0.2365, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2544: train_loss=0.2290, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2545: train_loss=0.2286, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2546: train_loss=0.2320, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2547: train_loss=0.2281, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2548: train_loss=0.2336, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2549: train_loss=0.2336, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2550: train_loss=0.2284, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2551: train_loss=0.2297, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2552: train_loss=0.2343, val_loss=0.2586, val_acc=0.8982\n",
      "Epoch 2553: train_loss=0.2322, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2554: train_loss=0.2322, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2555: train_loss=0.2297, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2556: train_loss=0.2348, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2557: train_loss=0.2331, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2558: train_loss=0.2335, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2559: train_loss=0.2317, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2560: train_loss=0.2336, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2561: train_loss=0.2359, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2562: train_loss=0.2307, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2563: train_loss=0.2320, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2564: train_loss=0.2257, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2565: train_loss=0.2291, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2566: train_loss=0.2308, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2567: train_loss=0.2297, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2568: train_loss=0.2314, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2569: train_loss=0.2281, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2570: train_loss=0.2316, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2571: train_loss=0.2313, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2572: train_loss=0.2360, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2573: train_loss=0.2315, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2574: train_loss=0.2354, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2575: train_loss=0.2254, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2576: train_loss=0.2317, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2577: train_loss=0.2343, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2578: train_loss=0.2301, val_loss=0.2585, val_acc=0.8982\n",
      "Epoch 2579: train_loss=0.2309, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2580: train_loss=0.2250, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2581: train_loss=0.2312, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2582: train_loss=0.2327, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2583: train_loss=0.2292, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2584: train_loss=0.2357, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2585: train_loss=0.2315, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2586: train_loss=0.2333, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2587: train_loss=0.2295, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2588: train_loss=0.2323, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2589: train_loss=0.2313, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2590: train_loss=0.2309, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2591: train_loss=0.2311, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2592: train_loss=0.2264, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2593: train_loss=0.2297, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2594: train_loss=0.2321, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2595: train_loss=0.2298, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2596: train_loss=0.2287, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2597: train_loss=0.2325, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2598: train_loss=0.2289, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2599: train_loss=0.2287, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2600: train_loss=0.2313, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2601: train_loss=0.2310, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2602: train_loss=0.2329, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2603: train_loss=0.2354, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2604: train_loss=0.2304, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2605: train_loss=0.2295, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2606: train_loss=0.2274, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2607: train_loss=0.2297, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2608: train_loss=0.2299, val_loss=0.2584, val_acc=0.8982\n",
      "Epoch 2609: train_loss=0.2290, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2610: train_loss=0.2303, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2611: train_loss=0.2328, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2612: train_loss=0.2298, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2613: train_loss=0.2308, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2614: train_loss=0.2280, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2615: train_loss=0.2312, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2616: train_loss=0.2314, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2617: train_loss=0.2302, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2618: train_loss=0.2356, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2619: train_loss=0.2362, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2620: train_loss=0.2338, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2621: train_loss=0.2313, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2622: train_loss=0.2320, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2623: train_loss=0.2326, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2624: train_loss=0.2268, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2625: train_loss=0.2308, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2626: train_loss=0.2277, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2627: train_loss=0.2308, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2628: train_loss=0.2289, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2629: train_loss=0.2271, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2630: train_loss=0.2290, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2631: train_loss=0.2312, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2632: train_loss=0.2335, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2633: train_loss=0.2320, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2634: train_loss=0.2323, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2635: train_loss=0.2316, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2636: train_loss=0.2335, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2637: train_loss=0.2311, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2638: train_loss=0.2328, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2639: train_loss=0.2335, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2640: train_loss=0.2324, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2641: train_loss=0.2268, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2642: train_loss=0.2324, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2643: train_loss=0.2267, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2644: train_loss=0.2280, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2645: train_loss=0.2331, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2646: train_loss=0.2320, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2647: train_loss=0.2345, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2648: train_loss=0.2321, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2649: train_loss=0.2318, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2650: train_loss=0.2284, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2651: train_loss=0.2256, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2652: train_loss=0.2340, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2653: train_loss=0.2339, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2654: train_loss=0.2303, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2655: train_loss=0.2246, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2656: train_loss=0.2271, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2657: train_loss=0.2276, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2658: train_loss=0.2299, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2659: train_loss=0.2297, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2660: train_loss=0.2317, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2661: train_loss=0.2320, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2662: train_loss=0.2286, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2663: train_loss=0.2298, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2664: train_loss=0.2296, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2665: train_loss=0.2310, val_loss=0.2582, val_acc=0.8982\n",
      "Epoch 2666: train_loss=0.2294, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2667: train_loss=0.2319, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2668: train_loss=0.2324, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2669: train_loss=0.2332, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2670: train_loss=0.2296, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2671: train_loss=0.2282, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2672: train_loss=0.2345, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2673: train_loss=0.2275, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2674: train_loss=0.2279, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2675: train_loss=0.2340, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2676: train_loss=0.2340, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2677: train_loss=0.2277, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2678: train_loss=0.2327, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2679: train_loss=0.2295, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2680: train_loss=0.2275, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2681: train_loss=0.2305, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2682: train_loss=0.2319, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2683: train_loss=0.2295, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2684: train_loss=0.2294, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2685: train_loss=0.2306, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2686: train_loss=0.2273, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2687: train_loss=0.2315, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2688: train_loss=0.2260, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2689: train_loss=0.2301, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2690: train_loss=0.2269, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2691: train_loss=0.2318, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2692: train_loss=0.2348, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2693: train_loss=0.2336, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2694: train_loss=0.2283, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2695: train_loss=0.2279, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2696: train_loss=0.2296, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2697: train_loss=0.2263, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2698: train_loss=0.2309, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2699: train_loss=0.2333, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2700: train_loss=0.2279, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2701: train_loss=0.2291, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2702: train_loss=0.2274, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2703: train_loss=0.2300, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2704: train_loss=0.2327, val_loss=0.2581, val_acc=0.8982\n",
      "Epoch 2705: train_loss=0.2293, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2706: train_loss=0.2292, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2707: train_loss=0.2258, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2708: train_loss=0.2293, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2709: train_loss=0.2295, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2710: train_loss=0.2303, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2711: train_loss=0.2276, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2712: train_loss=0.2282, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2713: train_loss=0.2332, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2714: train_loss=0.2324, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2715: train_loss=0.2286, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2716: train_loss=0.2276, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2717: train_loss=0.2321, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2718: train_loss=0.2249, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2719: train_loss=0.2291, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2720: train_loss=0.2315, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2721: train_loss=0.2267, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2722: train_loss=0.2340, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2723: train_loss=0.2293, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2724: train_loss=0.2317, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2725: train_loss=0.2340, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2726: train_loss=0.2277, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2727: train_loss=0.2309, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2728: train_loss=0.2317, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2729: train_loss=0.2277, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2730: train_loss=0.2269, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2731: train_loss=0.2304, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2732: train_loss=0.2306, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2733: train_loss=0.2296, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2734: train_loss=0.2330, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2735: train_loss=0.2275, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2736: train_loss=0.2252, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2737: train_loss=0.2268, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2738: train_loss=0.2299, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2739: train_loss=0.2252, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2740: train_loss=0.2273, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2741: train_loss=0.2295, val_loss=0.2580, val_acc=0.8982\n",
      "Epoch 2742: train_loss=0.2272, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2743: train_loss=0.2318, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2744: train_loss=0.2260, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2745: train_loss=0.2312, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2746: train_loss=0.2299, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2747: train_loss=0.2294, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2748: train_loss=0.2304, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2749: train_loss=0.2304, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2750: train_loss=0.2275, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2751: train_loss=0.2273, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2752: train_loss=0.2303, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2753: train_loss=0.2299, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2754: train_loss=0.2330, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2755: train_loss=0.2275, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2756: train_loss=0.2282, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2757: train_loss=0.2312, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2758: train_loss=0.2318, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2759: train_loss=0.2304, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2760: train_loss=0.2293, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2761: train_loss=0.2281, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2762: train_loss=0.2269, val_loss=0.2579, val_acc=0.8997\n",
      "Epoch 2763: train_loss=0.2273, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2764: train_loss=0.2235, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2765: train_loss=0.2323, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2766: train_loss=0.2272, val_loss=0.2579, val_acc=0.8982\n",
      "Epoch 2767: train_loss=0.2285, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2768: train_loss=0.2287, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2769: train_loss=0.2290, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2770: train_loss=0.2245, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2771: train_loss=0.2298, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2772: train_loss=0.2262, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2773: train_loss=0.2300, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2774: train_loss=0.2319, val_loss=0.2578, val_acc=0.8982\n",
      "Epoch 2775: train_loss=0.2294, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2776: train_loss=0.2280, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2777: train_loss=0.2297, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2778: train_loss=0.2322, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2779: train_loss=0.2338, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2780: train_loss=0.2296, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2781: train_loss=0.2270, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2782: train_loss=0.2247, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2783: train_loss=0.2290, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2784: train_loss=0.2255, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2785: train_loss=0.2270, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2786: train_loss=0.2328, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2787: train_loss=0.2293, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2788: train_loss=0.2283, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2789: train_loss=0.2283, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2790: train_loss=0.2237, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2791: train_loss=0.2314, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2792: train_loss=0.2283, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2793: train_loss=0.2323, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2794: train_loss=0.2288, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2795: train_loss=0.2280, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2796: train_loss=0.2299, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2797: train_loss=0.2265, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2798: train_loss=0.2300, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2799: train_loss=0.2296, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2800: train_loss=0.2280, val_loss=0.2578, val_acc=0.8968\n",
      "Epoch 2801: train_loss=0.2252, val_loss=0.2578, val_acc=0.8968\n",
      "\n",
      "Training SGD...\n",
      "Epoch 1: train_loss=0.3063, val_loss=0.2583, val_acc=0.8982\n",
      "Epoch 2: train_loss=0.2377, val_loss=0.2526, val_acc=0.8953\n",
      "Epoch 3: train_loss=0.2292, val_loss=0.2591, val_acc=0.8953\n",
      "Epoch 4: train_loss=0.2179, val_loss=0.2523, val_acc=0.8894\n",
      "Epoch 5: train_loss=0.2158, val_loss=0.2514, val_acc=0.8968\n",
      "Epoch 6: train_loss=0.2047, val_loss=0.2605, val_acc=0.8968\n",
      "Epoch 7: train_loss=0.1951, val_loss=0.2577, val_acc=0.8791\n",
      "Epoch 8: train_loss=0.1920, val_loss=0.2760, val_acc=0.8894\n",
      "Epoch 9: train_loss=0.1859, val_loss=0.2764, val_acc=0.8894\n",
      "Epoch 10: train_loss=0.1881, val_loss=0.2807, val_acc=0.8879\n",
      "\n",
      "Training Mini-Batch...\n",
      "Epoch 1: train_loss=0.5577, val_loss=0.4656, val_acc=0.8850\n",
      "Epoch 2: train_loss=0.4211, val_loss=0.3899, val_acc=0.8850\n",
      "Epoch 3: train_loss=0.3779, val_loss=0.3669, val_acc=0.8850\n",
      "Epoch 4: train_loss=0.3636, val_loss=0.3545, val_acc=0.8850\n",
      "Epoch 5: train_loss=0.3503, val_loss=0.3445, val_acc=0.8850\n",
      "Epoch 6: train_loss=0.3387, val_loss=0.3355, val_acc=0.8850\n",
      "Epoch 7: train_loss=0.3262, val_loss=0.3274, val_acc=0.8850\n",
      "Epoch 8: train_loss=0.3205, val_loss=0.3196, val_acc=0.8850\n",
      "Epoch 9: train_loss=0.3105, val_loss=0.3121, val_acc=0.8850\n",
      "Epoch 10: train_loss=0.3003, val_loss=0.3052, val_acc=0.8850\n",
      "Epoch 11: train_loss=0.2951, val_loss=0.2991, val_acc=0.8850\n",
      "Epoch 12: train_loss=0.2810, val_loss=0.2937, val_acc=0.8850\n",
      "Epoch 13: train_loss=0.2775, val_loss=0.2886, val_acc=0.8835\n",
      "Epoch 14: train_loss=0.2751, val_loss=0.2844, val_acc=0.8835\n",
      "Epoch 15: train_loss=0.2707, val_loss=0.2800, val_acc=0.8850\n",
      "Epoch 16: train_loss=0.2608, val_loss=0.2772, val_acc=0.8879\n",
      "Epoch 17: train_loss=0.2559, val_loss=0.2742, val_acc=0.8864\n",
      "Epoch 18: train_loss=0.2533, val_loss=0.2719, val_acc=0.8879\n",
      "Epoch 19: train_loss=0.2440, val_loss=0.2705, val_acc=0.8938\n",
      "Epoch 20: train_loss=0.2464, val_loss=0.2687, val_acc=0.8997\n",
      "Epoch 21: train_loss=0.2395, val_loss=0.2677, val_acc=0.8982\n",
      "Epoch 22: train_loss=0.2428, val_loss=0.2677, val_acc=0.8968\n",
      "Epoch 23: train_loss=0.2378, val_loss=0.2667, val_acc=0.8953\n",
      "Epoch 24: train_loss=0.2339, val_loss=0.2668, val_acc=0.8968\n",
      "Epoch 25: train_loss=0.2349, val_loss=0.2669, val_acc=0.8968\n",
      "Epoch 26: train_loss=0.2306, val_loss=0.2658, val_acc=0.8968\n",
      "Epoch 27: train_loss=0.2346, val_loss=0.2649, val_acc=0.8953\n",
      "Epoch 28: train_loss=0.2283, val_loss=0.2648, val_acc=0.8968\n",
      "Epoch 29: train_loss=0.2268, val_loss=0.2652, val_acc=0.8968\n",
      "Epoch 30: train_loss=0.2256, val_loss=0.2650, val_acc=0.8953\n",
      "Epoch 31: train_loss=0.2232, val_loss=0.2651, val_acc=0.8968\n",
      "Epoch 32: train_loss=0.2268, val_loss=0.2653, val_acc=0.8923\n",
      "Epoch 33: train_loss=0.2212, val_loss=0.2664, val_acc=0.8923\n",
      "\n",
      "Training Momentum...\n",
      "Epoch 1: train_loss=0.4141, val_loss=0.3381, val_acc=0.8850\n",
      "Epoch 2: train_loss=0.2986, val_loss=0.2921, val_acc=0.8850\n",
      "Epoch 3: train_loss=0.2551, val_loss=0.2719, val_acc=0.8909\n",
      "Epoch 4: train_loss=0.2308, val_loss=0.2701, val_acc=0.8909\n",
      "Epoch 5: train_loss=0.2286, val_loss=0.2744, val_acc=0.8938\n",
      "Epoch 6: train_loss=0.2228, val_loss=0.2658, val_acc=0.8968\n",
      "Epoch 7: train_loss=0.2201, val_loss=0.2731, val_acc=0.8938\n",
      "Epoch 8: train_loss=0.2133, val_loss=0.2710, val_acc=0.8938\n",
      "Epoch 9: train_loss=0.2096, val_loss=0.2693, val_acc=0.8953\n",
      "Epoch 10: train_loss=0.2082, val_loss=0.2700, val_acc=0.8938\n",
      "Epoch 11: train_loss=0.2028, val_loss=0.2783, val_acc=0.8968\n",
      "\n",
      "Training Nesterov...\n",
      "Epoch 1: train_loss=0.3924, val_loss=0.3146, val_acc=0.8850\n",
      "Epoch 2: train_loss=0.2808, val_loss=0.2723, val_acc=0.8909\n",
      "Epoch 3: train_loss=0.2461, val_loss=0.2651, val_acc=0.8923\n",
      "Epoch 4: train_loss=0.2329, val_loss=0.2685, val_acc=0.8968\n",
      "Epoch 5: train_loss=0.2212, val_loss=0.2645, val_acc=0.8923\n",
      "Epoch 6: train_loss=0.2157, val_loss=0.2629, val_acc=0.8968\n",
      "Epoch 7: train_loss=0.2149, val_loss=0.2676, val_acc=0.8953\n",
      "Epoch 8: train_loss=0.2137, val_loss=0.2649, val_acc=0.8923\n",
      "Epoch 9: train_loss=0.2071, val_loss=0.2669, val_acc=0.8938\n",
      "Epoch 10: train_loss=0.2058, val_loss=0.2621, val_acc=0.8953\n",
      "Epoch 11: train_loss=0.1984, val_loss=0.2667, val_acc=0.8923\n",
      "Epoch 12: train_loss=0.1970, val_loss=0.2684, val_acc=0.8938\n",
      "Epoch 13: train_loss=0.1969, val_loss=0.2804, val_acc=0.8938\n",
      "Epoch 14: train_loss=0.1913, val_loss=0.2760, val_acc=0.8909\n",
      "Epoch 15: train_loss=0.1820, val_loss=0.2763, val_acc=0.8938\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Optimizer: Batch GD\n",
      "Accuracy : 0.8939617083946981\n",
      "Precision: 0.5576923076923077\n",
      "Recall   : 0.3717948717948718\n",
      "F1-score : 0.4461538461538462\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Experiment: Evaluation of Optimization Techniques\n",
    "# Dataset: UCI Bank Marketing - bank.csv\n",
    "# Delimiter: ;\n",
    "# Target variable: y\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Reproducibility\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load dataset (with correct delimiter)\n",
    "# ------------------------------------------------------------\n",
    "CSV_PATH = r\"C:\\Users\\23adsb58\\Documents\\bank\\bank.csv\"  # <-- your CSV path\n",
    "\n",
    "# Correctly read semicolon-separated CSV\n",
    "data = pd.read_csv(CSV_PATH, sep=';', engine='python')\n",
    "\n",
    "# Verify columns\n",
    "print(\"Columns:\", data.columns)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Preprocessing\n",
    "# ------------------------------------------------------------\n",
    "# Encode target column\n",
    "data['y'] = data['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Split features and labels\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train / Validation / Test split (70 / 15 / 15)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Convert to PyTorch datasets\n",
    "# ------------------------------------------------------------\n",
    "def make_dataset(X, y):\n",
    "    return TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32),\n",
    "        torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train)\n",
    "val_ds   = make_dataset(X_val, y_val)\n",
    "test_ds  = make_dataset(X_test, y_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Neural network architecture\n",
    "# ------------------------------------------------------------\n",
    "class BankNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Training function with early stopping\n",
    "# ------------------------------------------------------------\n",
    "def train_model(optimizer_fn, batch_size):\n",
    "    model = BankNet(X_train.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optimizer_fn(model)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    epoch = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        # -------- Training --------\n",
    "        model.train()\n",
    "        train_losses, y_true, y_pred = [], [], []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            y_true.extend(yb.numpy())\n",
    "            y_pred.extend((preds > 0.5).float().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        val_losses, y_true, y_pred = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_losses.append(loss.item())\n",
    "                y_true.extend(yb.numpy())\n",
    "                y_pred.extend((preds > 0.5).float().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        history[\"train_loss\"].append(np.mean(train_losses))\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}: train_loss={np.mean(train_losses):.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # -------- Early stopping --------\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history, training_time\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Optimizers\n",
    "# ------------------------------------------------------------\n",
    "lr = 0.01\n",
    "\n",
    "optimizers = {\n",
    "    \"Batch GD\": lambda m: torch.optim.SGD(m.parameters(), lr=lr),\n",
    "    \"SGD\": lambda m: torch.optim.SGD(m.parameters(), lr=lr),\n",
    "    \"Mini-Batch\": lambda m: torch.optim.SGD(m.parameters(), lr=lr),\n",
    "    \"Momentum\": lambda m: torch.optim.SGD(m.parameters(), lr=lr, momentum=0.9),\n",
    "    \"Nesterov\": lambda m: torch.optim.SGD(m.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "}\n",
    "\n",
    "batch_sizes = {\n",
    "    \"Batch GD\": len(train_ds),\n",
    "    \"SGD\": 1,\n",
    "    \"Mini-Batch\": 32,\n",
    "    \"Momentum\": 32,\n",
    "    \"Nesterov\": 32\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Train all optimizers\n",
    "# ------------------------------------------------------------\n",
    "results = {}\n",
    "\n",
    "for name in optimizers:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model, history, t = train_model(\n",
    "        optimizers[name],\n",
    "        batch_sizes[name]\n",
    "    )\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"time\": t\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. Comparative plots\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name in results:\n",
    "    plt.plot(results[name][\"history\"][\"val_loss\"], label=name)\n",
    "plt.title(\"Validation Loss vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name in results:\n",
    "    plt.plot(results[name][\"history\"][\"val_acc\"], label=name)\n",
    "plt.title(\"Validation Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10. Test set evaluation\n",
    "# ------------------------------------------------------------\n",
    "best_optimizer = max(results, key=lambda k: max(results[k][\"history\"][\"val_acc\"]))\n",
    "best_model = results[best_optimizer][\"model\"]\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = best_model(torch.tensor(X_test, dtype=torch.float32))\n",
    "    test_preds = (test_preds > 0.5).float().numpy()\n",
    "\n",
    "print(\"\\nBest Optimizer:\", best_optimizer)\n",
    "print(\"Accuracy :\", accuracy_score(y_test, test_preds))\n",
    "print(\"Precision:\", precision_score(y_test, test_preds))\n",
    "print(\"Recall   :\", recall_score(y_test, test_preds))\n",
    "print(\"F1-score :\", f1_score(y_test, test_preds))\n",
    "\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e298d9a-64a2-4be8-96c1-8f397730c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
